This is “the structure” of the docs. Simply a nested bulleted list; to make a page appear in the docs add it in appropriate place here. Please note that blank lines in the TOC structure are not allowed - make sure you do not add new blank lines in between TOC entries!

Unity Manual TOC
Unity Manual
Unity Manual * Manual Versions * Switching between Unity versions in the documentation * New in Unity 5.5b
Working In Unity
Basics
Downloading and installing Unity
Deploying Unity offline
2D or 3D projects
Getting started
Learning the interface
Asset Workflow
Primitive and Placeholder Objects
Importing Assets
Import Settings
Importing from the Asset Store
Asset Packages
Standard Assets
The Main Windows
The Project Window
The Scene View
Scene View navigation
Positioning GameObjects
Scene View Control Bar
The Game View
The Hierarchy window
The Inspector Window
Editing Properties
Preset Libraries
Inspector Options
The Toolbar
Searching
Other Windows
Customizing Your Workspace
Unity Hotkeys
Creating Gameplay
Scenes
GameObjects
GameObject
Introduction to Components
Using Components
Transform
Creating Components
Deactivating GameObjects
Tags
Static GameObjects
Prefabs
Saving Your Work
Instantiating Prefabs at runtime
Input
Conventional Game Input
Input for Oculus
Input for OpenVR controllers
Mobile Device Input
Mobile Keyboard
Transforms
Adding Random Gameplay Elements
Rotation and Orientation in Unity
Trouble Shooting
Lights
Cameras
Cross-Platform Considerations
Publishing Builds
Editor Features
2D and 3D Mode Settings
Preferences
Build Settings
Settings Managers
Audio Manager
Editor settings
Input Manager
Network Manager
Physics Manager
Physics 2D Settings
Player Settings
Splash Screen
Quality Settings
Graphics Settings
Script Execution Order Settings
Tags and Layers
Time Manager
Network Emulation
Visual Studio C# integration
RenderDoc Integration
Editor Analytics
Check For Updates
IME in Unity
Special folder names
Exporting Packages
Version Control
Version control integration
Perforce Integration
Plastic SCM Integration
Using External Version Control Systems with Unity
Smart Merge
Troubleshooting The Editor
Advanced Development
The Profiler Window
Profiler window
CPU Usage Profiler
Rendering Profiler
Memory Profiler
Audio Profiler
Physics Profiler
GPU Profiler
Multi Scene Editing
Loading Resources at Runtime
Plugins
Plugin Inspector
Managed Plugins
Native Plugins
Building Plugins for Desktop Platforms
Low-level Native Plugin Interface
AssetBundles
Building AssetBundles
Asset Bundle Compression
Asset Bundle Internal Structure
Downloading AssetBundles
Loading and unloading objects from an AssetBundle
Keeping Track of loaded AssetBundles
Storing and loading binary data in an AssetBundle
Protecting Content
Including scripts in AssetBundles
Optimizing disk seeks with AssetBundles
AssetBundles FAQ
Reducing the File Size of the Build
Social API
JSON Serialization
Streaming Assets
ScriptableObject
Advanced Editor Topics
Build Player Pipeline
Command line arguments
Behind the Scenes
AssetDatabase
Text-Based Scene Files
Description of the Format
An Example of a YAML Scene File
YAML Class ID Reference
Cache Server
Modifying Source Assets Through Scripting
Extending the Editor
Editor Windows
Property Drawers
Custom Editors
Running Editor Script Code on Launch
Licenses and Activation
Online Activation
Offline / Manual Activation
Managing your License
Activation FAQ
Upgrade Guides
Using the Automatic API Updater
Upgrading to Unity 5.5 beta
Upgrading to Unity 5.4
5.4 Networking API Changes
Upgrading to Unity 5.3
Upgrading to Unity 5.2
Upgrading to Unity 5.0
AI in Unity 5.0
Animation in Unity 5.0
Audio in Unity 5.0
Baked Data in Unity 5.0
Plugins in Unity 5.0
Physics in Unity 5.0
Shaders in Unity 5.0
Other Upgrade Notes for Unity 5.0
Upgrading to Unity 4.0
Upgrading to Unity 3.5
Unity 2D
Gameplay in 2D
Sprites
Sprite Creator
Sprite Editor
Sprite Packer
2D Physics Reference
Rigidbody 2D
Collider 2D
Circle Collider 2D
Box Collider 2D
Polygon Collider 2D
Edge Collider 2D
Capsule Collider 2D Component
Physics Material 2D
2D Joints
Distance Joint 2D
Fixed Joint 2D
Friction Joint 2D
Hinge Joint 2D
Relative Joint 2D
Slider Joint 2D
Spring Joint 2D
Target Joint 2D
Wheel Joint 2D
Constant Force 2D
Area Effector 2D
Buoyancy Effector 2D
Point Effector 2D
Platform Effector 2D
Surface Effector 2D
Physics 2D Raycaster
Graphics
Graphics Overview
Lighting
Lighting overview
Light sources
Types of light
The Light inspector
Using Lights
Cookies
Shadows
Shadows
Directional light shadows
Global Illumination
Lighting Window
Using precomputed lighting
Light Probes
Light Probe Proxy Volumes component
Light Probe Group
Lightmap Parameters
Directional Lightmapping
Lighting Data Asset
Meta Pass
GI Visualisations in the Scene View
GI Cache
Light troubleshooting and performance
Related topics
Linear Rendering
Cameras
Using more than one camera
Camera Tricks
Understanding the View Frustum
The Size of the Frustum at a Given Distance from the Camera
Dolly Zoom (AKA the “Trombone” Effect)
Rays from the Camera
Using an Oblique Frustum
Creating an Impression of Large or Small Size
Occlusion Culling
Materials, Shaders & Textures
Creating and Using Materials
Standard Shader
Content and Context
Metallic vs Specular Workflow
Material parameters
Rendering Mode
Albedo Color and Transparency
Specular mode: Specular parameter
Metallic mode: Metallic Parameter
Smoothness
Normal map (Bump mapping)
Heightmap
Occlusion Map
Emission
Secondary Maps (Detail Maps) & Detail Mask
The Fresnel Effect
Material charts
Make your own
Accessing and Modifying Material parameters via script
Writing Shaders
Legacy Shaders
Usage and Performance of Built-in Shaders
Normal Shader Family
Vertex-Lit
Diffuse
Specular
Bumped Diffuse
Bumped Specular
Parallax Diffuse
Parallax Bumped Specular
Decal
Diffuse Detail
Transparent Shader Family
Transparent Vertex-Lit
Transparent Diffuse
Transparent Specular
Transparent Bumped Diffuse
Transparent Bumped Specular
Transparent Parallax Diffuse
Transparent Parallax Specular
Transparent Cutout Shader Family
Transparent Cutout Vertex-Lit
Transparent Cutout Diffuse
Transparent Cutout Specular
Transparent Cutout Bumped Diffuse
Transparent Cutout Bumped Specular
Self-Illuminated Shader Family
Self-Illuminated Vertex-Lit
Self-Illuminated Diffuse
Self-Illuminated Specular
Self-Illuminated Normal mapped Diffuse
Self-Illuminated Normal mapped Specular
Self-Illuminated Parallax Diffuse
Self-Illuminated Parallax Specular
Reflective Shader Family
Reflective Vertex-Lit
Reflective Diffuse
Reflective Specular
Reflective Bumped Diffuse
Reflective Bumped Specular
Reflective Parallax Diffuse
Reflective Parallax Specular
Reflective Normal Mapped Unlit
Reflective Normal mapped Vertex-lit
Terrain Engine
Creating and Editing Terrains
Height Tools
Terrain Textures
Trees
SpeedTree
Grass and Other Details
Wind Zones
Terrain Settings
Tree Editor
Building Your First Tree
Tree Basics
Branch Group Properties
Leaf Group Properties
Tree - Wind Zones
Particle Systems
What is a Particle System?
Using Particle Systems in Unity
Particle System How-Tos
A Simple Explosion
Exhaust Smoke from a Vehicle
Particle System vertex streams and Standard Shader support
Textures and Videos
Image effects overview
Reflection probes
Types of Reflection Probe
Using Reflection Probes
Advanced Reflection Probe Features
Reflection Probe Performance and Optimisation
Cluster Rendering
Deploying a Unity Cluster
Input for Cluster Rendering
API
Advanced Rendering Features
High Dynamic Range Rendering
HDR Color Picker
Rendering Paths
Level of Detail
DirectX 11 and OpenGL Core
OpenGL Core Details
Compute Shaders
Graphics Command Buffers
GPU instancing
Sparse Textures
http://mdeditor.infra.hq.unity3d.com/#GraphicsEmulation
CullingGroup API
Asynchronous Texture Upload
Procedural Materials
Procedural Materials - Memory Usage and Caching Behaviour
Procedural Mesh Geometry
Anatomy of a Mesh
Using the Mesh Class
Example - Creating a Quad
Optimizing graphics performance
Draw call batching
Modeling characters for optimal performance
Rendering Statistics Window
Frame Debugger
Optimizing Shader Load Time
Layers
Layer-based collision detection
Graphics Reference
Cameras Reference
Camera
Flare Layer
GUI Layer (Legacy)
Shader Reference
Writing Surface Shaders
Surface Shader examples
Custom Lighting models in Surface Shaders
Surface Shader lighting examples
Surface Shaders with DX11 / OpenGL Core Tessellation
Writing vertex and fragment shaders
Vertex and fragment shader examples
Shader semantics
Accessing shader properties in Cg/HLSL
Providing vertex data to vertex programs
Built-in shader include files
Predefined Shader preprocessor macros
Built-in shader helper functions
Built-in shader variables
Making multiple shader program variants
GLSL Shader programs
Shading Language used in Unity
Shader Compilation Target Levels
Shader data types and precision
ShaderLab Syntax
ShaderLab: Properties
ShaderLab: SubShader
ShaderLab: Pass
ShaderLab: Culling & Depth Testing
ShaderLab: Blending
ShaderLab: Pass Tags
ShaderLab: Stencil
ShaderLab: Name
ShaderLab: Legacy Lighting
ShaderLab: Legacy Texture Combiners
ShaderLab: Legacy Alpha Testing
ShaderLab: Legacy Fog
ShaderLab: Legacy BindChannels
ShaderLab: UsePass
ShaderLab: GrabPass
ShaderLab: SubShader Tags
ShaderLab: Fallback
ShaderLab: CustomEditor
ShaderLab: other commands
Shader assets
Advanced ShaderLab topics
Unity’s Rendering Pipeline
Performance tips when writing shaders
Rendering with Replaced Shaders
Custom Shader GUI
Using Depth Textures
Camera’s Depth Texture
Platform-specific rendering differences
Shader Level of Detail
Texture arrays
Debugging DirectX 11 shaders with Visual Studio
Implementing Fixed Function TexGen in Shaders
Particle Systems Reference
Particle system
Particle System Modules
Main module
Emission Module
Shape Module
Velocity Over Lifetime Module
Limit Velocity Over Lifetime Module
Inherit Velocity Module
Force Over Lifetime Module
Color Over Lifetime Module
Color By Speed Module
Size over Lifetime module
Size by Speed module
Rotation Over Lifetime Module
Rotation By Speed Module
External Forces Module
Collision Module
Triggers module
Sub Emitters Module
Texture Sheet Animation Module
Renderer Module
Particle Systems (Legacy, prior to release 3.5)
Ellipsoid Particle Emitter (Legacy)
Mesh Particle Emitter (Legacy)
Particle Animator (Legacy)
Particle Renderer (Legacy)
World Particle Collider (Legacy)
Visual Effects Reference
Halo
Lens Flare
Flare
Line Renderer
Trail Renderer
Billboard Renderer
Billboard Asset
Projector
Image Effect reference
Writing Image Effects
Antialiasing
Bloom
Bloom (Optimized)
Bloom and Lens Flares
Blur
Blur (Optimized)
Camera Motion Blur
Color Correction Curves
Color Correction Ramp Texture
Color Correction lookup texture
Contrast Enhance
Contrast Stretch
Crease Shading
Depth of Field (Deprecated)
Depth of Field
Edge Detection
Edge Detect Effect Normals
Fisheye
Global Fog
Grayscale
Motion Blur
Noise And Grain
Noise And Scratches
Screen Overlay
Sepia Tone
Sun Shafts
Screen Space Ambient Obscurance
Screen Space Ambient Occlusion
Tilt Shift
Tonemapping
Twirl
Vignetting and Chromatic Aberration
Vortex
Mesh Components
Meshes
Material
Mesh Filter
Mesh Renderer
Skinned Mesh Renderer
Text Mesh
Text Asset
Font
Texture Components
Textures
Importing Textures
Texture Types
Texture compression formats for platform-specific overrides
Procedural Material Assets
Render Texture
Movie Texture
3D Textures
Texture arrays
Rendering Components
Cubemap
Occlusion Area
Occlusion Portals
Skybox
Reflection Probe
LOD Group
Rendering Pipeline Details
Deferred shading rendering path
Forward Rendering Path Details
Legacy Deferred Lighting Rendering Path
Vertex Lit Rendering Path Details
Hardware Requirements for Unity’s Graphics Features
Sprite Renderer
Graphics HOWTOs
How do I Import Alpha Textures?
How do I Make a Skybox?
How do I make a Mesh Particle Emitter? (Legacy Particle System)
How do I make a Spot Light Cookie?
How do I fix the rotation of an imported model?
Water in Unity
Art Asset Best Practice Guide
How do I import Models from my 3D app?
FBX export guide
Importing Objects From Maya
Importing Objects From Cinema 4D
Importing Objects From 3D Studio Max
Importing Objects From Cheetah3D
Importing Objects From Modo
Importing Objects From Lightwave
Importing Objects From Blender
Importing Objects From SketchUp
How to do Stereoscopic Rendering
Graphics Tutorials
Shaders: ShaderLab and fixed function shaders
Shaders: vertex and fragment programs
Physics
Physics Overview
Rigidbody overview
Colliders
Joints
Character Controllers
3D Physics Reference
Box Collider
Capsule Collider
Character Controller
Character Joint
Configurable Joint
Constant Force
Fixed Joint
Hinge Joint
Mesh Collider
Rigidbody
Sphere Collider
Spring Joint
Cloth
Wheel Collider
Terrain Collider
Physic Material
Physics HOWTOs
Ragdoll Wizard
Joint and Ragdoll stability
WheelCollider Tutorial
Scripting
Scripting Overview
Creating and Using Scripts
Variables and the Inspector
Controlling GameObjects Using Components
Event Functions
Time and Framerate Management
Creating and Destroying GameObjects
Coroutines
Namespaces
Attributes
Execution Order of Event Functions
Understanding Automatic Memory Management
Platform dependent compilation
Special folders and script compilation order
Generic Functions
Scripting restrictions
Script Serialization
UnityEvents
What is a Null Reference Exception?
Important Classes
Vector Cookbook
Understanding Vector Arithmetic
Direction and Distance from One Object to Another
Computing a Normal/Perpendicular vector
The Amount of One Vector’s Magnitude that Lies in Another Vector’s Direction
Scripting Tools
Console Window
MonoDevelop
Attaching MonoDevelop Debugger To An Android Device
Log Files
Editor Test Runner
IL2CPP
Event System
Messaging System
Input Modules
Supported Events
Raycasters
Event System Reference
Event System Manager
Graphic Raycaster
Physics Raycaster
Standalone Input Module
Touch Input Module
Event Trigger
Multiplayer and Networking
Networking Overview
The High Level API
Network System Concepts
Setting up a Multiplayer Project from Scratch
Using the Network Manager
Object Spawning
Custom Spawn Functions
State Synchronization
Remote Actions
Player Objects
Object Visibility
Network Manager callbacks
NetworkBehaviour callbacks
Network Messages
Local Discovery
Scene Objects
Converting a single-player game to Unity Multiplayer
Multiplayer Lobby
Network Clients and Servers
Host Migration
Using the Transport Layer API
Internet Services
Networking Tips for Mobile devices.
UnityWebRequest
Common operations: using the HLAPI
Retrieving text or binary data from an HTTP Server (GET)
Retrieving a Texture from an HTTP Server (GET)
Downloading an AssetBundle from an HTTP server (GET)
Sending a form to an HTTP server (POST)
Uploading raw data to an HTTP server (PUT)
Advanced operations: Using the LLAPI
Creating UnityWebRequests
Creating UploadHandlers
Creating DownloadHandlers
Networking Reference
NetworkAnimator
NetworkBehaviour
NetworkClient
NetworkConnection
NetworkDiscovery
NetworkIdentity
Network Lobby Manager
Network Lobby Player
NetworkManager
Network Manager HUD
Network Proximity Checker
NetworkReader
NetworkServer
NetworkStartPosition
NetworkTransform
NetworkTransformChild
NetworkTransformVisualizer
NetworkTransport
NetworkWriter
Audio
Audio Overview
Audio files
Tracker Modules
Audio Mixer
An overview of the concepts and AudioMixer
Specifics on the AudioMixer window
AudioGroup Inspector
Overview of Usage and API
Native Audio Plugin SDK
Audio Spatializer SDK
Audio Profiler
Audio Reference
Audio Clip
Audio Listener
Audio Source
Audio Mixer
Audio Filters
Audio Low Pass Filter
Audio High Pass Filter
Audio Echo Filter
Audio Distortion Filter
Audio Reverb Filter
Audio Chorus Filter
Audio Effects
Audio Low Pass Effect
Audio High Pass Effect
Audio Echo Effect
Audio Flange Effect
Audio Distortion Effect
Audio Normalize Effect
Audio Parametric Equalizer Effect
Audio Pitch Shifter Effect
Audio Chorus Effect
Audio Compressor Effect
Audio SFX Reverb Effect
Audio Low Pass Simple Effect
Audio Delay Effect
Audio High Pass Simple Effect
Reverb Zones
Microphone
Audio Settings
Animation
Animation System Overview
Animation Clips
Animation from external sources
Working with humanoid animations
Creating the Avatar
Configuring the Avatar
Muscle setup
Asset Preparation and Import
Using Humanoid Characters
Preparing your own character
Non-humanoid Animations
Splitting Animations
Looping animation clips
Masking Imported Clips
Animation Curves on Imported Clips
Animation events on imported clips
Selecting a Root Motion Node
Euler Curve Import
Animation Window Guide
Using the Animation view
Creating a New Animation Clip
Animating a Game Object
Using Animation Curves
Editing Curves
Key manipulation in Dopesheet mode
Key manipulation in Curves mode
Objects with Multiple Moving Parts
Using Animation Events
Animator Controllers
The Animator Controller Asset
The Animator Window
Animation State Machines
State Machine Basics
Animation Parameters
State Machine Transitions
State Machine Behaviours
Sub-State Machines
Animation Layers
Solo and Mute functionality
Target Matching
Inverse Kinematics
Root Motion - how it works
Tutorial: Scripting Root Motion for “in-place” humanoid animations
Blend Trees
1D Blending
2D Blending
Direct Blending
Additional Blend Tree Options
Animation Blend Shapes
Animator Override Controllers
Retargeting of Humanoid animations
Performance and Optimization
Animation Reference
Animation Clip
Animator Component
Animator Controller
Creating an AnimatorController
Animation States
Animation Transitions
Avatar Mask
Human Template files
Importing models
Models
FBX Importer, Rig options
FBX Importer - Animations Tab
3D formats
Animation HOWTOs
Animation FAQ
Using Blender and Rigify
Playable API
Creating custom animation Playables
Graph Visualizer
A Glossary of animation terms
UI
Canvas
Basic Layout
Visual Components
Interaction Components
Animation Integration
Auto Layout
Rich Text
UI Reference
Rect Transform
Canvas Components
Canvas
Canvas Scaler
Canvas Group
Canvas Renderer
Visual Components
Text
Image
Raw Image
Mask
RectMask2D
UI Effect Components
Shadow
Outline
Position as UV1
Interaction Components
Selectable Base Class
Transition Options
Navigation Options
Button
Toggle
Toggle Group
Slider
Scrollbar
Dropdown
Input Field
Scroll Rect
Auto Layout
Layout Element
Content Size Fitter
Aspect Ratio Fitter
Horizontal Layout Group
Vertical Layout Group
Grid Layout Group
UI How Tos
Designing UI for Multiple Resolutions
Making UI elements fit the size of their content
Creating a World Space UI
Creating UI elements from scripting
Creating Screen Transitons
Immediate Mode GUI (IMGUI)
IMGUI Basics
Controls
Customization
IMGUI Layout Modes
Extending IMGUI
GUI Skin (IMGUI System)
GUI Style (IMGUI System)
Navigation and Pathfinding
Navigation Overview
Navigation System in Unity
Inner Workings of the Navigation System
Building a NavMesh
Advanced NavMesh Bake Settings
Creating a NavMesh Agent
Creating a NavMesh Obstacle
Creating an Off-mesh Link
Building Off-Mesh Links Automatically
Building Height Mesh for Accurate Character Placement
Navigation Areas and Costs
Loading Multiple NavMeshes using Additive Loading
Using NavMesh Agent with Other Components
Navigation Reference
NavMesh Agent
NavMesh Obstacle
Off-Mesh Link
Navigation How-Tos
Telling a NavMeshAgent to Move to a Destination
Moving an Agent to a Position Clicked by the Mouse
Making an Agent Patrol Between a Set of Points
Coupling Animation and Navigation
Unity Services
Setting up your project for Unity Services
Unity Ads
How to use Unity Ads
Best practices on rewarded ads
Important links
Unity Analytics
Setting Up Analytics
Custom Events
Analytics Tracker Component
Custom Event Scripting
Monetization
Receipt Verification
User Attributes
Upgrading Unity Analytics
Upgrade Unity Analytics 4.x–5.1 (SDK) to 5.2 onwards
Upgrade Unity Analytics 5.1 to 5.2 onwards
Unity Analytics Re-integrate SDK to 5.1
What to Do if Project IDs Don’t Match
Unity Analytics Raw Data Export
Unity Cloud Build
Continuous integration
Supported platforms
Supported versions of Unity
Version control systems
Using Git with Unity Cloud Build
Using GitHub with Unity Cloud Build
Using Bitbucket with Unity Cloud Build
Using Mercurial with Unity Cloud Build
Using Apache Subversion (SVN) with Unity Cloud Build
Using Perforce with Unity Cloud Build
Building for iOS
Advanced options
Development builds
Pre- and post-export methods
Xcode frameworks
Custom scripting #define directives
Including specific Scenes
Build manifest
Build manifest as JSON
Build manifest as ScriptableObject
Cloud Build REST API
Unity IAP
Setting up Unity IAP
Configuring for Apple App Store and Mac App Store
Configuring for Google Play Store
Configuring for Windows Store
Configuration for the Amazon Appstore and Amazon Underground stores
Samsung Galaxy IAP configuration
Configuring for Tizen Store
Cross Platform Guide
Defining products
Initialization
Browsing Product Metadata
Initiating Purchases
Processing Purchases
Handling purchase failures
Restoring Transactions
Purchase Receipts
Receipt validation
Store Extensions
Cross-store installation issues with Android in-app purchase stores
Store Guides
iOS & Mac App Stores
Windows Store
Google Play
Amazon Appstore and Amazon Underground Store
Samsung Galaxy apps
Tizen store
Implementing a Store
Initialization
Retrieving products
Handling purchases
Store Modules
Registering your store
Store Configuration
Store Extensions
Unity Collaborate
Setting up Unity Collaborate
Adding teammates to Collaborate
Viewing history
Enabling Cloud Build with Collaborate
Upgrading with Unity Collaborate
Reverting files
Resolving file conflicts
Collaborate Troubleshooting Tips
Unity Performance Reporting
Setting up Performance Reporting
Understanding exception reports
Unity Multiplayer
Setting up the Multiplayer Service
Integrating the Multiplayer Service
Integration using the HUD
Integration using Unity’s High-Level API
Integration using NetworkTransport
Common Errors
Virtual Reality
VR overview
VR reference
VR devices
Oculus
OpenVR
HoloLens
VR Audio Spatializers
Open-source repositories
How to contribute to Unity
Step 1: Get a Bitbucket account
Step 2: Fork the repository you want to contribute to
Step 3: Clone your fork
Step 4: Apply modifications to your fork
Step 5: Open a pull request on Bitbucket
Step 6: Wait for feedback
Further Reading
FAQ
Asset Store Publishing
Adding Keywords to Assets
Asset Store Publisher Administration
Asset Store FAQ
Asset Store Publishing Guide
DeprecateAssetGuide
Asset Store Manual
Publisher Admin Section Overview
Setting up Google Analytics
Viewing the status of your Asset Store submissions
Refunding your customers
Removing your Assets from the Asset Store
Providing support
Asset Store promotions
Platform-specific
Standalone
Standalone Player Settings
Multi-display
Apple Mac
How to deliver an application to the Apple Mac Store
WebGL
WebGL Player Settings
Getting started with WebGL development
WebGL Browser Compatibility
Building and running a WebGL project*
Deploying compressed builds
Debugging and trouble shooting WebGL builds
WebGL Graphics
WebGL Networking
Using Audio In WebGL
WebGL performance considerations
Memory Considerations when targeting WebGL
WebGL: Interacting with browser scripting
Using WebGL Templates
Cursor locking and full-screen mode in WebGL
Input in WebGL
Building games for Apple TV
iOS
Getting started with iOS development
Unity iOS Basics
iOS account setup
Inside the iOS build process
Structure of a Unity XCode Project
Customizing an iOS Splash Screen
iOS Hardware Guide
iOS Player Settings
iOS 2D Texture Overrides
Upgrading to 64 bit iOS
iOS Advanced Topics
Unity Remote
iOS Scripting
iOS Game Controller support
Advanced Unity Mobile Scripting
Optimizing Performance in iOS.
iOS Specific Optimizations
Measuring Performance with the Built-in Profiler
Optimizing the Size of the Built iOS Player
Optimizing Physics Performance
Building Plugins for iOS
Preparing your application for “In App Purchases”
Customising WWW Requests on iOS
App thinning
Features currently not supported by Unity iOS
Troubleshooting on iOS devices
Reporting crash bugs on iOS
Android
Getting started with Android development
Android SDK Setup
Unity Remote 4
Android Remote (DEPRECATED)
Troubleshooting Android development
Inside the Android build process
Reporting crash bugs under Android
Features currently not supported by Unity Android
Support for Split Application Binary (.OBB)
Android Scripting
Advanced Unity Mobile Scripting
Building Plugins for Android
Customizing an Android Splash Screen
Android Player Settings
Android 2D Textures Overrides
Gradle for Android
Gradle troubleshooting
Samsung TV
Getting started with Samsung TV development
Samsung TV Setup
Samsung TV Input
Samsung TV Debugging
SamsungTV Restrictions
Samsung TV Not Supported
Security Sandbox
Tizen
Getting Started with Tizen Development
Setting up Unity to Build to Your Tizen Device
Tizen FAQ
Tizen Player Settings
Features currently not supported by Unity Tizen
Building Plugins for Tizen
Tizen Emulator
Windows
Windows General
Windows Debugging
WindowsLowIntegrity
Windows Store Apps
Getting Started
Windows Store Apps: Deployment
Windows Store Apps: Profiler
Windows Store Apps: Command line arguments
Windows Store Apps: Association launching
AppCallbacks class
Windows Store Apps: WinRT API in C# scripts
WSA Player Settings
Windows Store: Windows SDKs
Windows Phone 8.1
Windows Phone 8.1: Debugging
Windows 8.1 Universal Applications
Universal Windows 10 Applications: Getting Started
Scripting Backends
Windows Store: .NET Scripting Backend
Windows Store Apps: Generated project with .NET scripting backend
Windows Store Apps: Missing .NET Types on .NET Scripting Backend
Windows Store Apps: Plugins on .NET Scripting Backend
Windows Store Apps: Debugging on .NET Scripting Backend
Windows Store: IL2CPP Scripting Backend
Windows Store: Generated project with IL2CPP scripting backend
Windows Store: Plugins on IL2CPP Scripting Backend
Windows Store Apps: Debugging on IL2CPP Scripting Backend
FAQ
Windows Store Apps: Examples
Windows Store Apps: Code snippets
Windows Holographic
Getting started
Set-up
Running your first application
Input types
Focus Point
Anchors
Persistence
Anchor Sharing
Spatial Mapping
Spatial mapping concepts
Spatial Mapping basic low level API usage
Spatial Mapping best practices
Spatial Mapping components
Web Camera
Photo capture
Video capture
Holographic Emulation
Web Player
Mobile Developer Checklist
Crashes
Profiling
Optimizations
Practical Guide to Optimization for Mobiles
Graphics Methods
Scripting and Gameplay Methods
Rendering Optimizations
Optimizing Scripts
Experimental
Introduction
Overview
The Look Dev view
Control panel
Settings menu
Views menu
HDRI environments in Look Dev
The HDRI view
HDRI menus
Environment Shadow
Legacy Topics
Asset Server (Team License)
Setting up the Asset Server
Legacy Network Reference Guide
High Level Networking Concepts (Legacy)
Networking Elements in Unity (Legacy)
Network Views (Legacy)
Network View
RPC Details (Legacy)
State Synchronization Details (Legacy)
Network Instantiate (Legacy)
Network Level Loading (Legacy)
Master Server (Legacy)
Building the Unity Networking Servers on your own (Legacy)
Minimizing Network Bandwidth (Legacy)
Legacy Asset Bundles
Creating Asset Bundles in Unity 4
Managing Asset Dependencies in Unity 4
Legacy Animation System
Animation
Animation Scripting (Legacy)
Legacy GUI
GUI Text (Legacy UI Component)
GUI Texture (Legacy UI Component)
Legacy Unity Analytics (SDK Workflow)
Basic Integration (SDK)
Import SDK
Create Game Script
Attach Game Script
Play To Validate
Advanced Integration (SDK)
Custom Events
Monetization
Receipt Verification
User Attributes
LegacyUnityRemote
Unity Remote 4
Unity Remote 3 (DEPRECATED)
Expert Guides
Texture Types
You can import different Texture types into the Unity Editor via the Texture Importer.

Below are the properties available to configure the various Texture types in Unity in the Texture Inspector window. Scroll down or select from the list below to find details of the Texture type you wish to learn about.

Default
Normal Map
Editor GUI and Legacy
Sprite (2D and UI)
Cursor
Cookie
Lightmap
Single Channel

Texture type: Default

Texture Inspector window - Texture Type:Default
Texture Inspector window - Texture Type:Default
Property:	Function:
Texture Type	Default is the most common setting used for all Textures. It provides access to most of the properties for Texture importing.
Texture Shape	Use this to define the shape of the Texture. See documentation on the Texture Importer for information on all Texture shapes.
sRGB (Color Texture)	Check this box to specify that the Texture is stored in gamma space. This should always be checked for non-HDR color Textures (such as Albedo and Specular Color). If the Texture stores information that has a specific meaning, and you need the exact values in the Shader (for example, the smoothness or the metalness), uncheck this box. This box is checked by default.
Alpha Source	Use this to specify how the alpha channel of the Texture is generated. This is set to None by default.
    None	The imported Texture does not have an alpha channel, whether or not the input Texture has one.
    Input Texture Alpha	This uses the alpha from the input Texture if a Texture is provided.
    From Gray Scale	This generates the alpha from the mean (average) of the input Texture RGB values.
Alpha is Transparency	If the alpha channel you specify is Transparency, enable Alpha is Transparency to dilate the color and avoid filtering artifacts on the edges.
Advanced
Non Power of 2	If the Texture has a non-power of two (NPOT) dimension size, this defines a scaling behavior at import time. See documentation on Importing Textures for more information on non-power of two sizes. This is set to None by default.
    None	Texture dimension size stays the same.
    To nearest	The Texture is scaled to the nearest power-of-two dimension size at import time. For example, a 257x511 px Texture is scaled to 256x512 px. Note that PVRTC formats require Textures to be square (that is width equal to height), so the final dimension size is upscaled to 512x512 px.
    To larger	The Texture is scaled to the power-of-two dimension size of the largest dimension size value at import time. For example, a 257x511 px Texture is scaled to 512x512 px.
    To smaller	The Texture is scaled to the power-of-two dimension size of the smallest dimension size value at import time. For example, a 257x511 px Texture is scaled to 256x256 px.
Read/Write Enabled	Check this box to enable access to the Texture data from script functions (such as Texture2D.SetPixels, Texture2D.GetPixels and other Texture2D functions). Note that a copy of the Texture data is made, doubling the amount of memory required for Texture Assets, so only use this property if absolutely necessary. This is only valid for uncompressed and DXT compressed Textures; other types of compressed textures cannot be read from. This property is disabled by default.
Generate Mip Maps	Check this box to enable mipmap generation. Mipmaps are smaller versions of the Texture that get used when the Texture is very small on screen. See documentation on Importing Textures for more information on mipmaps.
    Border Mip Maps	Check this box to avoid colors bleeding out to the edge of the lower MIP levels. Used for light cookies (see below). This box is unchecked by default.
    Mip Map Filtering	There are two ways of mipmap filtering available for optimizing image quality. The default option is Box.
        Box	This is the simplest way to fade out mipmaps. The MIP levels become smoother as they go down in dimension size.
        Kaiser	A sharpening algorithm runs on the mipmaps as they go down in dimension size. Try this option if your Textures are too blurry in the distance. (The algorothm is of a Kaiser Window type - see Wikipedia for further information.)
    Fadeout Mip Maps	Enable this to make the mipmaps fade to gray as the MIP levels progress. This is used for detail maps. The left-most scroll is the first MIP level to begin fading out. The right-most scroll defines the MIP level where the Texture is completely grayed out.
Wrap Mode	Select how the Texture behaves when tiled. The default option is Clamp.
    Repeat	The Texture repeats itself in tiles.
    Clamp	The Texture’s edges are stretched.
Filter Mode	Select how the Texture is filtered when it gets stretched by 3D transformations. The default option is Point (no filter).
    Point (no filter)	The Texture appears blocky up close.
    Bilinear	The Texture appears blurry up close.
    Trilinear	Like Bilinear, but the Texture also blurs between the different MIP levels.
Aniso Level	Increases Texture quality when viewing the Texture at a steep angle. Good for floor and ground Textures. See documentation on Importing Textures for more information on Anisotropic filtering.

Texture type: Normal Map

Texture Inspector window - Texture Type:Normal Map
Texture Inspector window - Texture Type:Normal Map
Property:	Function:
Texture Type	Select Normal map to turn the color channels into a format suitable for real-time normal mapping.
Texture Shape	Use this to define the shape of the Texture. See documentation on the Texture Importer for information on all Texture shapes.
Create from Greyscale	This creates the Normal Map from a greyscale heightmap. Check this to enable it and enabled it and see the Bumpiness and Filtering. This option is unchecked by default.
    Bumpiness	Control the amount of bumpiness. A low bumpiness value means that even sharp contrast in the heightmap is translated to gentle angles and bumps. A high value creates exaggerated bumps and very high-contrast lighting responses to the bumps. This option is only visible if Create from Greyscale is checked.
    Filtering	Determine how the bumpiness is calculated:
        Smooth	This generates Normal Maps with standard (forward differences) algorithms.
        Sharp	Also known as a Sobel filter, this generates Normal Maps that are sharper than Standard.
Advanced
Non Power of 2	If the Texture has a non-power of two (NPOT) dimension size, this defines a scaling behavior at import time. See documentation on Importing Textures for more information on non-power of two dimension sizes. This is set to None by default.
    None	Texture dimension size stays the same.
    To nearest	The Texture is scaled to the nearest power-of-two dimension size at import time. For example, a 257x511 px Texture is scaled to 256x512 px. Note that PVRTC formats require Textures to be square (width equal to height), so the final dimension size is upscaled to 512x512 px.
    To larger	The Texture is scaled to the power-of-two dimension size of the largest dimension size value at import time. For example, a 257x511 px Texture is scaled to 512x512 px.
    To smaller	The Texture is scaled to the power-of-two dimension size of the smallest dimension size value at import time. For example, a 257x511 px Texture is scaled to 256x256 px.
Read/Write Enabled	Check this box to enable access to the Texture data from script functions (such as Texture2D.SetPixels, Texture2D.GetPixels and other Texture2D functions). Note that a copy of the Texture data is made, doubling the amount of memory required for Texture Assets, so only use this property if absolutely necessary. This is only valid for uncompressed and DXT compressed Textures; other types of compressed textures cannot be read from. This property is disabled by default.
Generate Mip Maps	Check this box to enable mipmap generation. Mipmaps are smaller versions of the Texture that get used when the Texture is very small on screen. See documentation on Importing Textures for more information on mipmaps.
    Border Mip Maps	Check this box to avoid colors bleeding out to the edge of the lower MIP levels. Used for light cookies (see below). This box is unchecked by default.
    Mip Map Filtering	There are two ways of mipmap filtering available for optimizing image quality. The default option is Box.
        Box	This is the simplest way to fade out mipmaps. The MIP levels become smoother as they go down in dimension size.
        Kaiser	A sharpening algorithm runs on the mipmaps as they go down in dimension size. Try this option if your Textures are too blurry in the distance. (The algorothm is of a Kaiser Window type - see Wikipedia for further information.)
    Fadeout Mip Maps	Enable this to make the mipmaps fade to gray as the MIP levels progress. This is used for detail maps. The left-most scroll is the first MIP level to begin fading out. The right-most scroll defines the MIP level where the Texture is completely grayed out.
Wrap Mode	Select how the Texture behaves when tiled. The default option is Clamp.
    Repeat	The Texture repeats itself in tiles.
    Clamp	The Texture’s edges are stretched.
Filter Mode	Select how the Texture is filtered when it gets stretched by 3D transformations. The default option is Point (no filter).
    Point (no filter)	The Texture appears blocky up close.
    Bilinear	The Texture appears blurry up close.
    Trilinear	Like Bilinear, but the Texture also blurs between the different MIP levels.
Aniso Level	Increases Texture quality when viewing the Texture at a steep angle. Good for floor and ground Textures. See documentation on Importing Textures for more information on Anisotropic filtering.

Texture type: Editor GUI and Legacy GUI

Texture Inspector window - Texture Type:Editor GUI and Legacy GUI
Texture Inspector window - Texture Type:Editor GUI and Legacy GUI
Property:	Function:
Texture Type	Select Editor GUI and Legacy GUI if you are using the Texture on any HUD or GUI controls.
Texture Shape	Use this to define the shape of the Texture. See documentation on the Texture Importer for information on all Texture shapes.
Advanced
Alpha Source	Use this to specify how the alpha channel of the Texture is generated. This is set to None by default.
    None	The imported Texture does not have an alpha channel, whether or not the input Texture has one.
    Input Texture Alpha	This uses the alpha from the input Texture if a Texture is provided.
    From Gray Scale	This generates the alpha from the mean (average) of the input Texture RGB values.
Alpha is Transparency	If the alpha channel you specify is Transparency, enable Alpha is Transparency to dilate the color and avoid filtering artifacts on the edges.
Non Power of 2	If the Texture has a non-power of two (NPOT) dimension size, this defines a scaling behavior at import time. See documentation on Importing Textures for more information on non-power of two sizes. This is set to None by default.
    None	Texture dimension size stays the same.
    To nearest	The Texture is scaled to the nearest power-of-two dimension size at import time. For example, a 257x511 px Texture is scaled to 256x512 px. Note that PVRTC formats require Textures to be square (width equal to height), so the final dimension size is upscaled to 512x512 px.
    To larger	The Texture is scaled to the power-of-two dimension size of the largest dimension size value at import time. For example, a 257x511 px Texture is scaled to 512x512 px.
    To smaller	The Texture is scaled to the power-of-two dimension size of the smallest dimension size value at import time. For example, a 257x511px Texture is scaled to 256x256.
Read/Write Enabled	Check this box to enable access to the Texture data from script functions (such as Texture2D.SetPixels, Texture2D.GetPixels and other Texture2D functions). Note that a copy of the Texture data is made, doubling the amount of memory required for Texture Assets, so only use this property if absolutely necessary. This is only valid for uncompressed and DXT compressed Textures; other types of compressed textures cannot be read from. This property is disabled by default.
Generate Mip Maps	Check this box to enable mipmap generation. Mipmaps are smaller versions of the Texture that get used when the Texture is very small on screen. See documentation on Importing Textures for more information on mipmaps.
    Border Mip Maps	Check this box to avoid colors bleeding out to the edge of the lower MIP levels. Used for light cookies (see below). This box is unchecked by default.
    Mip Map Filtering	There are two ways of mipmap filtering available for optimizing image quality. The default option is Box.
        Box	This is the simplest way to fade out mipmaps. The MIP levels become smoother as they go down in dimension size.
        Kaiser	A sharpening algorithm runs on the mipmaps as they go down in dimension size. Try this option if your Textures are too blurry in the distance. (The algorothm is of a Kaiser Window type - see Wikipedia for further information.)
    Fadeout Mip Maps	Enable this to make the mipmaps fade to gray as the MIP levels progress. This is used for detail maps. The left-most scroll is the first MIP level to begin fading out. The right-most scroll defines the MIP level where the Texture is completely grayed out.
Wrap Mode	Select how the Texture behaves when tiled. The default option is Clamp.
    Repeat	The Texture repeats itself in tiles.
    Clamp	The Texture’s edges are stretched.
Filter Mode	Select how the Texture is filtered when it gets stretched by 3D transformations. The default option is Point (no filter).
    Point (no filter)	The Texture appears blocky up close.
    Bilinear	The Texture appears blurry up close.
    Trilinear	Like Bilinear, but the Texture also blurs between the different MIP levels.
Aniso Level	Increases Texture quality when viewing the Texture at a steep angle. Good for floor and ground Textures. See documentation on Importing Textures for more information on Anisotropic filtering.

Texture type: Sprite (2D and UI)

Texture Inspector window - Texture Type:Sprite (2D and UI)
Texture Inspector window - Texture Type:Sprite (2D and UI)
Property:	Function:
Texture Type	Select Sprite (2D and UI) if you are using the Texture in a 2D game as a Sprite.
Texture Shape	Use this to define the shape of the Texture. See documentation on the Texture Importer for information on all Texture shapes.
Sprite mode	Use this setting to specify how the the Sprite graphic is extracted from the image. The default for this option is Single.
    Single	Use the Sprite image in isolation.
    Multiple	Keep multiple related Sprites together in the same image (for example, animation frames or separate Sprite elements that belong to a single game character).
Packing Tag	Specify by name a Sprite atlas which you want to pack this Texture into.
Pixels Per Unit	The number of pixels of width/height in the Sprite image that correspond to one distance unit in world space.
Mesh Type	This defines the Mesh type that is generated for the Sprite. The default for this option is Tight. See the example images below for a comparison of the two Mesh types.
    Full Rect	This creates a quad to map the Sprite onto it.
    Tight	This generates a Mesh based on pixel alpha value. The Mesh generated generally follows the shape of the Sprite. 
Note: Any Sprite that is smaller than 32x32 uses Full Rect, even when Tight is specified.
Extrude Edges	Use the slider to determine how much area to leave around the Sprite in the generated Mesh. See the example images below for a comparison of two Extrude Edges values.
Pivot	The location in the image where the Sprite’s local coordinate system originates. Choose one of the pre-set options, or select Custom to set your own Pivot location.
    Custom	Define the X and Y to set a custom Pivot location in the image.
Advanced
sRGB (Color Texture)	Use this to specify whether or not the Texture is stored in gamma space. This should be the case for all non-HDR color Textures (such as Albedo and Specular Color). If the Texture stores information that has a specific meaning, and you need the exact values in the Shader (for example, the smoothness or the metalness), uncheck this box.
Alpha Source	Use this to specify how the alpha channel of the Texture is generated.
    None	The imported Texture does not have an alpha channel, whether or not the input Texture has one.
    Input Texture Alpha	This uses the alpha from the input Texture. This option does not appear in the menu if there is no alpha in the imported Texture.
    From Gray Scale	This generates the alpha from the mean (average) of the input Texture RGB values.
Alpha is Transparency	If the provided alpha channel is Transparency, enable Alpha is Transparency to dilate the color and avoid filtering artifacts on the edges.
Read/Write Enabled	Check this box to enable access to the Texture data from script functions (such as Texture2D.SetPixels, Texture2D.GetPixels and other Texture2D functions). Note however that a copy of the Texture data will be made, doubling the amount of memory required for Texture Asset, so only use this property if absolutely necessary. This is only valid for uncompressed and DXT compressed Textures; other types of compressed textures cannot be read from. This property is disabled by default.
Generate Mip Maps	Check this box to enable mipmap generation. Mipmaps are smaller versions of the Texture that get used when the Texture is very small on screen. See the Details section at the end of the page.
    Border Mip Maps	Select this to avoid colors bleeding out to the edge of the lower MIP levels. Used for Light Cookies (see below).
    Mip Map Filtering	There are two ways of mipmap filtering available for optimizing image quality:
        Box	This is the simplest way to fade out mipmaps. The MIP levels become smoother as they go down in dimension size.
        Kaiser	A sharpening algorithm runs on the mipmaps as they go down in dimension size. Try this option if your Textures are too blurry in the distance. (The algorothm is of a Kaiser Window type - see Wikipedia for further information.)
    Fadeout Mip Maps	Enable this to make the mipmaps fade to gray as the MIP levels progress. This is used for detail maps. The left-most scroll is the first MIP level to begin fading out. The right-most scroll defines the MIP level where the Texture is completely grayed out.
Wrap Mode	Select how the Texture behaves when tiled:
    Repeat	The Texture repeats itself in tiles.
    Clamp	The Texture’s edges are stretched.
Filter Mode	Select how the Texture is filtered when it gets stretched by 3D transformations:
    Point	The Texture appears blocky up close.
    Bilinear	The Texture appears blurry up close.
    Trilinear	Like Bilinear, but the Texture also blurs between the different MIP levels.
Aniso Level	Increases Texture quality when viewing the Texture at a steep angle. Good for floor and ground Textures. See the Details section at the end of the page.
Example: Mesh type

|
Full Rect|
Tight Mesh | |:—|:—|

Example: Extrude Edges

|
Extrude Edges = 0|
Extrude Edges = 32 | |:—|:—|


Texture type: Cursor

Texture Inspector window - Texture Type:Cursor 
Texture Inspector window - Texture Type:Cursor
Property:	Function:
Texture Type	Select Cursor if you are using the Texture as a custom cursor.
Texture Shape	Use this to define the shape of the Texture. See documentation on the Texture Importer for information on all Texture shapes.
Advanced
Alpha Source	Use this to specify how the alpha channel of the Texture is generated. This is set to None by default.
    None	The imported Texture does not have an alpha channel, whether or not the input Texture has one.
    Input Texture Alpha	This uses the alpha from the input Texture if a Texture is provided.
    From Gray Scale	This generates the alpha from the mean (average) of the input Texture RGB values.
Alpha is Transparency	If the alpha channel you specify is Transparency, enable Alpha is Transparency to dilate the color and avoid filtering artifacts on the edges.
Non Power of 2	If the Texture has a non-power of two (NPOT) dimension size, this defines a scaling behavior at import time. See documentation on Importing Textures for more information on non-power of two dimension sizes. This is set to None by default.
    None	Texture dimension size stays the same.
    To nearest	The Texture is scaled to the nearest power-of-two dimension size at import time. For example, a 257x511 px Texture is scaled to 256x512 px. Note that PVRTC formats require Textures to be square (width equal to height), so the final dimension size is upscaled to 512x512 px.
    To larger	The Texture is scaled to the power-of-two dimension size of the largest dimension size value at import time. For example, a 257x511 px Texture is scaled to 512x512 px.
    To smaller	The Texture is scaled to the power-of-two dimension size of the smallest dimension size value at import time. For example, a 257x511 px Texture is scaled to 256x256.
Read/Write Enabled	Check this box to enable access to the Texture data from script functions (such as Texture2D.SetPixels, Texture2D.GetPixels and other Texture2D functions). Note that a copy of the Texture data is made, doubling the amount of memory required for Texture Assets, so only use this property if absolutely necessary. This is only valid for uncompressed and DXT compressed Textures; other types of compressed textures cannot be read from. This property is disabled by default.
Generate Mip Maps	Check this box to enable mipmap generation. Mipmaps are smaller versions of the Texture that get used when the Texture is very small on screen. See documentation on Importing Textures for more information on mipmaps.
    Border Mip Maps	Check this box to avoid colors bleeding out to the edge of the lower MIP levels. Used for light cookies (see below). This box is unchecked by default.
    Mip Map Filtering	There are two ways of mipmap filtering available for optimizing image quality. The default option is Box.
        Box	This is the simplest way to fade out mipmaps. The MIP levels become smoother as they go down in dimension size.
        Kaiser	A sharpening algorithm runs on the mipmaps as they go down in dimension size. Try this option if your Textures are too blurry in the distance. (The algorothm is of a Kaiser Window type - see Wikipedia for further information.)
    Fadeout Mip Maps	Enable this to make the mipmaps fade to gray as the MIP levels progress. This is used for detail maps. The left-most scroll is the first MIP level to begin fading out. The right-most scroll defines the MIP level where the Texture is completely grayed out.
Wrap Mode	Select how the Texture behaves when tiled. The default option is Clamp.
    Repeat	The Texture repeats itself in tiles.
    Clamp	The Texture’s edges are stretched.
Filter Mode	Select how the Texture is filtered when it gets stretched by 3D transformations. The default option is Point (no filter).
    Point (no filter)	The Texture appears blocky up close.
    Bilinear	The Texture appears blurry up close.
    Trilinear	Like Bilinear, but the Texture also blurs between the different MIP levels.
Aniso Level	Increases Texture quality when viewing the Texture at a steep angle. Good for floor and ground Textures. See documentation on Importing Textures for more information on Anisotropic filtering.

Texture type: Cookie

Texture Inspector window - Texture Type:Cookie 
Texture Inspector window - Texture Type:Cookie
Property:	Function:
Texture Type	Select Cookie to set your Texture up with the basic parameters used for the Cookies of your Scene’s Lights.
Texture Shape	Use this to define the shape of the Texture. See documentation on the Texture Importer for information on all Texture shapes.
Light Type	Define the type of Light that the Texture is applied to. Directional and Spotlight cookies must be 2D Textures, and Point Light Cookies must be cubemaps. The system automatically enforces the right shape depending on the Light type. 
For Directional Lights this Texture tiles, so in the Texture inspector set the Edge Mode to Repeat. 
For Spotlights, keep the edges of your Cookie Texture solid black to get the proper effect. In the Texture Inspector, set the Edge Mode to Clamp.
Alpha is Transparency	If the alpha channel you specify is Transparency, enable Alpha is Transparency to dilate the color and avoid filtering artifacts on the edges.
Advanced
Non Power of 2	If the Texture has a non-power of two (NPOT) dimension size, this defines a scaling behavior at import time. See documentation on Importing Textures for more information on non-power of two sizes. This is set to None by default.
    None	Texture dimension size stays the same.
    To nearest	The Texture is scaled to the nearest power-of-two dimension size at import time. For example, a 257x511 px Texture is scaled to 256x512 px. Note that PVRTC formats require Textures to be square (width equal to height), so the final dimension size is upscaled to 512x512 px.
    To larger	The Texture is scaled to the power-of-two dimension size of the largest dimension size value at import time. For example, a 257x511 px Texture is scaled to 512x512 px.
    To smaller	The Texture is scaled to the power-of-two dimension size of the smallest dimension size value at import time. For example, a 257x511 px Texture is scaled to 256x256.
Read/Write Enabled	Check this box to enable access to the Texture data from script functions (such as Texture2D.SetPixels, Texture2D.GetPixels and other Texture2D functions). Note that a copy of the Texture data is made, doubling the amount of memory required for Texture Assets, so only use this property if absolutely necessary. This is only valid for uncompressed and DXT compressed Textures; other types of compressed textures cannot be read from. This property is disabled by default.
Generate Mip Maps	Check this box to enable mipmap generation. Mipmaps are smaller versions of the Texture that get used when the Texture is very small on screen. See documentation on Importing Textures for more information on mipmaps.
    Border Mip Maps	Check this box to avoid colors bleeding out to the edge of the lower MIP levels. Used for light cookies (see below). This box is unchecked by default.
    Mip Map Filtering	There are two ways of mipmap filtering available for optimizing image quality. The default option is Box.
        Box	This is the simplest way to fade out mipmaps. The MIP levels become smoother as they go down in dimension size.
        Kaiser	A sharpening algorithm runs on the mipmaps as they go down in dimension size. Try this option if your Textures are too blurry in the distance. (The algorothm is of a Kaiser Window type - see Wikipedia for further information.)
    Fadeout Mip Maps	Enable this to make the mipmaps fade to gray as the MIP levels progress. This is used for detail maps. The left-most scroll is the first MIP level to begin fading out. The right-most scroll defines the MIP level where the Texture is completely grayed out.
Wrap Mode	Select how the Texture behaves when tiled. The default option is Clamp.
    Repeat	The Texture repeats itself in tiles.
    Clamp	The Texture’s edges are stretched.
Filter Mode	Select how the Texture is filtered when it gets stretched by 3D transformations. The default option is Point (no filter).
    Point (no filter)	The Texture appears blocky up close.
    Bilinear	The Texture appears blurry up close.
    Trilinear	Like Bilinear, but the Texture also blurs between the different MIP levels.
Aniso Level	Increases Texture quality when viewing the Texture at a steep angle. Good for floor and ground Textures. See documentation on Importing Textures for more information on Anisotropic filtering.

Texture type: Lightmap

Texture Inspector window - Texture Type:Lightmap
Texture Inspector window - Texture Type:Lightmap
Property:	Function:
Texture Type	Select Lightmap if you are using the Texture as a Lightmap. This option enables encoding into a specific format (RGBM or dLDR, depending on the platform) and a post-processing step on Texture data (a push-pull dilation pass).
Texture Shape	Use this to define the shape of the Texture. See documentation on the Texture Importer for information on all Texture shapes.
Advanced
Non Power of 2	If the Texture has a non-power of two (NPOT) dimension size, this defines a scaling behavior at import time. See documentation on Importing Textures for more information on non-power of two dimension sizes. This is set to None by default.
    None	Texture size stays the same.
    To nearest	The Texture is scaled to the nearest power-of-two dimension size at import time. For example, a 257x511 px Texture is scaled to 256x512 px. Note that PVRTC formats require Textures to be square (width equal to height), so the final dimension size is upscaled to 512x512 px.
    To larger	The Texture is scaled to the power-of-two dimension size of the largest size value at import time. For example, a 257x511 px Texture is scaled to 512x512 px.
    To smaller	The Texture is scaled to the power-of-two dimension size of the smallest dimension size value at import time. For example, a 257x511 px Texture is scaled to 256x256 px.
Read/Write Enabled	Check this box to enable access to the Texture data from script functions (such as Texture2D.SetPixels, Texture2D.GetPixels and other Texture2D functions). Note that a copy of the Texture data is made, doubling the amount of memory required for Texture Assets, so only use this property if absolutely necessary. This is only valid for uncompressed and DXT compressed Textures; other types of compressed textures cannot be read from. This property is disabled by default.
Generate Mip Maps	Check this box to enable mipmap generation. Mipmaps are smaller versions of the Texture that get used when the Texture is very small on screen. See documentation on Importing Textures for more information on mipmaps.
    Border Mip Maps	Check this box to avoid colors bleeding out to the edge of the lower MIP levels. Used for light cookies (see below). This box is unchecked by default.
    Mip Map Filtering	There are two ways of mipmap filtering available for optimizing image quality. The default option is Box.
        Box	This is the simplest way to fade out mipmaps. The MIP levels become smoother as they go down in dimension size.
        Kaiser	A sharpening algorithm runs on the mipmaps as they go down in dimension size. Try this option if your Textures are too blurry in the distance. (The algorothm is of a Kaiser Window type - see Wikipedia for further information.)
    Fadeout Mip Maps	Enable this to make the mipmaps fade to gray as the MIP levels progress. This is used for detail maps. The left-most scroll is the first MIP level to begin fading out. The right-most scroll defines the MIP level where the Texture is completely grayed out.
Wrap Mode	Select how the Texture behaves when tiled. The default option is Clamp.
    Repeat	The Texture repeats itself in tiles.
    Clamp	The Texture’s edges are stretched.
Filter Mode	Select how the Texture is filtered when it gets stretched by 3D transformations. The default option is Point (no filter).
    Point (no filter)	The Texture appears blocky up close.
    Bilinear	The Texture appears blurry up close.
    Trilinear	Like Bilinear, but the Texture also blurs between the different MIP levels.
Aniso Level	Increases Texture quality when viewing the Texture at a steep angle. Good for floor and ground Textures. See documentation on Importing Textures for more information on Anisotropic filtering.

Texture type: Single Channel

Texture Inspector window - Texture Type:Single Channel
Texture Inspector window - Texture Type:Single Channel
Property:	Function:
Texture Type	Select Single Channel if you only need one channel in the Texture.
Texture Shape	Use this to define the shape of the Texture. See documentation on the Texture Importer for information on all Texture shapes.
Alpha Source	Use this to specify how the alpha channel of the Texture is generated. This is set to None by default.
    None	The imported Texture does not have an alpha channel, whether or not the input Texture has one.
    Input Texture Alpha	This uses the alpha from the input Texture if a Texture is provided.
    From Gray Scale	This generates the alpha from the mean (average) of the input Texture RGB values.
Alpha is Transparency	If the alpha channel you specify is Transparency, enable Alpha is Transparency to dilate the color and avoid filtering artifacts on the edges.
Advanced
Non Power of 2	If the Texture has a non-power of two (NPOT) dimension size, this defines a scaling behavior at import time. See documentation on Importing Textures for more information on non-power of two sizes. This is set to None by default.
    None	Texture dimension size stays the same.
    To nearest	The Texture is scaled to the nearest power-of-two dimension size at import time. For example, a 257x511 px Texture is scaled to 256x512 px. Note that PVRTC formats require Textures to be square (width equal to height), so the final dimension size is upscaled to 512x512 px.
    To larger	The Texture is scaled to the power-of-two dimension size of the largest dimension size value at import time. For example, a 257x511 px Texture is scaled to 512x512 px.
    To smaller	The Texture is scaled to the power-of-two dimension size of the smallest size value at import time. For example, a 257x511 px Texture is scaled to 256x256.
Read/Write Enabled	Check this box to enable access to the Texture data from script functions (such as Texture2D.SetPixels, Texture2D.GetPixels and other Texture2D functions). Note that a copy of the Texture data is made, doubling the amount of memory required for Texture Assets, so only use this property if absolutely necessary. This is only valid for uncompressed and DXT compressed Textures; other types of compressed textures cannot be read from. This property is disabled by default.
Generate Mip Maps	Check this box to enable mipmap generation. Mipmaps are smaller versions of the Texture that get used when the Texture is very small on screen. See documentation on Importing Textures for more information on mipmaps.
    Border Mip Maps	Check this box to avoid colors bleeding out to the edge of the lower MIP levels. Used for light cookies (see below). This box is unchecked by default.
    Mip Map Filtering	There are two ways of mipmap filtering available for optimizing image quality. The default option is Box.
        Box	This is the simplest way to fade out mipmaps. The MIP levels become smoother as they go down in dimension size.
        Kaiser	A sharpening algorithm runs on the mipmaps as they go down in dimension size. Try this option if your Textures are too blurry in the distance. (The algorothm is of a Kaiser Window type - see Wikipedia for further information.)
    Fadeout Mip Maps	Enable this to make the mipmaps fade to gray as the MIP levels progress. This is used for detail maps. The left-most scroll is the first MIP level to begin fading out. The right-most scroll defines the MIP level where the Texture is completely grayed out.
Wrap Mode	Select how the Texture behaves when tiled. The default option is Clamp.
    Repeat	The Texture repeats itself in tiles.
    Clamp	The Texture’s edges are stretched.
Filter Mode	Select how the Texture is filtered when it gets stretched by 3D transformations. The default option is Point (no filter).
    Point (no filter)	The Texture appears blocky up close.
    Bilinear	The Texture appears blurry up close.
    Trilinear	Like Bilinear, but the Texture also blurs between the different MIP levels.
Aniso Level	Increases Texture quality when viewing the Texture at a steep angle. Good for floor and ground Textures. See documentation on Importing Textures for more information on Anisotropic filtering.

YAML Class ID Reference
A reference of common class ID numbers used by the YAML file format is given below, both in numerical order of class IDs and alphabetical order of class names. Note that some ranges of numbers are intentionally omitted from the sequence - these may represent classes that have been removed from the API or may be reserved for future use. Classes defined from scripts will always have class ID 114 (MonoBehaviour).

Classes Ordered by ID Number

ID	Class
1	GameObject
2	Component
3	LevelGameManager
4	Transform
5	TimeManager
6	GlobalGameManager
8	Behaviour
9	GameManager
11	AudioManager
12	ParticleAnimator
13	InputManager
15	EllipsoidParticleEmitter
17	Pipeline
18	EditorExtension
19	Physics2DSettings
20	Camera
21	Material
23	MeshRenderer
25	Renderer
26	ParticleRenderer
27	Texture
28	Texture2D
29	SceneSettings
30	GraphicsSettings
33	MeshFilter
41	OcclusionPortal
43	Mesh
45	Skybox
47	QualitySettings
48	Shader
49	TextAsset
50	Rigidbody2D
51	Physics2DManager
53	Collider2D
54	Rigidbody
55	PhysicsManager
56	Collider
57	Joint
58	CircleCollider2D
59	HingeJoint
60	PolygonCollider2D
61	BoxCollider2D
62	PhysicsMaterial2D
64	MeshCollider
65	BoxCollider
66	SpriteCollider2D
68	EdgeCollider2D
72	ComputeShader
74	AnimationClip
75	ConstantForce
76	WorldParticleCollider
78	TagManager
81	AudioListener
82	AudioSource
83	AudioClip
84	RenderTexture
87	MeshParticleEmitter
88	ParticleEmitter
89	Cubemap
90	Avatar
91	AnimatorController
92	GUILayer
93	RuntimeAnimatorController
94	ScriptMapper
95	Animator
96	TrailRenderer
98	DelayedCallManager
102	TextMesh
104	RenderSettings
108	Light
109	CGProgram
110	BaseAnimationTrack
111	Animation
114	MonoBehaviour
115	MonoScript
116	MonoManager
117	Texture3D
118	NewAnimationTrack
119	Projector
120	LineRenderer
121	Flare
122	Halo
123	LensFlare
124	FlareLayer
125	HaloLayer
126	NavMeshAreas
127	HaloManager
128	Font
129	PlayerSettings
130	NamedObject
131	GUITexture
132	GUIText
133	GUIElement
134	PhysicMaterial
135	SphereCollider
136	CapsuleCollider
137	SkinnedMeshRenderer
138	FixedJoint
140	RaycastCollider
141	BuildSettings
142	AssetBundle
143	CharacterController
144	CharacterJoint
145	SpringJoint
146	WheelCollider
147	ResourceManager
148	NetworkView
149	NetworkManager
150	PreloadData
152	MovieTexture
153	ConfigurableJoint
154	TerrainCollider
155	MasterServerInterface
156	TerrainData
157	LightmapSettings
158	WebCamTexture
159	EditorSettings
160	InteractiveCloth
161	ClothRenderer
162	EditorUserSettings
163	SkinnedCloth
164	AudioReverbFilter
165	AudioHighPassFilter
166	AudioChorusFilter
167	AudioReverbZone
168	AudioEchoFilter
169	AudioLowPassFilter
170	AudioDistortionFilter
171	SparseTexture
180	AudioBehaviour
181	AudioFilter
182	WindZone
183	Cloth
184	SubstanceArchive
185	ProceduralMaterial
186	ProceduralTexture
191	OffMeshLink
192	OcclusionArea
193	Tree
194	NavMeshObsolete
195	NavMeshAgent
196	NavMeshSettings
197	LightProbesLegacy
198	ParticleSystem
199	ParticleSystemRenderer
200	ShaderVariantCollection
205	LODGroup
206	BlendTree
207	Motion
208	NavMeshObstacle
210	TerrainInstance
212	SpriteRenderer
213	Sprite
214	CachedSpriteAtlas
215	ReflectionProbe
216	ReflectionProbes
218	Terrain
220	LightProbeGroup
221	AnimatorOverrideController
222	CanvasRenderer
223	Canvas
224	RectTransform
225	CanvasGroup
226	BillboardAsset
227	BillboardRenderer
228	SpeedTreeWindAsset
229	AnchoredJoint2D
230	Joint2D
231	SpringJoint2D
232	DistanceJoint2D
233	HingeJoint2D
234	SliderJoint2D
235	WheelJoint2D
238	NavMeshData
240	AudioMixer
241	AudioMixerController
243	AudioMixerGroupController
244	AudioMixerEffectController
245	AudioMixerSnapshotController
246	PhysicsUpdateBehaviour2D
247	ConstantForce2D
248	Effector2D
249	AreaEffector2D
250	PointEffector2D
251	PlatformEffector2D
252	SurfaceEffector2D
258	LightProbes
271	SampleClip
272	AudioMixerSnapshot
273	AudioMixerGroup
290	AssetBundleManifest
1001	Prefab
1002	EditorExtensionImpl
1003	AssetImporter
1004	AssetDatabase
1005	Mesh3DSImporter
1006	TextureImporter
1007	ShaderImporter
1008	ComputeShaderImporter
1011	AvatarMask
1020	AudioImporter
1026	HierarchyState
1027	GUIDSerializer
1028	AssetMetaData
1029	DefaultAsset
1030	DefaultImporter
1031	TextScriptImporter
1032	SceneAsset
1034	NativeFormatImporter
1035	MonoImporter
1037	AssetServerCache
1038	LibraryAssetImporter
1040	ModelImporter
1041	FBXImporter
1042	TrueTypeFontImporter
1044	MovieImporter
1045	EditorBuildSettings
1046	DDSImporter
1048	InspectorExpandedState
1049	AnnotationManager
1050	PluginImporter
1051	EditorUserBuildSettings
1052	PVRImporter
1053	ASTCImporter
1054	KTXImporter
1101	AnimatorStateTransition
1102	AnimatorState
1105	HumanTemplate
1107	AnimatorStateMachine
1108	PreviewAssetType
1109	AnimatorTransition
1110	SpeedTreeImporter
1111	AnimatorTransitionBase
1112	SubstanceImporter
1113	LightmapParameters
1120	LightmapSnapshot
Classes Ordered Alphabetically

Class	ID
ASTCImporter	1053
AnchoredJoint2D	229
Animation	111
AnimationClip	74
Animator	95
AnimatorController	91
AnimatorOverrideController	221
AnimatorState	1102
AnimatorStateMachine	1107
AnimatorStateTransition	1101
AnimatorTransitionBase	1111
AnimatorTransition	1109
AnnotationManager	1049
AreaEffector2D	249
AssetBundle	142
AssetBundleManifest	290
AssetDatabase	1004
AssetImporter	1003
AssetMetaData	1028
AssetServerCache	1037
AudioBehaviour	180
AudioChorusFilter	166
AudioClip	83
AudioDistortionFilter	170
AudioEchoFilter	168
AudioFilter	181
AudioHighPassFilter	165
AudioImporter	1020
AudioListener	81
AudioLowPassFilter	169
AudioManager	11
AudioMixer	240
AudioMixerController	241
AudioMixerEffectController	244
AudioMixerGroup	273
AudioMixerGroupController	243
AudioMixerSnapshot	272
AudioMixerSnapshotController	245
AudioReverbFilter	164
AudioReverbZone	167
AudioSource	82
Avatar	90
AvatarMask	1011
BaseAnimationTrack	110
Behaviour	8
BillboardAsset	226
BillboardRenderer	227
BlendTree	206
BoxCollider	65
BoxCollider2D	61
BuildSettings	141
CachedSpriteAtlas	214
Camera	20
Canvas	223
CanvasGroup	225
CanvasRenderer	222
CapsuleCollider	136
CGProgram	109
CharacterController	143
CharacterJoint	144
CircleCollider2D	58
Cloth	183
ClothRenderer	161
Collider	56
Collider2D	53
Component	2
ComputeShader	72
ComputeShaderImporter	1008
ConfigurableJoint	153
ConstantForce	75
ConstantForce2D	247
Cubemap	89
DDSImporter	1046
DefaultAsset	1029
DefaultImporter	1030
DelayedCallManager	98
DistanceJoint2D	232
EdgeCollider2D	68
EditorBuildSettings	1045
EditorExtension	18
EditorExtensionImpl	1002
EditorSettings	159
EditorUserBuildSettings	1051
EditorUserSettings	162
Effector2D	248
EllipsoidParticleEmitter	15
FBXImporter	1041
FixedJoint	138
Flare	121
FlareLayer	124
Font	128
GameManager	9
GameObject	1
GlobalGameManager	6
GraphicsSettings	30
GUIDSerializer	1027
GUIElement	133
GUILayer	92
GUIText	132
GUITexture	131
Halo	122
HaloLayer	125
HaloManager	127
HierarchyState	1026
HingeJoint	59
HingeJoint2D	233
HumanTemplate	1105
InputManager	13
InspectorExpandedState	1048
InteractiveCloth	160
Joint	57
Joint2D	230
KTXImporter	1054
LensFlare	123
LevelGameManager	3
LibraryAssetImporter	1038
Light	108
LightmapParameters	1113
LightmapSettings	157
LightmapSnapshot	1120
LightProbeGroup	220
LightProbes	258
LightProbesLegacy	197
LineRenderer	120
LODGroup	205
MasterServerInterface	155
Material	21
Mesh	43
Mesh3DSImporter	1005
MeshCollider	64
MeshFilter	33
MeshParticleEmitter	87
MeshRenderer	23
ModelImporter	1040
MonoBehaviour	114
MonoImporter	1035
MonoManager	116
MonoScript	115
Motion	207
MovieImporter	1044
MovieTexture	152
NamedObject	130
NativeFormatImporter	1034
NavMeshAgent	195
NavMeshAreas	126
NavMeshData	238
NavMeshObsolete	194
NavMeshObstacle	208
NavMeshSettings	196
NetworkManager	149
NetworkView	148
NewAnimationTrack	118
OcclusionArea	192
OcclusionPortal	41
OffMeshLink	191
ParticleAnimator	12
ParticleEmitter	88
ParticleRenderer	26
ParticleSystem	198
ParticleSystemRenderer	199
PhysicMaterial	134
Physics2DManager	51
Physics2DSettings	19
PhysicsManager	55
PhysicsMaterial2D	62
PhysicsUpdateBehaviour2D	246
Pipeline	17
PlatformEffector2D	251
PlayerSettings	129
PluginImporter	1050
PointEffector2D	250
PolygonCollider2D	60
Prefab	1001
PreloadData	150
PreviewAssetType	1108
ProceduralMaterial	185
ProceduralTexture	186
Projector	119
PVRImporter	1052
QualitySettings	47
RaycastCollider	140
RectTransform	224
ReflectionProbe	215
ReflectionProbes	216
Renderer	25
RenderSettings	104
RenderTexture	84
ResourceManager	147
Rigidbody	54
Rigidbody2D	50
RuntimeAnimatorController	93
SampleClip	271
SceneAsset	1032
SceneSettings	29
ScriptMapper	94
Shader	48
ShaderImporter	1007
ShaderVariantCollection	200
SkinnedCloth	163
SkinnedMeshRenderer	137
Skybox	45
SliderJoint2D	234
SparseTexture	171
SphereCollider	135
SpringJoint	145
SpringJoint2D	231
Sprite	213
SpriteCollider2D	66
SpriteRenderer	212
SpeedTreeImporter	1110
SpeedTreeWindAsset	228
SubstanceArchive	184
SubstanceImporter	1112
SurfaceEffector2D	252
TagManager	78
Terrain	218
TerrainCollider	154
TerrainData	156
TerrainInstance	210
TextAsset	49
TextMesh	102
TextScriptImporter	1031
Texture	27
Texture2D	28
Texture3D	117
TextureImporter	1006
TimeManager	5
TrailRenderer	96
Transform	4
Tree	193
TrueTypeFontImporter	1042
WebCamTexture	158
WheelCollider	146
WheelJoint2D	235
WindZone	182
WorldParticleCollider	76

Vertex and fragment shader examples
This page contains vertex and fragment program examples. For a basic introduction to shaders, see the shader tutorials: Part 1 and Part 2. For an easy way of writing regular material shaders, see Surface Shaders.

You can download the examples shown below as a zipped Unity project.

Setting up the scene

If you are not familiar with Unity’s Scene View, Hierarchy View, Project View and Inspector, now would be a good time to read the first few sections from the manual, starting with Unity Basics.

The first step is to create some objects which you will use to test your shaders. Select Game Object > 3D Object > Capsule in the main menu. Then position the camera so it shows the capsule. Double-click the Capsule in the Hierarchy to focus the scene view on it, then select the Main Camera object and click Game object > Align with View from the main menu.


Create a new Material by selecting Create > Material from the menu in the Project View. A new material called New Material will appear in the Project View.


Creating a shader

Now create a new Shader asset in a similar way. Select Create > Shader > Unlit Shader from the menu in the Project View. This creates a basic shader that just displays a texture without any lighting.


Other entries in the Create > Shader menu create barebone shaders or other types, for example a basic surface shader.

Linking the mesh, material and shader

Make the material use the shader via the material’s inspector, or just drag the shader asset over the material asset in the Project View. The material inspector will display a white sphere when it uses this shader.


Now drag the material onto your mesh object in either the Scene or the Hierarchy views. Alternatively, select the object, and in the inspector make it use the material in the Mesh Renderer component’s Materials slot.


With these things set up, you can now begin looking at the shader code, and you will see the results of your changes to the shader on the capsule in the Scene View.

Main parts of the shader

To begin examining the code of the shader, double-click the shader asset in the Project View. The shader code will open in your script editor (MonoDevelop or Visual Studio).

The shader starts off with this code:

Shader "Unlit/NewUnlitShader"
{
    Properties
    {
        _MainTex ("Texture", 2D) = "white" {}
    }
    SubShader
    {
        Tags { "RenderType"="Opaque" }
        LOD 100

        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            // make fog work
            #pragma multi_compile_fog
            
            #include "UnityCG.cginc"

            struct appdata
            {
                float4 vertex : POSITION;
                float2 uv : TEXCOORD0;
            };

            struct v2f
            {
                float2 uv : TEXCOORD0;
                UNITY_FOG_COORDS(1)
                float4 vertex : SV_POSITION;
            };

            sampler2D _MainTex;
            float4 _MainTex_ST;
            
            v2f vert (appdata v)
            {
                v2f o;
                o.vertex = UnityObjectToClipPos(v.vertex);
                o.uv = TRANSFORM_TEX(v.uv, _MainTex);
                UNITY_TRANSFER_FOG(o,o.vertex);
                return o;
            }
            
            fixed4 frag (v2f i) : SV_Target
            {
                // sample the texture
                fixed4 col = tex2D(_MainTex, i.uv);
                // apply fog
                UNITY_APPLY_FOG(i.fogCoord, col);
                return col;
            }
            ENDCG
        }
    }
}
This initial shader does not look very simple! But don’t worry, we will go over each part step-by-step.

Let’s see the main parts of our simple shader.

Shader
The Shader command contains a string with the name of the shader. You can use forward slash characters “/” to place your shader in sub-menus when selecting your shader in the Material inspector.

Properties
The Properties block contains shader variables (textures, colors etc.) that will be saved as part of the Material, and displayed in the material inspector. In our unlit shader template, there is a single texture property declared.

SubShader
A Shader can contain one or more SubShaders, which are primarily used to implement shaders for different GPU capabilities. In this tutorial we’re not much concerned with that, so all our shaders will contain just one SubShader.

Pass
Each SubShader is composed of a number of passes, and each Pass represents an execution of the vertex and fragment code for the same object rendered with the material of the shader. Many simple shaders use just one pass, but shaders that interact with lighting might need more (see Lighting Pipeline for details). Commands inside Pass typically setup fixed function state, for example blending modes.

CGPROGRAM .. ENDCG
These keywords surround portions of HLSL code within the vertex and fragment shaders. Typically this is where most of the interesting code is. See vertex and fragment shaders for details.

Simple unlit shader

The unlit shader template does a few more things than would be absolutely needed to display an object with a texture. For example, it supports Fog, and texture tiling/offset fields in the material. Let’s simplify the shader to bare minimum, and add more comments:

Shader "Unlit/SimpleUnlitTexturedShader"
{
    Properties
    {
        // we have removed support for texture tiling/offset,
        // so make them not be displayed in material inspector
        [NoScaleOffset] _MainTex ("Texture", 2D) = "white" {}
    }
    SubShader
    {
        Pass
        {
            CGPROGRAM
            // use "vert" function as the vertex shader
            #pragma vertex vert
            // use "frag" function as the pixel (fragment) shader
            #pragma fragment frag

            // vertex shader inputs
            struct appdata
            {
                float4 vertex : POSITION; // vertex position
                float2 uv : TEXCOORD0; // texture coordinate
            };

            // vertex shader outputs ("vertex to fragment")
            struct v2f
            {
                float2 uv : TEXCOORD0; // texture coordinate
                float4 vertex : SV_POSITION; // clip space position
            };

            // vertex shader
            v2f vert (appdata v)
            {
                v2f o;
                // transform position to clip space
                // (multiply with model*view*projection matrix)
                o.vertex = mul(UNITY_MATRIX_MVP, v.vertex);
                // just pass the texture coordinate
                o.uv = v.uv;
                return o;
            }
            
            // texture we will sample
            sampler2D _MainTex;

            // pixel shader; returns low precision ("fixed4" type)
            // color ("SV_Target" semantic)
            fixed4 frag (v2f i) : SV_Target
            {
                // sample texture and return it
                fixed4 col = tex2D(_MainTex, i.uv);
                return col;
            }
            ENDCG
        }
    }
}
The Vertex Shader is a program that runs on each vertex of the 3D model. Quite often it does not do anything particularly interesting. Here we just transform vertex position from object space into so called “clip space”, which is what’s used by the GPU to rasterize the object on screen. We also pass the input texture coordinate unmodified - we’ll need it to sample the texture in the fragment shader.

The Fragment Shader is a program that runs on each and every pixel that object occupies on-screen, and is usually used to calculate and output the color of each pixel. Usually there are millions of pixels on the screen, and the fragment shaders are executed for all of them! Optimizing fragment shaders is quite an important part of overall game performance work.

Some variable or function definitions are followed by a Semantic Signifier - for example : POSITION or : SV_Target. These semantics signifiers communicate the “meaning” of these variables to the GPU. See the shader semantics page for details.

When used on a nice model with a nice texture, our simple shader looks pretty good!


Even simpler single color shader

Let’s simplify the shader even more – we’ll make a shader that draws the whole object in a single color. This is not terribly useful, but hey we’re learning here.

Shader "Unlit/SingleColor"
{
    Properties
    {
        // Color property for material inspector, default to white
        _Color ("Main Color", Color) = (1,1,1,1)
    }
    SubShader
    {
        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            
            // vertex shader
            // this time instead of using "appdata" struct, just spell inputs manually,
            // and instead of returning v2f struct, also just return a single output
            // float4 clip position
            float4 vert (float4 vertex : POSITION) : SV_POSITION
            {
                return mul(UNITY_MATRIX_MVP, vertex);
            }
            
            // color from the material
            fixed4 _Color;

            // pixel shader, no inputs needed
            fixed4 frag () : SV_Target
            {
                return _Color; // just return it
            }
            ENDCG
        }
    }
}
This time instead of using structs for input (appdata) and output (v2f), the shader functions just spell out inputs manually. Both ways work, and which you choose to use depends on your coding style and preference.


Using mesh normals for fun and profit

Let’s proceed with a shader that displays mesh normals in world space. Without further ado:

Shader "Unlit/WorldSpaceNormals"
{
    // no Properties block this time!
    SubShader
    {
        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            // include file that contains UnityObjectToWorldNormal helper function
            #include "UnityCG.cginc"

            struct v2f {
                // we'll output world space normal as one of regular ("texcoord") interpolators
                half3 worldNormal : TEXCOORD0;
                float4 pos : SV_POSITION;
            };

            // vertex shader: takes object space normal as input too
            v2f vert (float4 vertex : POSITION, float3 normal : NORMAL)
            {
                v2f o;
                o.pos = UnityObjectToClipPos(vertex);
                // UnityCG.cginc file contains function to transform
                // normal from object to world space, use that
                o.worldNormal = UnityObjectToWorldNormal(normal);
                return o;
            }
            
            fixed4 frag (v2f i) : SV_Target
            {
                fixed4 c = 0;
                // normal is a 3D vector with xyz components; in -1..1
                // range. To display it as color, bring the range into 0..1
                // and put into red, green, blue components
                c.rgb = i.worldNormal*0.5+0.5;
                return c;
            }
            ENDCG
        }
    }
}

Besides resulting in pretty colors, normals are used for all sorts of graphics effects – lighting, reflections, silhouettes and so on.

In the shader above, we started using one of Unity’s built-in shader include files. Here, UnityCG.cginc was used which contains a handy function UnityObjectToWorldNormal. We have also used the utility function UnityObjectToClipPos, which transforms the vertex from object space to the screen. This just makes the code easier to read and is more efficient under certain circumstances.

We’ve seen that data can be passed from the vertex into fragment shader in so-called “interpolators” (or sometimes called “varyings”). In HLSL shading language they are typically labeled with TEXCOORDn semantic, and each of them can be up to a 4-component vector (see semantics page for details).

Also we’ve learned a simple technique in how to visualize normalized vectors (in –1.0 to +1.0 range) as colors: just multiply them by half and add half. See more vertex data visualization examples in vertex program inputs page.

Environment reflection using world-space normals

When a Skybox is used in the scene as a reflection source (see Lighting Window), then essentially a “default” Reflection Probe is created, containing the skybox data. A reflection probe is internally a Cubemap texture; we will extend the world-space normals shader above to look into it.

The code is starting to get a bit involved by now. Of course, if you want shaders that automatically work with lights, shadows, reflections and the rest of the lighting system, it’s way easier to use surface shaders. This example is intended to show you how to use parts of the lighting system in a “manual” way.

Shader "Unlit/SkyReflection"
{
    SubShader
    {
        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            #include "UnityCG.cginc"

            struct v2f {
                half3 worldRefl : TEXCOORD0;
                float4 pos : SV_POSITION;
            };

            v2f vert (float4 vertex : POSITION, float3 normal : NORMAL)
            {
                v2f o;
                o.pos = UnityObjectToClipPos(vertex);
                // compute world space position of the vertex
                float3 worldPos = mul(_Object2World, vertex).xyz;
                // compute world space view direction
                float3 worldViewDir = normalize(UnityWorldSpaceViewDir(worldPos));
                // world space normal
                float3 worldNormal = UnityObjectToWorldNormal(normal);
                // world space reflection vector
                o.worldRefl = reflect(-worldViewDir, worldNormal);
                return o;
            }
        
            fixed4 frag (v2f i) : SV_Target
            {
                // sample the default reflection cubemap, using the reflection vector
                half4 skyData = UNITY_SAMPLE_TEXCUBE(unity_SpecCube0, i.worldRefl);
                // decode cubemap data into actual color
                half3 skyColor = DecodeHDR (skyData, unity_SpecCube0_HDR);
                // output it!
                fixed4 c = 0;
                c.rgb = skyColor;
                return c;
            }
            ENDCG
        }
    }
}

The example above uses several things from the built-in shader include files:

unity_SpecCube0, unity_SpecCube0_HDR, Object2World, UNITY_MATRIX_MVP from the built-in shader variables. unity_SpecCube0 contains data for the active reflection probe.
UNITY_SAMPLE_TEXCUBE is a built-in macro to sample a cubemap. Most regular cubemaps are declared and used using standard HLSL syntax (samplerCUBE and texCUBE), however the reflection probe cubemaps in Unity are declared in a special way to save on sampler slots. If you don’t know what that is, don’t worry, just know that in order to use unity_SpecCube0 cubemap you have to use UNITY_SAMPLE_TEXCUBE macro.
UnityWorldSpaceViewDir function from UnityCG.cginc, and DecodeHDR function from the same file. The latter is used to get actual color from the reflection probe data – since Unity stores reflection probe cubemap in specially encoded way.
reflect is just a built-in HLSL function to compute vector reflection around a given normal.
Environment reflection with a normal map

Often Normal Maps are used to create additional detail on objects, without creating additional geometry. Let’s see how to make a shader that reflects the environment, with a normal map texture.

Now the math is starting to get really involved, so we’ll do it in a few steps. In the shader above, the reflection direction was computed per-vertex (in the vertex shader), and the fragment shader was only doing the reflection probe cubemap lookup. However once we start using normal maps, the surface normal itself needs to be calculated on a per-pixel basis, which means we also have to compute how the environment is reflected per-pixel!

So first of all, let’s rewrite the shader above to do the same thing, except we will move some of the calculations to the fragment shader, so they are computed per-pixel:

Shader "Unlit/SkyReflection Per Pixel"
{
    SubShader
    {
        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            #include "UnityCG.cginc"

            struct v2f {
                float3 worldPos : TEXCOORD0;
                half3 worldNormal : TEXCOORD1;
                float4 pos : SV_POSITION;
            };

            v2f vert (float4 vertex : POSITION, float3 normal : NORMAL)
            {
                v2f o;
                o.pos = UnityObjectToClipPos(vertex);
                o.worldPos = mul(_Object2World, vertex).xyz;
                o.worldNormal = UnityObjectToWorldNormal(normal);
                return o;
            }
        
            fixed4 frag (v2f i) : SV_Target
            {
                // compute view direction and reflection vector
                // per-pixel here
                half3 worldViewDir = normalize(UnityWorldSpaceViewDir(i.worldPos));
                half3 worldRefl = reflect(-worldViewDir, i.worldNormal);

                // same as in previous shader
                half4 skyData = UNITY_SAMPLE_TEXCUBE(unity_SpecCube0, worldRefl);
                half3 skyColor = DecodeHDR (skyData, unity_SpecCube0_HDR);
                fixed4 c = 0;
                c.rgb = skyColor;
                return c;
            }
            ENDCG
        }
    }
}
That by itself does not give us much – the shader looks exactly the same, except now it runs slower since it does more calculations for each and every pixel on screen, instead of only for each of the model’s vertices. However, we’ll need these calculations really soon. Higher graphics fidelity often requires more complex shaders.

We’ll have to learn a new thing now too; the so-called “tangent space”. Normal map textures are most often expressed in a coordinate space that can be thought of as “following the surface” of the model. In our shader, we will need to to know the tangent space basis vectors, read the normal vector from the texture, transform it into world space, and then do all the math from the above shader. Let’s get to it!

Shader "Unlit/SkyReflection Per Pixel"
{
    Properties {
        // normal map texture on the material,
        // default to dummy "flat surface" normalmap
        _BumpMap("Normal Map", 2D) = "bump" {}
    }
    SubShader
    {
        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            #include "UnityCG.cginc"

            struct v2f {
                float3 worldPos : TEXCOORD0;
                // these three vectors will hold a 3x3 rotation matrix
                // that transforms from tangent to world space
                half3 tspace0 : TEXCOORD1; // tangent.x, bitangent.x, normal.x
                half3 tspace1 : TEXCOORD2; // tangent.y, bitangent.y, normal.y
                half3 tspace2 : TEXCOORD3; // tangent.z, bitangent.z, normal.z
                // texture coordinate for the normal map
                float2 uv : TEXCOORD4;
                float4 pos : SV_POSITION;
            };

            // vertex shader now also needs a per-vertex tangent vector.
            // in Unity tangents are 4D vectors, with the .w component used to
            // indicate direction of the bitangent vector.
            // we also need the texture coordinate.
            v2f vert (float4 vertex : POSITION, float3 normal : NORMAL, float4 tangent : TANGENT, float2 uv : TEXCOORD0)
            {
                v2f o;
                o.pos = UnityObjectToClipPos(vertex);
                o.worldPos = mul(_Object2World, vertex).xyz;
                half3 wNormal = UnityObjectToWorldNormal(normal);
                half3 wTangent = UnityObjectToWorldDir(tangent.xyz);
                // compute bitangent from cross product of normal and tangent
                half tangentSign = tangent.w * unity_WorldTransformParams.w;
                half3 wBitangent = cross(wNormal, wTangent) * tangentSign;
                // output the tangent space matrix
                o.tspace0 = half3(wTangent.x, wBitangent.x, wNormal.x);
                o.tspace1 = half3(wTangent.y, wBitangent.y, wNormal.y);
                o.tspace2 = half3(wTangent.z, wBitangent.z, wNormal.z);
                o.uv = uv;
                return o;
            }

            // normal map texture from shader properties
            sampler2D _BumpMap;
        
            fixed4 frag (v2f i) : SV_Target
            {
                // sample the normal map, and decode from the Unity encoding
                half3 tnormal = UnpackNormal(tex2D(_BumpMap, i.uv));
                // transform normal from tangent to world space
                half3 worldNormal;
                worldNormal.x = dot(i.tspace0, tnormal);
                worldNormal.y = dot(i.tspace1, tnormal);
                worldNormal.z = dot(i.tspace2, tnormal);

                // rest the same as in previous shader
                half3 worldViewDir = normalize(UnityWorldSpaceViewDir(i.worldPos));
                half3 worldRefl = reflect(-worldViewDir, worldNormal);
                half4 skyData = UNITY_SAMPLE_TEXCUBE(unity_SpecCube0, worldRefl);
                half3 skyColor = DecodeHDR (skyData, unity_SpecCube0_HDR);
                fixed4 c = 0;
                c.rgb = skyColor;
                return c;
            }
            ENDCG
        }
    }
}
Phew, that was quite involved. But look, normal mapped reflections!


Adding more textures

Let’s add more textures to the normal-mapped, sky-reflecting shader above. We’ll add the base color texture, seen in the first unlit example, and an occlusion map to darken the cavities.

Shader "Unlit/More Textures"
{
    Properties {
        // three textures we'll use in the material
        _MainTex("Base texture", 2D) = "white" {}
        _OcclusionMap("Occlusion", 2D) = "white" {}
        _BumpMap("Normal Map", 2D) = "bump" {}
    }
    SubShader
    {
        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            #include "UnityCG.cginc"

            // exactly the same as in previous shader
            struct v2f {
                float3 worldPos : TEXCOORD0;
                half3 tspace0 : TEXCOORD1;
                half3 tspace1 : TEXCOORD2;
                half3 tspace2 : TEXCOORD3;
                float2 uv : TEXCOORD4;
                float4 pos : SV_POSITION;
            };
            v2f vert (float4 vertex : POSITION, float3 normal : NORMAL, float4 tangent : TANGENT, float2 uv : TEXCOORD0)
            {
                v2f o;
                o.pos = UnityObjectToClipPos(vertex);
                o.worldPos = mul(_Object2World, vertex).xyz;
                half3 wNormal = UnityObjectToWorldNormal(normal);
                half3 wTangent = UnityObjectToWorldDir(tangent.xyz);
                half tangentSign = tangent.w * unity_WorldTransformParams.w;
                half3 wBitangent = cross(wNormal, wTangent) * tangentSign;
                o.tspace0 = half3(wTangent.x, wBitangent.x, wNormal.x);
                o.tspace1 = half3(wTangent.y, wBitangent.y, wNormal.y);
                o.tspace2 = half3(wTangent.z, wBitangent.z, wNormal.z);
                o.uv = uv;
                return o;
            }

            // textures from shader properties
            sampler2D _MainTex;
            sampler2D _OcclusionMap;
            sampler2D _BumpMap;
        
            fixed4 frag (v2f i) : SV_Target
            {
                // same as from previous shader...
                half3 tnormal = UnpackNormal(tex2D(_BumpMap, i.uv));
                half3 worldNormal;
                worldNormal.x = dot(i.tspace0, tnormal);
                worldNormal.y = dot(i.tspace1, tnormal);
                worldNormal.z = dot(i.tspace2, tnormal);
                half3 worldViewDir = normalize(UnityWorldSpaceViewDir(i.worldPos));
                half3 worldRefl = reflect(-worldViewDir, worldNormal);
                half4 skyData = UNITY_SAMPLE_TEXCUBE(unity_SpecCube0, worldRefl);
                half3 skyColor = DecodeHDR (skyData, unity_SpecCube0_HDR);                
                fixed4 c = 0;
                c.rgb = skyColor;

                // modulate sky color with the base texture, and the occlusion map
                fixed3 baseColor = tex2D(_MainTex, i.uv).rgb;
                fixed occlusion = tex2D(_OcclusionMap, i.uv).r;
                c.rgb *= baseColor;
                c.rgb *= occlusion;

                return c;
            }
            ENDCG
        }
    }
}
Balloon cat is looking good!


Texturing shader examples

Procedural checkerboard pattern

Here’s a shader that outputs a checkerboard pattern based on texture coordinates of a mesh:

Shader "Unlit/Checkerboard"
{
    Properties
    {
        _Density ("Density", Range(2,50)) = 30
    }
    SubShader
    {
        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            #include "UnityCG.cginc"

            struct v2f
            {
                float2 uv : TEXCOORD0;
                float4 vertex : SV_POSITION;
            };

            float _Density;

            v2f vert (float4 pos : POSITION, float2 uv : TEXCOORD0)
            {
                v2f o;
                o.vertex = UnityObjectToClipPos(pos);
                o.uv = uv * _Density;
                return o;
            }
            
            fixed4 frag (v2f i) : SV_Target
            {
                float2 c = i.uv;
                c = floor(c) / 2;
                float checker = frac(c.x + c.y) * 2;
                return checker;
            }
            ENDCG
        }
    }
}
The density slider in the Properties block controls how dense the checkerboard is. In the vertex shader, the mesh UVs are multiplied by the density value to take them from a range of 0 to 1 to a range of 0 to density. Let’s say the density was set to 30 - this will make i.uv input into the fragment shader contain floating point values from zero to 30 for various places of the mesh being rendered.

Then the fragment shader code takes only the integer part of the input coordinate using HLSL’s built-in floor function, and divides it by two. Recall that the input coordinates were numbers from 0 to 30; this makes them all be “quantized” to values of 0, 0.5, 1, 1.5, 2, 2.5, and so on. This was done on both the x and y components of the input coordinate.

Next up, we add these x and y coordinates together (each of them only having possible values of 0, 0.5, 1, 1.5, …) and only take the fractional part using another built-in HLSL function, frac. Result of this can only be either 0.0 or 0.5. We then multiply it by two to make it either 0.0 or 1.0, and output as a color (this results in black or white color respectively).


Tri-planar texturing

For complex or procedural meshes, instead of texturing them using the regular UV coordinates, it is sometimes useful to just “project” texture onto the object from three primary directions. This is called “tri-planar” texturing. The idea is to use surface normal to weight the three texture directions. Here’s the shader:

Shader "Unlit/Triplanar"
{
    Properties
    {
        _MainTex ("Texture", 2D) = "white" {}
        _Tiling ("Tiling", Float) = 1.0
        _OcclusionMap("Occlusion", 2D) = "white" {}
    }
    SubShader
    {
        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            #include "UnityCG.cginc"

            struct v2f
            {
                half3 objNormal : TEXCOORD0;
                float3 coords : TEXCOORD1;
                float2 uv : TEXCOORD2;
                float4 pos : SV_POSITION;
            };

            float _Tiling;

            v2f vert (float4 pos : POSITION, float3 normal : NORMAL, float2 uv : TEXCOORD0)
            {
                v2f o;
                o.pos = UnityObjectToClipPos(pos);
                o.coords = pos.xyz * _Tiling;
                o.objNormal = normal;
                o.uv = uv;
                return o;
            }

            sampler2D _MainTex;
            sampler2D _OcclusionMap;
            
            fixed4 frag (v2f i) : SV_Target
            {
                // use absolute value of normal as texture weights
                half3 blend = abs(i.objNormal);
                // make sure the weights sum up to 1 (divide by sum of x+y+z)
                blend /= dot(blend,1.0);
                // read the three texture projections, for x,y,z axes
                fixed4 cx = tex2D(_MainTex, i.coords.yz);
                fixed4 cy = tex2D(_MainTex, i.coords.xz);
                fixed4 cz = tex2D(_MainTex, i.coords.xy);
                // blend the textures based on weights
                fixed4 c = cx * blend.x + cy * blend.y + cz * blend.z;
                // modulate by regular occlusion map
                c *= tex2D(_OcclusionMap, i.uv);
                return c;
            }
            ENDCG
        }
    }
}

Calculating lighting

Typically when you want a shader that works with Unity’s lighting pipeline, you would write a surface shader. This does most of the “heavy lifting” for you, and your shader code just needs to define surface properties.

However in some cases you want to bypass the standard surface shader path; either because you want to only support some limited subset of whole lighting pipeline for performance reasons, or you want to do custom things that aren’t quite “standard lighting”. The following examples will show how to get to the lighting data from manually-written vertex and fragment shaders. Looking at the code generated by surface shaders (via shader inspector) is also a good learning resource.

Simple diffuse lighting

The first thing we need to do is to indicate that our shader does in fact need lighting information passed to it. Unity’s rendering pipeline supports various ways of rendering; here we’ll be using the default forward rendering one.

We’ll start by only supporting one directional light. Forward rendering in Unity works by rendering the main directional light, ambient, lightmaps and reflections in a single pass called ForwardBase. In the shader, this is indicated by adding a pass tag: Tags {“LightMode”=“ForwardBase”}. This will make directional light data be passed into shader via some built-in variables.

Here’s the shader that computes simple diffuse lighting per vertex, and uses a single main texture:

Shader "Lit/Simple Diffuse"
{
    Properties
    {
        [NoScaleOffset] _MainTex ("Texture", 2D) = "white" {}
    }
    SubShader
    {
        Pass
        {
            // indicate that our pass is the "base" pass in forward
            // rendering pipeline. It gets ambient and main directional
            // light data set up; light direction in _WorldSpaceLightPos0
            // and color in _LightColor0
            Tags {"LightMode"="ForwardBase"}
        
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            #include "UnityCG.cginc" // for UnityObjectToWorldNormal
            #include "UnityLightingCommon.cginc" // for _LightColor0

            struct v2f
            {
                float2 uv : TEXCOORD0;
                fixed4 diff : COLOR0; // diffuse lighting color
                float4 vertex : SV_POSITION;
            };

            v2f vert (appdata_base v)
            {
                v2f o;
                o.vertex = UnityObjectToClipPos(v.vertex);
                o.uv = v.texcoord;
                // get vertex normal in world space
                half3 worldNormal = UnityObjectToWorldNormal(v.normal);
                // dot product between normal and light direction for
                // standard diffuse (Lambert) lighting
                half nl = max(0, dot(worldNormal, _WorldSpaceLightPos0.xyz));
                // factor in the light color
                o.diff = nl * _LightColor0;
                return o;
            }
            
            sampler2D _MainTex;

            fixed4 frag (v2f i) : SV_Target
            {
                // sample texture
                fixed4 col = tex2D(_MainTex, i.uv);
                // multiply by lighting
                col *= i.diff;
                return col;
            }
            ENDCG
        }
    }
}
This makes the object react to light direction - parts of it facing the light are illuminated, and parts facing away are not illuminated at all.


Diffuse lighting with ambient

The example above does not take any ambient lighting or light probes into account. Let’s fix this! It turns out we can do this by adding just a single line of code. Both ambient and light probe data is passed to shaders in Spherical Harmonics form, and ShadeSH9 function from UnityCG.cginc include file does all the work of evaluating it, given a world space normal.

Shader "Lit/Diffuse With Ambient"
{
    Properties
    {
        [NoScaleOffset] _MainTex ("Texture", 2D) = "white" {}
    }
    SubShader
    {
        Pass
        {
            Tags {"LightMode"="ForwardBase"}
        
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            #include "UnityCG.cginc"
            #include "UnityLightingCommon.cginc"

            struct v2f
            {
                float2 uv : TEXCOORD0;
                fixed4 diff : COLOR0;
                float4 vertex : SV_POSITION;
            };

            v2f vert (appdata_base v)
            {
                v2f o;
                o.vertex = UnityObjectToClipPos(v.vertex);
                o.uv = v.texcoord;
                half3 worldNormal = UnityObjectToWorldNormal(v.normal);
                half nl = max(0, dot(worldNormal, _WorldSpaceLightPos0.xyz));
                o.diff = nl * _LightColor0;

                // the only difference from previous shader:
                // in addition to the diffuse lighting from the main light,
                // add illumination from ambient or light probes
                // ShadeSH9 function from UnityCG.cginc evaluates it,
                // using world space normal
                o.diff.rgb += ShadeSH9(half4(worldNormal,1));
                return o;
            }
            
            sampler2D _MainTex;

            fixed4 frag (v2f i) : SV_Target
            {
                fixed4 col = tex2D(_MainTex, i.uv);
                col *= i.diff;
                return col;
            }
            ENDCG
        }
    }
}
This shader is in fact starting to look very similar to the built-in Legacy Diffuse shader!


Implementing shadow casting

Our shader currently can neither receive nor cast shadows. Let’s implement shadow casting first.

In order to cast shadows, a shader has to have a ShadowCaster pass type in any of its subshaders or any fallback. The ShadowCaster pass is used to render the object into the shadowmap, and typically it is fairly simple - the vertex shader only needs to evaluate the vertex position, and the fragment shader pretty much does not do anything. The shadowmap is only the depth buffer, so even the color output by the fragment shader does not really matter.

This means that for a lot of shaders, the shadow caster pass is going to be almost exactly the same (unless object has custom vertex shader based deformations, or has alpha cutout / semitransparent parts). The easiest way to pull it in is via UsePass shader command:

Pass
{
    // regular lighting pass
}
// pull in shadow caster from VertexLit built-in shader
UsePass "Legacy Shaders/VertexLit/SHADOWCASTER"
However we’re learning here, so let’s do the same thing “by hand” so to speak. For shorter code, we’ve replaced the lighting pass (“ForwardBase”) with code that only does untextured ambient. Below it, there’s a “ShadowCaster” pass that makes the object support shadow casting.

Shader "Lit/Shadow Casting"
{
    SubShader
    {
        // very simple lighting pass, that only does non-textured ambient
        Pass
        {
            Tags {"LightMode"="ForwardBase"}
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            #include "UnityCG.cginc"
            struct v2f
            {
                fixed4 diff : COLOR0;
                float4 vertex : SV_POSITION;
            };
            v2f vert (appdata_base v)
            {
                v2f o;
                o.vertex = UnityObjectToClipPos(v.vertex);
                half3 worldNormal = UnityObjectToWorldNormal(v.normal);
                // only evaluate ambient
                o.diff.rgb = ShadeSH9(half4(worldNormal,1));
                o.diff.a = 1;
                return o;
            }
            fixed4 frag (v2f i) : SV_Target
            {
                return i.diff;
            }
            ENDCG
        }

        // shadow caster rendering pass, implemented manually
        // using macros from UnityCG.cginc
        Pass
        {
            Tags {"LightMode"="ShadowCaster"}

            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            #pragma multi_compile_shadowcaster
            #include "UnityCG.cginc"

            struct v2f { 
                V2F_SHADOW_CASTER;
            };

            v2f vert(appdata_base v)
            {
                v2f o;
                TRANSFER_SHADOW_CASTER_NORMALOFFSET(o)
                return o;
            }

            float4 frag(v2f i) : SV_Target
            {
                SHADOW_CASTER_FRAGMENT(i)
            }
            ENDCG
        }
    }
}
Now there’s a plane underneath, using a regular built-in Diffuse shader, so that we can see our shadows working (remember, our current shader does not support receiving shadows yet!).


We’ve used the #pragma multi_compile_shadowcaster directive. This causes the shader to be compiled into several variants with different preprocessor macros defined for each (see multiple shader variants page for details). When rendering into the shadowmap, the cases of point lights vs other light types need slightly different shader code, that’s why this directive is needed.

Receiving shadows

Implementing support for receiving shadows will require compiling the base lighting pass into several variants, to handle cases of “directional light without shadows” and “directional light with shadows” properly. #pragma multi_compile_fwdbase directive does this (see multiple shader variants for details). In fact it does a lot more: it also compiles variants for the different lightmap types, realtime GI being on or off etc. Currently we don’t need all that, so we’ll explicitly skip these variants.

Then to get actual shadowing computations, we’ll #include “AutoLight.cginc” shader include file and use SHADOW_COORDS, TRANSFER_SHADOW, SHADOW_ATTENUATION macros from it.

Here’s the shader:

Shader "Lit/Diffuse With Shadows"
{
    Properties
    {
        [NoScaleOffset] _MainTex ("Texture", 2D) = "white" {}
    }
    SubShader
    {
        Pass
        {
            Tags {"LightMode"="ForwardBase"}
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            #include "UnityCG.cginc"
            #include "Lighting.cginc"

            // compile shader into multiple variants, with and without shadows
            // (we don't care about any lightmaps yet, so skip these variants)
            #pragma multi_compile_fwdbase nolightmap nodirlightmap nodynlightmap novertexlight
            // shadow helper functions and macros
            #include "AutoLight.cginc"

            struct v2f
            {
                float2 uv : TEXCOORD0;
                SHADOW_COORDS(1) // put shadows data into TEXCOORD1
                fixed3 diff : COLOR0;
                fixed3 ambient : COLOR1;
                float4 pos : SV_POSITION;
            };
            v2f vert (appdata_base v)
            {
                v2f o;
                o.pos = UnityObjectToClipPos(v.vertex);
                o.uv = v.texcoord;
                half3 worldNormal = UnityObjectToWorldNormal(v.normal);
                half nl = max(0, dot(worldNormal, _WorldSpaceLightPos0.xyz));
                o.diff = nl * _LightColor0.rgb;
                o.ambient = ShadeSH9(half4(worldNormal,1));
                // compute shadows data
                TRANSFER_SHADOW(o)
                return o;
            }

            sampler2D _MainTex;

            fixed4 frag (v2f i) : SV_Target
            {
                fixed4 col = tex2D(_MainTex, i.uv);
                // compute shadow attenuation (1.0 = fully lit, 0.0 = fully shadowed)
                fixed shadow = SHADOW_ATTENUATION(i);
                // darken light's illumination with shadow, keep ambient intact
                fixed3 lighting = i.diff * shadow + i.ambient;
                col.rgb *= lighting;
                return col;
            }
            ENDCG
        }

        // shadow casting support
        UsePass "Legacy Shaders/VertexLit/SHADOWCASTER"
    }
}
Look, we have shadows now!


Other shader examples

Fog

Shader "Custom/TextureCoordinates/Fog" {
    SubShader {
        Pass {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            
            //Needed for fog variation to be compiled.
            #pragma multi_compile_fog

            #include "UnityCG.cginc"

            struct vertexInput {
                float4 vertex : POSITION;
                float4 texcoord0 : TEXCOORD0;
            };

            struct fragmentInput{
                float4 position : SV_POSITION;
                float4 texcoord0 : TEXCOORD0;
                
                //Used to pass fog amount around number should be a free texcoord.
                UNITY_FOG_COORDS(1)
            };

            fragmentInput vert(vertexInput i){
                fragmentInput o;
                o.position = UnityObjectToClipPos(i.vertex);
                o.texcoord0 = i.texcoord0;
                
                //Compute fog amount from clip space position.
                UNITY_TRANSFER_FOG(o,o.position);
                return o;
            }

            fixed4 frag(fragmentInput i) : SV_Target {
                fixed4 color = fixed4(i.texcoord0.xy,0,0);
                
                //Apply fog (additive pass are automatically handled)
                UNITY_APPLY_FOG(i.fogCoord, color); 
                
                //to handle custom fog color another option would have been 
                //#ifdef UNITY_PASS_FORWARDADD
                //  UNITY_APPLY_FOG_COLOR(i.fogCoord, color, float4(0,0,0,0));
                //#else
                //  fixed4 myCustomColor = fixed4(0,0,1,0);
                //  UNITY_APPLY_FOG_COLOR(i.fogCoord, color, myCustomColor);
                //#endif
                
                r

eturn color;
            }
            ENDCG
        }
    }
}
You can download the examples shown above as a zipped Unity project.

Further reading

Writing Vertex and Fragment Programs.
Shader Semantics.
Writing Surface Shaders.
Shader Reference.

Unity Analytics Raw Data Export
Overview

Unity Analytics Raw Data Export gives you full access to raw event data. This lets you use the data in whatever way you choose; for example, to construct custom queries or data visualizations.

There are two ways you can access raw data export:

Analytics dashboard user interface
Call the REST API
Analytics dashboard user interface

The Analytics dashboard provides a way to export and access your raw event data without writing any code. In your Analytics dashboard (analytics.cloud.unity3d.com), select your project and navigate to Analytics > Raw Data Export.

The Raw Data Export screen
The Raw Data Export screen
Then follow these steps in the Export section of that screen:

Specify the Data Set you want to export (ie. appRunning, appStart, custom, deviceInfo, transaction, userInfo).
Specify the Start Date, or alternatively continue exporting data from a previous job by selecting the Previous Job Id from the dropdown box.
Specify the End Date.
The file format defaults to JSON. Specify your preferred file format by clicking on and selecting an item from the drop box under Export as JSON.
The Export section and Activity table on the Raw Data Export screen
The Export section and Activity table on the Raw Data Export screen
Raw Data Export automatically creates the job and displays it in the Activity table on the screen.

Once the job is done, you can download your data by selecting the files from the Download column.

REST (Representational State Transfer) API

Every data point sent to Unity Analytics is stored in Unity’s data store. The Raw Data Export APIs allow you to download your raw event data in files as the data is received and stored.

Requirements

Every request requires HTTP Basic authentication using your Unity Project ID (UPID) and API Key.


Limitations

The request period (startDate to endDate) is limited to 31 days.
Limits to usage may be subject to change.
User workflow

To export raw data, call the Create Raw Data Export API. This request triggers an asynchronous job to process the data. The time this takes depends on how much data is being exported. To get the current status or result, poll using the Get Raw Data Export API. Once the export is completed, you can get the result in the response of this API. The result contains the list of files and the corresponding download URLs. You can loop through the URLs and download the exported data.
Notes:

All dates and times in requests or responses are in the UTC timezone.
API requests or responses use the JSON format (which is formatted for readability in this document). The data file format is configurable.
Data is exported in .gzip compressed files.
The URL base for the API is https://analytics.cloud.unity3d.com.
Create Raw Data Export

A Raw Data Export is specific to a project and specific to a single dataset (event type). The request period is limited to 31 days. This may be revised in future releases.

Use the following HTTP method to create a Raw Data Export:

POST api/v2/projects/{UNITY_PROJECT_ID}/rawdataexports
Arguments are provided in the payload of the request, and are in JSON format with the Content-Type application/json.

Request Argument	Required or optional	Type	Description
startDate	Required unless continueFrom is specified	string	The start date (inclusive) of the export. The date is expressed in YYYY-MM-DD format (ISO–8601).
endDate	required	string	The end data (exclusive) of the export. The date is expressed in YYYY-MM-DD format (ISO 8601). This is the date at which to close the query. When searching for the current day, use the following day’s date.
format	required	string	The output data format: json (newline-delimited json) or tsv
dataset	required	string	One of the following event types: appStart, appRunning, deviceInfo, custom, transaction, or userInfo
continueFrom	optional	string	The Raw Data Export ID to continue exporting data from. This is used for continuing a previous export from the point it finished. See Continuing for more information. Instead of specifying a startDate, a prior Raw Data Export ID could be specified in continueFrom. It is an error to specify both continueFrom and startDate.
Template for a Request using cURL on the command line:

curl --user {UNITY_PROJECT_ID}:{API_KEY} --request POST --header "Content-Type: application/json" --data {REQUEST_JSON}
https://analytics.cloud.unity3d.com/api/v2/projects/{UNITY_PROJECT_ID}/rawdataexports
Example values:

UNITY_PROJECT_ID = aa43ae0a-a7a7-4016-ae96-e253bb126aa8
API_KEY = 166291ff148b2878375a8e54aebb1549
REQUEST_JSON = { "startDate": "2016-05-15" , "endDate": "2016-05-16", "format": "tsv", "dataset": "appStart" }
An actual request using the example values:

curl --user aa43ae0a-a7a7-4016-ae96-e253bb126aa8:166291ff148b2878375a8e54aebb1549 --request POST --header "Content-Type: application/json" --data '{ "startDate": "2016-05-15" , "endDate": "2016-05-16", "format": "tsv", "dataset": "appStart" }' https://analytics.cloud.unity3d.com/api/v2/projects/aa43ae0a-a7a7-4016-ae96-e253bb126aa8/rawdataexports
The response uses the common Raw Data Export Response Attributes in JSON format.

Raw Data Export Response Attributes

Response Attribute	Type	Description
id	string	The Raw Data Export ID.
upid	string	The Unity Project ID.
createdAt	string	The created time in ISO 8601 format.
status	string	The current status of the export. Possible values are: running, completed, or failed.
duration	long	The time it took to export data (in milliseconds).
request	json	The request arguments.
result	json	The result contains attributes that detail the exported data. The result is only available after the export is successfully completed. See below for result attributes.
result.size	long	The total size of the data exported (in bytes).
result.eventCount	long	The total number of events exported.
result.intraDay	boolean	When the request includes the current day it might not contain all the data for the day. This attribute is FALSE if the data is incomplete for the last day.
result.fileList	json	The list of files containing the data exported. The file list is empty when there is no data.
result.fileList.name	string	The file name.
result.fileList.url	string	The download URL for the file. The files are compressed in gzip format.
result.fileList.size	long	The size of the file in bytes.
result.fileList.date	string	The file contains events for this specific date. This date is based on the submit time of the event. There may be multiple files for the same date. The date in is in ISO 8601 format.
An example of response:

{  
   "id":"8228d1e9-31b3-4a5e-aabe-55d9c8afa052",
   "upid":"beff3f49-b9ed-41a4-91ea-677e9b85e71e",
   "createdAt":"2016-05-10T10:10:10.100+0000",
   "status":"running",
   "duration" : 0,
   "request":{  
      "startDate":"2016-05-01",
      "endDate":"2016-05-02",
      "format":"json",
      "dataset":"appRunning"
   }
}
Continue creating from a prior Raw Data Export

When you are running periodic Raw Data Exports, you must provide the continueFrom argument instead of the startDate to make sure you are continuing from the previous Raw Data Export. Prior Raw Data Export IDs can be fetched via the GET APIs or accessed via the Dashboard.

Prior Raw Data Exports in the Unity Analytics Project Dashboard
Prior Raw Data Exports in the Unity Analytics Project Dashboard
Get Raw Data Export

Use the following HTTP method to get a specific Raw Data Export or ongoing export status:

GET api/v2/projects/{UNITY_PROJECT_ID}/rawdataexports/{raw_data_export_id} 
All required arguments are part of the URL path.

An example of a request:

curl --user {UNITY_PROJECT_ID}:${API_KEY} https://analytics.cloud.unity3d.com/api/v2/projects/{UNITY_PROJECT_ID}/rawdataexports/${ID}
The response is the Raw Data Export in JSON format. It is the same as the response in Create Raw Data Export.

An example of a response:

{  
   "id":"6601f70e-6a0b-48ed-909f-26711af82b49",
   "status":"success",
   "createdAt":"2016-05-21T04:41:54.000+0000",
   "duration":8631714000,
   "request":{  
      "startDate":"2016-02-11T00:00:00.000+0000",
      "endDate":"2016-03-11T00:00:00.000+0000",
      "format":"tsv",
      "dataset":"custom"
   },
   "result":{  
      "size":78355,
      "eventCount":17473,
      "fileList":[  
         {  
            "name":"headers.gz",
            "url":"https://uca-export.s3.amazonaws.com/staging/devTest/custom/appid%3DUNITY_PROJECT_ID/jid%3D6601f70e-6a0b-48ed-909f-26711af82b49/headers.gz?AWSAccessKeyId=AKIAJUXGNF66F4XPWSWA&Expires=1463872651&Signature=PnzIeeI%2FNxSOlKkLVpLcfK%2FxVpU%3D",
            "size":105
         },
         {  
            "name":"part-4b0cf376-3478-4bc8-845e-f73aff5c0be4.gz",
            "url":"https://uca-export.s3.amazonaws.com/staging/devTest/custom/appid%3DUNITY_PROJECT_ID/jid%3D6601f70e-6a0b-48ed-909f-26711af82b49/part-4b0cf376-3478-4bc8-845e-f73aff5c0be4.gz?AWSAccessKeyId=AKIAJUXGNF66F4XPWSWA&Expires=1463872651&Signature=xZk3%2BzQNTQ6yjK2Mh%2FaH338ABn8%3D",
            "size":78250,
            "date":"2016-02-13T00:00:00.000+0000"
         }
      ],
      "intraDay":false
   }
}
List all Raw Data Exports

Use this HTTP method to get the list of all Raw Data Exports for a specific project:

GET api/v2/projects/{UNITY_PROJECT_ID}/rawdataexports
All required arguments are part of the URL path.

An example of a request:

curl --user {UNITY_PROJECT_ID}:${API_KEY} https://analytics.cloud.unity3d.com/api/v2/projects/${UNITY_PROJECT_ID}/rawdataexports/
The response is a list of Raw Data Exports in JSON format. See Raw Data Export Response Attributes for the definition of each export list element.

An example of a response:

[
{  
   "id":"6601f70e-6a0b-48ed-909f-26711af82b49",
   "status":"success",
   "createdAt":"2016-05-21T04:41:54.000+0000",
   "duration":8631714000,
   "request":{  
      "startDate":"2016-02-11T00:00:00.000+0000",
      "endDate":"2016-03-11T00:00:00.000+0000",
      "format":"tsv",
      "dataset":"custom"
   },
   "result":{  
      "size":78355,
      "eventCount":17473,
      "fileList":[  
         {  
            "name":"headers.gz",
            "url":"https://uca-export.s3.amazonaws.com/staging/devTest/custom/appid%3DUNITY_PROJECT_ID/jid%3D6601f70e-6a0b-48ed-909f-26711af82b49/headers.gz?AWSAccessKeyId=AKIAJUXGNF66F4XPWSWA&Expires=1463872651&Signature=PnzIeeI%2FNxSOlKkLVpLcfK%2FxVpU%3D",
            "size":105
         },
         {  
            "name":"part-4b0cf376-3478-4bc8-845e-f73aff5c0be4.gz",
            "url":"https://uca-export.s3.amazonaws.com/staging/devTest/custom/appid%3DUNITY_PROJECT_ID/jid%3D6601f70e-6a0b-48ed-909f-26711af82b49/part-4b0cf376-3478-4bc8-845e-f73aff5c0be4.gz?AWSAccessKeyId=AKIAJUXGNF66F4XPWSWA&Expires=1463872651&Signature=xZk3%2BzQNTQ6yjK2Mh%2FaH338ABn8%3D",
            "size":78250,
            "date":"2016-02-13T00:00:00.000+0000"
         }
      ],
      "intraDay":false
   }
},
{  
   "id":"6601f70e-6a0b-48ed-909f-26711af82b48",
   "status":"success",
   "createdAt":"2016-05-21T04:41:54.000+0000",
   "duration":8631714000,
   "request":{  
      "startDate":"2016-02-11T00:00:00.000+0000",
      "endDate":"2016-03-11T00:00:00.000+0000",
      "format":"tsv",
      "dataset":"custom"
   },
   "result":{  
      "size":78355,
      "eventCount":17473,
      "fileList":[  
         {  
            "name":"headers.gz",
            "url":"https://uca-export.s3.amazonaws.com/staging/devTest/custom/appid%3DUNITY_PROJECT_ID/jid%3D6601f70e-6a0b-48ed-909f-26711af82b48/headers.gz?AWSAccessKeyId=AKIAJUXGNF66F4XPWSWA&Expires=1463872651&Signature=PnzIeeI%2FNxSOlKkLVpLcfK%2FxVpU%3D",
            "size":105
         },
         {  
            "name":"part-4b0cf376-3478-4bc8-845e-f73aff5c0be4.gz",
            "url":"https://uca-export.s3.amazonaws.com/staging/devTest/custom/appid%3DUNITY_PROJECT_ID/jid%3D6601f70e-6a0b-48ed-909f-26711af82b48/part-4b0cf376-3478-4bc8-845e-f73aff5c0be4.gz?AWSAccessKeyId=AKIAJUXGNF66F4XPWSWA&Expires=1463872651&Signature=xZk3%2BzQNTQ6yjK2Mh%2FaH338ABn8%3D",
            "size":78250,
            "date":"2016-02-13T00:00:00.000+0000"
         }
      ],
      "intraDay":false
   }
}
]
TSV format

If you choose to export in TSV format, the headers are provided in a separate file in headers.gz. The data files do not include headers.

An example headers file:

ts  appid   type    userid  sessionid   remote_ip   platform    sdk_ver debug_device    user_agent  submit_time name    custom_params
Dataset

Each of the six data types (event types) differ. See below for their schema definitions.

Notes:

ts is the timestamp at which the event was generated on the device. Note that device-generated timestamps can be skewed due to the device clock and latency in receiving the event.
submit_time is the timestamp at which the event was received by Unity Analytics.
AppStart Event

{
   "namespace":"com.unity.analytics.commons.schema",
   "name":"AppStartEvent",
   "type":"record",
   "fields":[
       {"name": "ts",   "type": "long", "default": 0}, 
       {"name": "appid", "type": "string", "default": ""},
       {"name": "type", "type": "string", "default": ""}, 
       {"name": "userid", "type": "string", "default": ""},
       {"name": "sessionid", "type": "string", "default": ""},
       {"name": "remote_ip", "type": "string", "default": ""},
       {"name": "platform", "type": "string", "default": ""},
       {"name": "sdk_ver", "type": "string", "default": ""},
       {"name": "debug_device", "type": "boolean", "default": false},
       {"name": "user_agent", "type": "string", "default": ""},
       {"name": "submit_time", "type": "long", "default": 0} 
   ]
}  
AppRunning Event

{
   "namespace":"com.unity.analytics.commons.schema",
   "name":"AppRunningEvent",
   "type":"record",
   "fields":[
       {"name": "ts",   "type": "long", "default": 0}, 
       {"name": "appid", "type": "string", "default": ""},
       {"name": "type", "type": "string", "default": ""},
       {"name": "userid", "type": "string", "default": ""},
       {"name": "sessionid", "type": "string", "default": ""},
       {"name": "remote_ip", "type": "string", "default": ""},
       {"name": "platform", "type": "string", "default": ""},
       {"name": "sdk_ver", "type": "string", "default": ""},
       {"name": "debug_device", "type": "boolean", "default": false},
       {"name": "user_agent", "type": "string", "default": ""},
       {"name": "submit_time", "type": "long", "default": 0}, 
       {"name": "duration", "type": "int", "default": 0}
   ]
}
Custom Event

{
   "namespace":"com.unity.analytics.commons.schema",
   "name":"CustomEvent",
   "type":"record",
   "fields":[
       {"name": "ts",   "type": "long", "default": 0}, 
       {"name": "appid", "type": "string", "default": ""},
       {"name": "type", "type": "string", "default": ""},
       {"name": "userid", "type": "string", "default": ""},
       {"name": "sessionid", "type": "string", "default": ""},
       {"name": "remote_ip", "type": "string", "default": ""},
       {"name": "platform", "type": "string", "default": ""},
       {"name": "sdk_ver", "type": "string", "default": ""},
       {"name": "debug_device", "type": "boolean", "default": false},
       {"name": "user_agent", "type": "string", "default": ""},
       {"name": "submit_time", "type": "long", "default": 0}, 
       {"name": "name", "type": "string", "default": ""},
       {
           "name":"custom_params",
           "type":["null",{
               "type":"map",
               "values": ["string","null"],
               "default": ""
           }],
           "default": null
       }
   ]
}
DeviceInfo Event

{
   "namespace":"com.unity.analytics.commons.schema",
   "name":"DeviceInfoEvent",
   "type":"record",
   "fields":[
       {"name": "ts",   "type": "long", "default": 0}, 
       {"name": "appid", "type": "string", "default": ""},
       {"name": "type", "type": "string", "default": ""}, 
       {"name": "userid", "type": "string", "default": ""},
       {"name": "sessionid", "type": "string", "default": ""},
       {"name": "remote_ip", "type": "string", "default": ""},
       {"name": "platform", "type": "string", "default": ""},
       {"name": "sdk_ver", "type": "string", "default": ""},
       {"name": "debug_device", "type": "boolean", "default": false},
       {"name": "user_agent", "type": "string", "default": ""},
       {"name": "submit_time", "type": "long", "default": 0}, 
       {"name": "debug_build", "type": "boolean", "default": false},
       {"name": "rooted_jailbroken", "type": "boolean", "default": false},
       {"name": "processor_type", "type": "string", "default": ""},
       {"name": "system_memory_size", "type": "string", "default": ""},
       {"name": "make", "type": "string", "default": ""},
       {"name": "app_ver", "type": "string", "default": ""},
       {"name": "deviceid", "type": "string", "default": ""},
       {"name": "license_type", "type": "string", "default": ""},
       {"name": "app_install_mode", "type": "string", "default": ""},
       {"name": "model", "type": "string", "default": ""},
       {"name": "engine_ver", "type": "string", "default": ""},
       {"name": "os_ver", "type": "string", "default": ""},
       {"name": "app_name", "type": "string", "default": ""},
       {"name": "timezone", "type": "string", "default": ""},
       {"name": "ads_tracking", "type": "boolean", "default": false},
       {"name": "adsid", "type": "string", "default": ""}
   ]
}  
Transaction Event

{
   "namespace":"com.unity.analytics.commons.schema",
   "name":"TransactionEvent",
   "type":"record",
   "fields":[
       {"name": "ts",   "type": "long", "default": 0},
       {"name": "appid", "type": "string", "default": ""},
       {"name": "type", "type": "string", "default": ""},
       {"name": "userid", "type": "string", "default": ""},
       {"name": "sessionid", "type": "string", "default": ""},
       {"name": "remote_ip", "type": "string", "default": ""},
       {"name": "platform", "type": "string", "default": ""},
       {"name": "sdk_ver", "type": "string", "default": ""},
       {"name": "debug_device", "type": "boolean", "default": false},
       {"name": "user_agent", "type": "string", "default": ""},
       {"name": "submit_time", "type": "long", "default": 0},        {
           "name":"receipt",
           "type":["null",{
               "type":"record",
               "name": "receiptRecord",
               "fields":[
                   {"name": "data", "type": "string", "default": ""},
                   {"name": "signature", "type": "string", "default": ""}
               ]
           }],
           "default": null
       },
       {"name": "currency", "type": "string", "default": "USD"},
       {"name": "amount", "type": "float", "default": 0},
       {"name": "transactionid", "type": "long", "default": 0},
       {"name": "productid", "type": "string", "default": ""}
   ]
}
UserInfo Event

{
   "namespace":"com.unity.analytics.commons.schema",
   "name":"UserInfoEvent",
   "type":"record",
   "fields":[
       {"name": "ts",   "type": "long", "default": 0},
       {"name": "appid", "type": "string", "default": ""},
       {"name": "type", "type": "string", "default": ""},
       {"name": "userid", "type": "string", "default": ""},
       {"name": "sessionid", "type": "string", "default": ""},
       {"name": "remote_ip", "type": "string", "default": ""},
       {"name": "platform", "type": "string", "default": ""},
       {"name": "sdk_ver", "type": "string", "default": ""},
       {"name": "debug_device", "type": "boolean", "default": false},
       {"name": "user_agent", "type": "string", "default": ""},
       {"name": "submit_time", "type": "long", "default": 0},
       {"name": "sex", "type": "string", "default": ""},
       {"name": "custom_userid", "type": "string", "default": ""},
       {"name": "birth_year", "type": "int", "default": 0}
   ]
}   

Setting up a Multiplayer Project from Scratch
This document describes steps to setup a new multiplayer project from nothing using the new networking system. This step-by-step process is generic, but can be customized for many types of multiplayer games once it is started.

To get started, create a new empty Unity project.

NetworkManager Setup

The first step is to create a NetworkManager object in the project:

Add a new empty game object, from the menu Game Object -> Create Empty.
Find the newly created object in the Hierarchy View and select it
Rename the object to “NetworkManager” from the right-click context menu or by clicking on the object’s name and typing.
In the inspector window for the object, click the Add Component button
Find the component Network -> NetworkManager and add it to the object. This component manages the network state of the game.

Find the component Network -> NetworkManagerHUD and add it to the object. This component supplies a simple user interface in your game for controlling the network state.

For more details, see Using the NetworkManager.

Setup the Player Prefab

The next step is to setup the Unity Prefab that represents the player in the game. By default, the NetworkManager instantiates an object for each player by cloning the player prefab. In this example, the player object will be a simple cube.

Create a new Cube from the menu Game Object -> 3D Object -> Cube

Find the cube in the Hierarchy view and select it
Rename the object to “PlayerCube”
In the inspector window for the object click the Add Component button
Add the component Network -> NetworkIdentity to the object. This component is used to identify the object between the server and clients.

Set the “Local Player Authority” checkbox on the NetworkIdentity to true. This will allow the client to control the movement of the player object

Make a prefab from the player cube object by dragging it into the Assets window. This will create a prefab called “PlayerCube”
Delete the PlayerCube object from the scene - we don’t need it now that we have a prefab
See Player Objects.

Register the Player Prefab

Once the player prefab is created, it must be registered with the network system.

Find the NetworkManager object in the Heirarchy View and select it
Open the “Spawn Info” foldout of the inspector for the NetworkManager
Find the “Player Prefab” slot
Drag the PlayerCube prefab into the “Player Prefab” slot

Now is a good time to save the project for the first time. From the menu File -> Save Project, save the project. You should also save the scene. Lets call this scene the “offline” scene.

Player Movement (Single Player Version)

The first piece of game functionality is to move the player object. This will first be done without any networking, so it will only work in a single-player mode.

Find the PlayerCube prefab in the Asset view.
Click the Add Component button and choose “New Script”
Enter the name “PlayerMove” for the new script name. A new script will be created.
Open this new script in an editor (such as Visual Studio) by double clicking it
Add this simple movement code to the script:
using UnityEngine;

public class PlayerMove : MonoBehaviour
{
    void Update()
    {
        var x = Input.GetAxis("Horizontal")*0.1f;
        var z = Input.GetAxis("Vertical")*0.1f;

        transform.Translate(x, 0, z);
    }
}
This hooks up the cube to be controlled by the arrow keys or a controller pad. The cube only moves on the client right now - it is not networked.

Save the project again.

Test a Hosted Game

Enter play mode in the editor by clicking the play button. You should see the NetworkManagerHUD default user interface:


Press “Host” to start the game as the host of the game. This will cause a player object to be created, and the HUD will change to show the server is active. This game is running as a “host” - which is a server and a client in the same process.

See Network Concepts.

Pressing the arrow keys should make the player cube object move around.

Exit play mode by pressing the stop button in the editor.

Test Player Movement for a Client

Use the menu File -> Build Settings to open the Build Settings dialog.
Add the current scene to the build by pressing the “Add Open Scenes” button

Create a build by pressing the “Build and Run” button. This will prompt for a name for the executable, enter a name such as “networkTest”
A stand-alone player will launch, and show a resolution choice dialogue.
Choose the “windowed” checkbox and a lower resolution such as 640x480
The stand-alone player will start and show the NetworkManager HUD.
Choose “Host” from the menu to start as a host. A player cube should be created
Press the arrow keys to move the player cube around a little
Switch back to the editor and close the Build Settings dialog.
Enter play mode with the play button
From the NetworkManagerHUD user interace, choose “LAN Client” to connect to the host as a client
There should be two cubes, one for the local player on the host and one for the remote player for this client
Press the arrow keys to move the cube around
Both cube currently move! This because the movement script is not network-aware.
Make Player Movement Networked

Close the stand-alone player
Exit play mode in the editor
Open the PlayerMove script.
Update the script to only move the local player
Add “using UnityEngine.Networking”
Change “MonoBehaviour” to “NetworkBehaviour”
Add a check for “isLocalPlayer” in the Update function, so that only the local player processes input
using UnityEngine;
using UnityEngine.Networking;

public class PlayerMove : NetworkBehaviour
{
    void Update()
    {
        if (!isLocalPlayer)
            return;

        var x = Input.GetAxis("Horizontal")*0.1f;
        var z = Input.GetAxis("Vertical")*0.1f;

        transform.Translate(x, 0, z);
    }
}
Find the PlayerCube prefab in the Asset View and select
Click the “Add Component” button and add the Networking -> NetworkTransform component. This component makes the object sychronize it’s position across the network.

Save the Project again
Test Multiplayer Movement

Build and run the stand-alone player again and start as host
Enter play mode in the editor and connect as a client
The player objects should now move independently of each other, and are controlled by the local player on their client.
Identify Your Player

The cubes in the game are currently all white, so the user cannot tell which one is their cube. To identify the player, we will make the cube of the local player red.

Open the PlayerMove script
Add an implementation of the OnStartLocalPlayer function to change the player object’s color.
    public override void OnStartLocalPlayer()
    {
        GetComponent<MeshRenderer>().material.color = Color.red;
    }
This function is only called on the local player on their client. This will make the user see their cube as red. The OnStartLocalPlayer function is a good place to do initialization that is only for the local player, such as configuring cameras and input.

There are also other useful virtual functions on the NetworkBehaviour base class. See Spawning.

Build and run the game
The cube controlled by the local player should now be red, while the others are still white.
Shooting Bullets (Not Networked)

A common feature in multiplayer games is to have player fire bullets. This section adds non-networked bullets to the example. Networking for bullets is added in the next section.

Create a sphere game object

Rename the sphere object to “Bullet”
Change scale of the bullet from 1.o to 0.2
Drag the bullet to the assets folder to make a prefab of the bullet
Delete the bullet object from the scene
Add a Rigidbody component to the bullet

Set the “Use Gravity” checkbox on the rigidbody to false
Update the PlayerMove script to fire bullets:
Add a public slot for the bullet prefab
Add input handling in Update() function
Add a function to fire a bullet
using UnityEngine;
using UnityEngine.Networking;

public class PlayerMove : NetworkBehaviour
{
    public GameObject bulletPrefab;

    public override void OnStartLocalPlayer()
    {
        GetComponent<MeshRenderer>().material.color = Color.red;
    }

    void Update()
    {
        if (!isLocalPlayer)
            return;

        var x = Input.GetAxis("Horizontal")*0.1f;
        var z = Input.GetAxis("Vertical")*0.1f;

        transform.Translate(x, 0, z);

        if (Input.GetKeyDown(KeyCode.Space))
        {
            Fire();
        }
    }

    void Fire()
    {
        // create the bullet object from the bullet prefab
        var bullet = (GameObject)Instantiate(
            bulletPrefab,
            transform.position - transform.forward,
            Quaternion.identity);

        // make the bullet move away in front of the player
        bullet.GetComponent<Rigidbody>().velocity = -transform.forward*4;
        
        // make bullet disappear after 2 seconds
        Destroy(bullet, 2.0f);        
    }
}
Save the script and return to the editor
Select the PlayerCube prefab and find the PlayerMove component
Find the bulletPrefab slot on the component
Drag the bull prefab into the bulletPrefab slot

Make a build then start the stand-alone player as the host
Enter play mode in the editor and connect as a client
Pressing the space bar should cause a bullet to be created and fired from the player object
The bullet is not fired on other clients, only the one where the space bar was pressed.
Shooting Bullets with Networking

This section adds networking to the bullets in the example.

Find the bullet prefab and select it
Add NetworkIdentity to the bullet prefab
Add NetworkTransform component to the bullet prefab
Set the send rate to zero on the NetworkTransform component on the bullet prefab. The bullet doesnt change direction or velocity after it is shot, so it does not need to send movement updates.

Select the NetworkManager and open the “Spawn Info” foldout
Add a new spawn prefab with the plus button
Drag the Bullet prefab into the new spawn prefab slot

Open the PlayerMove script
Update the PlayerMove script to network the bullet:
Change the Fire function to be a networked command, by adding the [Command] custom attribute and the “Cmd” prefix
Use Network.Spawn() on bullet object
using UnityEngine;
using UnityEngine.Networking;

public class PlayerMove : NetworkBehaviour
{
    public GameObject bulletPrefab;
    
    public override void OnStartLocalPlayer()
    {
        GetComponent<MeshRenderer>().material.color = Color.red;
    }

    [Command]
    void CmdFire()
    {
       // This [Command] code is run on the server!

       // create the bullet object locally
       var bullet = (GameObject)Instantiate(
            bulletPrefab,
            transform.position - transform.forward,
            Quaternion.identity);

       bullet.GetComponent<Rigidbody>().velocity = -transform.forward*4;
       
       // spawn the bullet on the clients
       NetworkServer.Spawn(bullet);
       
       // when the bullet is destroyed on the server it will automaticaly be destroyed on clients
       Destroy(bullet, 2.0f);
    }

    void Update()
    {
        if (!isLocalPlayer)
            return;

        var x = Input.GetAxis("Horizontal")*0.1f;
        var z = Input.GetAxis("Vertical")*0.1f;

        transform.Translate(x, 0, z);

        if (Input.GetKeyDown(KeyCode.Space))
        {
            // Command function is called from the client, but invoked on the server
            CmdFire();
        }
    }
}
This code uses a [Command] to fire the bullet on the server. For more information see Networked Actions.

Make a build then start the stand-alone player as the host
Enter play mode in the editor and connect as a client
Pressing the space bar should make bullet fire for the correct player (only) on all clients
Bullet Collisions

This adds a collision handler so that bullets will disappear when they hit a player cube object.

Find the bullet prefab and select it
Choose the Add Component button and add a new script
Call the new script “Bullet”
Open the new script and add the collision handler that destroys the bullet when it hits a player object
using UnityEngine;

public class Bullet : MonoBehaviour
{
    void OnCollisionEnter(Collision collision)
    {
        var hit = collision.gameObject;
        var hitPlayer = hit.GetComponent<PlayerMove>();
        if (hitPlayer != null)
        {
            Destroy(gameObject);
        }
    }
}

Now when a bullet hits a player object it will be destroyed. When the bullet on the server is destroyed, since it is a spawned object managed by the network, it will be destroyed on clients too.

Player State (Non-Networked Health)

A common feature related to bullets is that the player object has a “health” property that starts at a full value and then is reduced when the player takes damage from a bullet hitting them. This section adds non-networked health to the player object.

Select the PlayerCube prefab
Choose the Add Component button and add a new script
Call the script “Combat”
Open the Combat script, add the health variables and TakeDamage function
using UnityEngine;

public class Combat : MonoBehaviour 
{
    public const int maxHealth = 100;
    public int health = maxHealth;

    public void TakeDamage(int amount)
    {
        health -= amount;
        if (health <= 0)
        {
            health = 0;
            Debug.Log("Dead!");
        }
    }
}
The bullet script needs to be updated to call the TakeDamage function on a hit. * Open the bullet script * Add a call to TakeDamage() from the Combat script in the collision handler function

using UnityEngine;

public class Bullet : MonoBehaviour
{
    void OnCollisionEnter(Collision collision)
    {
        var hit = collision.gameObject;
        var hitPlayer = hit.GetComponent<PlayerMove>();
        if (hitPlayer != null)
        {
            var combat = hit.GetComponent<Combat>();
            combat.TakeDamage(10);

            Destroy(gameObject);
        }
    }
}
This will make health on the player object go down when hit by a bullet. But you cannot see this happening in the game. We need to add a simple health bar. * Select the PlayerCube prefab * Choose the Add Component button and add a new script called HealthBar * Open the HealthBar script

This is a lot of code that uses the old GUI system. This is not very relevant for networking so we’ll just use it without explaination for now.

using UnityEngine;
using System.Collections;

public class HealthBar : MonoBehaviour 
{
    GUIStyle healthStyle;
    GUIStyle backStyle;
    Combat combat;

    void Awake()
    {
        combat = GetComponent<Combat>();
    }

    void OnGUI()
    {
        InitStyles();

        // Draw a Health Bar

        Vector3 pos = Camera.main.WorldToScreenPoint(transform.position);
        
        // draw health bar background
        GUI.color = Color.grey;
        GUI.backgroundColor = Color.grey;
        GUI.Box(new Rect(pos.x-26, Screen.height - pos.y + 20, Combat.maxHealth/2, 7), ".", backStyle);
        
        // draw health bar amount
        GUI.color = Color.green;
        GUI.backgroundColor = Color.green;
        GUI.Box(new Rect(pos.x-25, Screen.height - pos.y + 21, combat.health/2, 5), ".", healthStyle);
    }

    void InitStyles()
    {
        if( healthStyle == null )
        {
            healthStyle = new GUIStyle( GUI.skin.box );
            healthStyle.normal.background = MakeTex( 2, 2, new Color( 0f, 1f, 0f, 1.0f ) );
        }

        if( backStyle == null )
        {
            backStyle = new GUIStyle( GUI.skin.box );
            backStyle.normal.background = MakeTex( 2, 2, new Color( 0f, 0f, 0f, 1.0f ) );
        }
    }
    
    Texture2D MakeTex( int width, int height, Color col )
    {
        Color[] pix = new Color[width * height];
        for( int i = 0; i < pix.Length; ++i )
        {
            pix[ i ] = col;
        }
        Texture2D result = new Texture2D( width, height );
        result.SetPixels( pix );
        result.Apply();
        return result;
    }
}
Save the project
Build and Run the game and see health bar on the player object
If a player shoots another player now, the health goes down on that particular client, but not on other clients.
Player State (Networked Health)

Changes to health are being applied everywhere now - independently on the client and host. This allows health to look different for different players. Health should only be applied on the server and the changes replicated to clients. We call this “server authority” for health.

Open the Combat script
Change script to be a NetworkBehaviour
Make health a [SyncVar]
Add isServer check to TakeDamage, so it will only be applied on the server
For more information on SyncVars, see State Synchronization.

using UnityEngine;
using UnityEngine.Networking;

public class Combat :  NetworkBehaviour 
{
    public const int maxHealth = 100;

    [SyncVar]
    public int health = maxHealth;

    public void TakeDamage(int amount)
    {
        if (!isServer)
            return;

        health -= amount;
        if (health <= 0)
        {
            health = 0;
            Debug.Log("Dead!");
        }
    }
}

Death and Respawning

Currently, nothing currently happens when the health of a player reaches zero except a log message. To make it more of a game, when health reaches zero, the player should be teleported back to the starting location with full health.

Open the Combat script
Add a [ClientRpc] function to respawn the player object. For more information see Networked Actions.
Call the repawn function on the server when health reaches zero
using UnityEngine;
using UnityEngine.Networking;

public class Combat :  NetworkBehaviour 
{
    public const int maxHealth = 100;

    [SyncVar]
    public int health = maxHealth;

    public void TakeDamage(int amount)
    {
        if (!isServer)
            return;

        health -= amount;
        if (health <= 0)
        {
            health = maxHealth;

            // called on the server, will be invoked on the clients
            RpcRespawn();
        }
    }

    [ClientRpc]
    void RpcRespawn()
    {
        if (isLocalPlayer)
        {
            // move back to zero location
            transform.position = Vector3.zero;
        }
    }
}
In this game, the client controls the position of the player object - the player object has “local authority” on the client. If the server just set the player’s position to the start position, it would be overridden by the client, since the client has authority. To avoid this, the server tells the owning client to move the player object to the start position.

Build and run the game
Move the player objects away from the start position
Shoot bullets at one player until their health reaches zero
The player object should teleport to the starting position.
Non-Player Objects

While player objects are spawned when client connect to the host, most games have non-player objects that exist in the game world, such as enemies. In this section a spawner is added that creates non-player objects that can be shot and killed.

From the GameObject menu create a new empty game object
Rename this object to “EnemySpawner”
Select the EnemySpawner object
Choose the Add Component button and add a NetworkIdentity to the object
In the NetworkIdentity click the “Server Only” checkbox. This makes the spawner not be sent to clients.
Choose the Add Component button and create a new script called “EnemySpawner”
Edit the new script
Make it a NetworkBehaviour
Implement the virtual function OnStartServer to create the enemies
using UnityEngine;
using UnityEngine.Networking;

public class EnemySpawner : NetworkBehaviour {

    public GameObject enemyPrefab;
    public int numEnemies;

    public override void OnStartServer()
    {
        for (int i=0; i < numEnemies; i++)
        {
            var pos = new Vector3(
                Random.Range(-8.0f, 8.0f),
                0.2f,
                Random.Range(-8.0f, 8.0f)
                );

            var rotation = Quaternion.Euler( Random.Range(0,180), Random.Range(0,180), Random.Range(0,180));

            var enemy = (GameObject)Instantiate(enemyPrefab, pos, rotation);
            NetworkServer.Spawn(enemy);
        }
    }
}
Now create an Enemy prefab:

From the GameObject menu create a new Capsule.
Rename the object to “Enemy”
Choose the Add Component button add a NetworkIdentity component to the Enemy
Choose the Add Component button add a NetworkTransform component to the Enemy
Drag the Enemy object into the Asset view to create a prefab
there should be a prefab asset now called “Enemy”
Delete the Enemy object from the scene
Select the Enemy prefab
Choose the Add Component button and add the Combat script to the Enemy
Choose the Add Component button and add the HealthBar script to the Enemy
Select the NetworkManager and in Spawn Info add a new spawnable prefab
Set the new spawn prefab to the Enemy Prefab
The bullet script was setup to only work for players. Now update the bullet script to work with any object that has the Combat script on it:

Open the Bullet script
Change the collision check to use Combat instead of PlayerMove:
using UnityEngine;

public class Bullet : MonoBehaviour
{
    void OnCollisionEnter(Collision collision)
    {
        var hit = collision.gameObject;
        var hitCombat = hit.GetComponent<Combat>();
        if (hitCombat != null)
        {
            hitCombat.TakeDamage(10);
            Destroy(gameObject);
        }
    }
}
Hookup the EnemySpawner with the Enemy object:

Select the EnemySpawner object
Find the “Enemy” slot on the EnemySpawner component
Drag the Enemy prefab into the slot
Set the numEnemies value to 4
Test Enemies:

Build and run the game
When starting as Host, four enemies should be created at random locations
The player should be able to shoot enemies, and their health should go down
When the client joins they should see the enemies in the same positions, and same health values as on the server
Destroying Enemies

While the enemies can be shot by bullets and their health goes down, then respawn like players. Enemies should be destroyed when their health reaches zero instead of respawning.

Open the Combat script
Add a “destroyOnDeath” variable
Check destroyOnDeath when health reaches zero
using UnityEngine;
using UnityEngine.Networking;

public class Combat :  NetworkBehaviour 
{
    public const int maxHealth = 100;
    public bool destroyOnDeath;

    [SyncVar]
    public int health = maxHealth;

    public void TakeDamage(int amount)
    {
        if (!isServer)
            return;

        health -= amount;
        if (health <= 0)
        {
            if (destroyOnDeath)
            {
                Destroy(gameObject);
            }
            else
            {
                health = maxHealth;

                // called on the server, will be invoked on the clients
                RpcRespawn();
            }
        }
    }

    [ClientRpc]
    void RpcRespawn()
    {
        if (isLocalPlayer)
        {
            // move back to zero location
            transform.position = Vector3.zero;
        }
    }
}

Select the Enemy prefab
Set the destroyOnDeath checkbox to true for the Enemy
Now the enemy will be destroyed when health reaches zero, but players will respawn.

Spawn Positions for players

Players currently all appear at the zero point when they are created. This means that they are potentially on top of each other. Player should spawn at different locations. The NetworkStartPosition component can be used to do this.

Create a new empty GameObject
Rename the object to “Pos1”
Choose the Add Component button and add the NetworkStartPosition component
Move the Pos1 object to the position (–3,0,0)
Create a second empty GameObject
Rename the object to “Pos2”
Choose the Add Component button and add the NetworkStartPosition component
Move the Pos2 object to the position (3,0,0)
Find the NetworkManager and select it.
Open the “Spawn Info” foldout
Change the “Player Spawn Method” to “Round Robin”
Build and run the game
Player objects should now be created at the locations of the Pos1 and Pos2 objects instead of at zero.

Optimizing Scripts
This section demonstrates how you would go about optimizing the actual scripts and methods your game uses, and it also goes into detail about the reasons why the optimizations work, and why applying them will benefit you in certain situations.

Profiler is King

There is no such thing as a list of boxes to check that will ensure your project runs smoothly. To optimize a slow project, you have to profile to find specific offenders that take up a disproportionate amount of time. Trying to optimize without profiling or without thoroughly understanding the results that the profiler gives is like trying to optimize with a blindfold on.

Internal mobile profiler

You can use the internal profiler to figure out what kind of process is slowing your game down, be it physics, scripts, or rendering, but you can’t drill down into specific scripts and methods to find the actual offenders. However, by building switches into your game which enable and disable certain functionality, you can narrow down the worst offenders significantly. For example, if you remove the enemy characters’ AI script and the framerate doubles, you know that the script, or something that it brings into the game, has to be optimized. The only problem is that you may have to try a lot of different things before you find the problem.

For more about profiling on mobile devices, see the profiling section.

Optimized by Design

Attempting to develop something which is fast from the beginning is risky, because there is a trade-off between wasting time making things that would be just as fast if they weren’t optimized and making things which will have to be cut or replaced later because they are too slow. It takes intuition and knowledge of the hardware to make good decisions in this regard, especially because every game is different and what might be a crucial optimization for one game may be a flop in another.

Object Pooling

We gave object pooling as an example of the intersection between good gameplay and good code design in the introduction to optimized scripting methods. Using object pooling for ephemeral objects is faster than creating and destroying them, because it makes memory allocation simpler and removes dynamic memory allocation overhead and Garbage Collection, or GC.

Memory Allocation

Simple Explanation of what Automatic Memory Management is

Scripts you write in Unity use automatic memory management. Just about all scripting languages do this. In contrast, lower level languages such as C and C++ use manual memory allocation, where the programmer is allowed to read and write from memory addresses directly, and as a consequence they are responsible for removing every object they create. For example, if you create objects in your C++, you have to manually de-allocate the memory that they take up when you are done with them. In a scripting language, it is enough to say objectReference = null;

Note: If I have a game object variable like GameObject myGameObject; or var myGameObject : GameObject;, why isn’t it destroyed when I say myGameObject = null;?

The game object is still referenced by Unity, because Unity has to maintain a reference to it in order for it to be drawn, updated, etc. Calling Destroy(myGameObject); removes that reference and deletes the object.
But if you create an object that Unity has no idea about, for example, an instance of a class that does not inherit from anything (in contrast, most classes or “script components” inherit from MonoBehaviour) and then set your reference variable to it to null, what actually happens is that the object is lost as far as your script and Unity are concerned; they can’t access it and will never see it again, but it stays in memory. Then, some time later, the Garbage Collector runs, and it removes anything in memory that is not referenced anywhere. It is able to do this because, behind the scenes, the number of references to each block of memory is kept track of. This is one reason why scripting languages are slower than C++.

Read more about Automatic Memory Management and the Garbage Collector.
How to Avoid Allocating Memory

Every time an object is created, memory is allocated. Very often in code, you are creating objects without even knowing it.

Debug.Log("boo" + "hoo"); creates an object.
Use System.String.Empty instead of "" when dealing with lots of strings.
Immediate Mode GUI (UnityGUI) is slow and should not be used at any time when performance is an issue.
Difference between class and struct:
Classes are objects and behave as references. If Foo is a class and

  Foo foo = new Foo();
  MyFunction(foo); 
then MyFunction will receive a reference to the original Foo object that was allocated on the heap. Any changes to foo inside MyFunction will be visible anywhere foo is referenced.

Classes are data and behave as such. If Foo is a struct and

  Foo foo = new Foo();
  MyFunction(foo); 
then MyFunction will receive a copy of foo. foo is never allocated on the heap and never garbage collected. If MyFunction modifies it’s copy of foo, the other foo is unaffected.

Objects which stick around for a long time should be classes, and objects which are ephemeral should be structs. Vector3 is probably the most famous struct. If it were a class, everything would be a lot slower.
Why Object Pooling is Faster

The upshot of this is that using Instantiate and Destroy a lot gives the Garbage Collector a lot to do, and this can cause a “hitch” in gameplay. As the Automatic Memory Management page explains, there are other ways to get around the common performance hitches that surround Instantiate and Destroy, such as triggering the Garbage Collector manually when nothing is going on, or triggering it very often so that a large backlog of unused memory never builds up.

Another reason is that, when a specific prefab is instantiated for the first time, sometimes additional things have to be loaded into RAM, or textures and meshes need to be uploaded to the GPU. This can cause a hitch as well, and with object pooling, this happens when the level loads instead of during gameplay.

Imagine a puppeteer who has an infinite box of puppets, where every time the script calls for a character to appear, they get a new copy of its puppet out of the box, and every time the character exits the stage, they toss the current copy. Object pooling is the equivalent of getting all the puppets out of the box before the show starts, and leaving them on the table behind the stage whenever they are not supposed to be visible.

Why Object Pooling can be Slower

One issue is that the creation of a pool reduces the amount of heap memory available for other purposes; so if you keep allocating memory on top of the pools you just created, you might trigger garbage collection even more often. Not only that, every collection will be slower, because the time taken for a collection increases with the number of live objects. With these issues in mind, it should be apparent that performance will suffer if you allocate pools that are too large or keep them active when the objects they contain will not be needed for some time. Furthermore, many types of objects don’t lend themselves well to object pooling. For example, the game may include spell effects that persist for a considerable time or enemies that appear in large numbers but which are only killed gradually as the game progresses. In such cases, the performance overhead of an object pool greatly outweighs the benefits and so it should not be used.

Implementation

Here’s a simple side by side comparison of a script for a simple projectile, one using Instantiation, and one using Object Pooling.

 // GunWithInstantiate.js                                                  // GunWithObjectPooling.js

 #pragma strict                                                            #pragma strict

 var prefab : ProjectileWithInstantiate;                                   var prefab : ProjectileWithObjectPooling;
                                                                           var maximumInstanceCount = 10;
 var power = 10.0;                                                         var power = 10.0;

                                                                           private var instances : ProjectileWithObjectPooling[];

                                                                           static var stackPosition = Vector3(-9999, -9999, -9999);

                                                                           function Start () {
                                                                               instances = new ProjectileWithObjectPooling[maximumInstanceCount];
                                                                               for(var i = 0; i < maximumInstanceCount; i++) {
                                                                                   // place the pile of unused objects somewhere far off the map
                                                                                   instances[i] = Instantiate(prefab, stackPosition, Quaternion.identity);
                                                                                   // disable by default, these objects are not active yet.
                                                                                   instances[i].enabled = false;
                                                                               }
                                                                           }

 function Update () {                                                      function Update () {
     if(Input.GetButtonDown("Fire1")) {                                        if(Input.GetButtonDown("Fire1")) {
         var instance : ProjectileWithInstantiate =                                var instance : ProjectileWithObjectPooling = GetNextAvailiableInstance();
             Instantiate(prefab, transform.position, transform.rotation);          if(instance != null) {
         instance.velocity = transform.forward * power;                                instance.Initialize(transform, power);
     }                                                                             }
 }                                                                             }
                                                                           }

                                                                           function GetNextAvailiableInstance () : ProjectileWithObjectPooling {
                                                                               for(var i = 0; i < maximumInstanceCount; i++) {
                                                                                   if(!instances[i].enabled) return instances[i];
                                                                               }
                                                                               return null;
                                                                           }




 // ProjectileWithInstantiate.js                                           // ProjectileWithObjectPooling.js

 #pragma strict                                                            #pragma strict

 var gravity = 10.0;                                                       var gravity = 10.0;
 var drag = 0.01;                                                          var drag = 0.01;
 var lifetime = 10.0;                                                      var lifetime = 10.0;

 var velocity : Vector3;                                                   var velocity : Vector3;

 private var timer = 0.0;                                                  private var timer = 0.0;

                                                                           function Initialize(parent : Transform, speed : float) {
                                                                               transform.position = parent.position;
                                                                               transform.rotation = parent.rotation;
                                                                               velocity = parent.forward * speed;
                                                                               timer = 0;
                                                                               enabled = true;
                                                                           }

 function Update () {                                                      function Update () {
     velocity -= velocity * drag * Time.deltaTime;                             velocity -= velocity * drag * Time.deltaTime;
     velocity -= Vector3.up * gravity * Time.deltaTime;                        velocity -= Vector3.up * gravity * Time.deltaTime;
     transform.position += velocity * Time.deltaTime;                          transform.position += velocity * Time.deltaTime;

     timer += Time.deltaTime;                                                  timer += Time.deltaTime;
     if(timer > lifetime) {                                                    if(timer > lifetime) {
                                                                                   transform.position = GunWithObjectPooling.stackPosition;
         Destroy(gameObject);                                                      enabled = false;
     }                                                                         }
 }                                                                         }


Of course, for a large, complicated game, you will want to make a generic solution that works for all your prefabs.

Another Example: Coin Party!

The example of “Hundreds of rotating, dynamically lit, collectable coins onscreen at once” which was given in the Scripting Methods section will be used to demonstrate how script code, Unity components like the Particle System, and custom shaders can be used to create a stunning effect without taxing the weak mobile hardware.

Imagine that this effect lives in the context of a 2D sidescrolling game with tons of coins that fall, bounce, and rotate. The coins are dynamically lit by point lights. We want to capture the light glinting off the coins to make our game more impressive.

If we had powerful hardware, we could use a standard approach to this problem. Make every coin an object, shade the object with either vertex-lit, forward, or deferred lighting, and then add glow on top as an image effect to get the brightly reflecting coins to bleed light onto the surrounding area.

But mobile hardware would choke on that many objects, and a glow effect is totally out of the question. So what do we do?


Animated Sprite Particle System

If you want to display a lot of objects which all move in a similar way and can never be carefully inspected by the player, you might be able to render large amounts of them in no time using a particle system. Here are a few stereotypical applications of this technique:

Collectables or Coins
Flying Debris
Hordes or Flocks of Simple Enemies
Cheering Crowds
Hundreds of Projectiles or Explosions
There is a free editor extension called Sprite Packer that facilitates the creation of animated sprite particle systems. It renders frames of your object to a texture, which can then be used as an animated sprite sheet on a particle system. For our use case, we would use it on our rotating coin.


Reference Implementation

Included in the Sprite Packer project is an example that demonstrates a solution to this exact problem.

It uses a family of assets of all different kinds to achieve a dazzling effect on a low computing budget:

A control script
Specialized textures created from the output of the SpritePacker
A specialized shader which is intimately connected with both the control script and the texture.
A readme file is included with the example which attempts to explain why and how the system works, outlining the process that was used to determine what features were needed and how they were implemented. This is that file:

The problem was defined as “Hundreds of rotating, dynamically lit, collectable coins onscreen at once.”

The naive approach is to Instantiate a bunch of copies of a coin prefab, but instead we are going to use particles to render our coins. However, this introduces a number of challenges that we have to overcome.

Viewing angles are a problem because particles don’t have them.
We assume that the camera stays right-side up and the coins rotate around the Y-axis.
We create the illusion of coin rotation with an animated texture that we packed using the SpritePacker.
This introduces a new problem: Monotony of rotating coins all rotating at the same speed and in the same direction
We keep track of rotation and lifetime ourselves and “render” rotation to the particle lifetimes in script to fix this.
Normals are a problem because particles don’t have them, and we need real time lighting.
Generate a single normal vector for the face of the coin in each animation frame generated by the Sprite Packer.
Do Blinn-Phong lighting for each particle in script, based on the normal vector grabbed from the above list.
Apply the result to the particle as a color.
Handle the face of the coin and the rim of the coin separately in the shader. Introduces a new problem: How does the shader know where the rim is, and what part of the rim it’s on?
Can’t use UV’s, they are already used for the animation.
Use a texture map.
Need Y-position relative to coin.
Need binary “on face” vs “on rim”.
We don’t want to introduce another texture, more texture reads, more texture memory.
Combine needed information into one channel and replace one of the texture’s color channels with it.
Now our coin is the wrong color! What do we do?
Use the shader to reconstruct missing channel as a combination of the two remaining channels.
Say we want glow from light glinting off our coins. Post process is too expensive for mobile devices.
Create another particle system and give it a softened, glowy version of the coin animation.
Color a glow only when the corresponding coin’s color is super bright.
Can’t have glow rendered on every coin every frame - fill rate killer.
Reset glows every frame, only position ones with brightness > 0.
Physics is a problem, collecting coins is a problem - particles don’t collide very well.
Could use built-in particle collision?
Instead, just wrote collision into the script.
Finally, we have one more problem - this script does a lot, and its getting slow!
Performance scales linearly with number of active coins.
Limit maximum coins. This works well enough to acheive our goal: 100 coins, 2 lights, runs really fast on mobile devices.
Things to try to optimize further:
Instead of calculating lighting for every coin individually, cut the world into chunks and calculate lighting conditions for every rotation frame in every chunk.
Use as a lookup table with coin position and coin rotation as indices.
Increase fidelity by using bilinear interpolation with position.
Sparse updates on the lookup table, or, entirely static lookup table.
Use Light Probes for this? *Instead of calculating lighting in script, use normal-mapped particles?
Use “Display Normals” shader to bake frame animation of normals.
Limits number of lights.
Fixes slow script problem.
The end goal of this example or “moral of the story” is that if there is something which your game really needs, and it causes lag when you try to achieve it through conventional means, that doesn’t mean that it is impossible, it just means that you have to put in some work on a system of your own that runs much faster.

Techniques for Managing Thousands of Objects

These are specific scripting optimizations which are applicable in situations where hundreds or thousands of dynamic objects are involved. Applying these techniques to every script in your game is a terrible idea; they should be reserved as tools and design guidelines for large scripts which handle tons of objects or data at run time.

Avoid or minimize O(n2) operations on large data sets

In computer science, the Order of an operation, denoted by O(n), refers to the way that the number of times that the operation has to be evaluated increases as the number of objects it is applied to (n) increases.

For example, consider a basic sorting algorithm. I have n numbers and I want to sort them from smallest to largest.

 void sort(int[] arr) {
    int i, j, newValue;
    for (i = 1; i < arr.Length; i++) {
        // record
        newValue = arr[i];
        //shift everything that is larger to the right
        j = i;
        while (j > 0 && arr[j - 1] > newValue) {
            arr[j] = arr[j - 1];
            j--;
        }
        // place recorded value to the left of large values
        arr[j] = newValue;
    }
 }
The important part is that there are two loops here, one inside the other.

 for (i = 1; i < arr.Length; i++) {
    ...
    j = i;
    while (j > 0 && arr[j - 1] > newValue) {
        ...
        j--;
    }
 }
Let’s assume that we give the algorithm the worst possible case: the input numbers are sorted, but in reverse order. In that case, the innermost loop will run j times. On average, as i goes from 1 to arr.Length–1, j will be arr.Length/2. In terms of O(n), arr.Length is our n, so, in total, the innermost loop runs nn/2* times, or n2/2** times. But in O(n) terms, we chuck all constants like 1/2, because we want to talk about the way that the number of operations increases, not the actual number of operations. So the algorithm is O(n2). The order of an operation matters a lot if the data set is large, because the number of operations can explode exponentially.

An in-game example of an O(n2) operation is 100 enemies, where the AI of each enemy takes the movements of every other enemy into account. It might be faster to divide the map into cells, record the movement of each enemy into the nearest cell, and then have each enemy sample the nearest few cells. That would be an O(n) operation.

Cache references instead of performing unnecessary searches

Say you have 100 enemies in your game, and they all move towards the player.

 // EnemyAI.js
 var speed = 5.0;
 
 function Update () {
    transform.LookAt(GameObject.FindWithTag("Player").transform);
    // this would be even worse:
    //transform.LookAt(FindObjectOfType(Player).transform);
 
    transform.position += transform.forward * speed * Time.deltaTime;
 }
That could be slow, if there are enough of them running at the same time. Little known fact: all of the component accessors in MonoBehaviour, things like transform, renderer, and audio, are equivalent to their GetComponent(Transform) counterparts, and they are actually a bit slow. GameObject.FindWithTag has been optimized, but in some cases, for example, in inner loops, or on scripts that run on a lot of instances, this script might be a bit slow.

This is a better version of the script.

 // EnemyAI.js
 var speed = 5.0;
 
 private var myTransform : Transform;
 private var playerTransform : Transform;
 
 function Start () {
    myTransform = transform;
    playerTransform = GameObject.FindWithTag("Player").transform;
 }
 
 function Update () {
    myTransform.LookAt(playerTransform);
 
    myTransform.position += myTransform.forward * speed * Time.deltaTime;
 }

Minimize expensive math functions

Transcendental functions (Mathf.Sin, Mathf.Pow, etc), Division, and Square Root all take about 100x the time of a multiplication. (In the grand scheme of things, no time at all, but if you are calling them thousands of times per frame it can add up).

The most common case of this is vector normalization. If you are normalizing the same vector over and over, consider normalizing it once instead and caching the result for use later.

If you are both using the length of a vector and normalizing it, it would be faster to obtain the normalized vector by multiplying the vector by the reciprocal of the length rather than by using the .normalized property.

If you are comparing distances, you don’t have to compare the actual distances. You can compare the squares of the distances instead by using the .sqrMagnitude property and save a square root or two.

Another one, if you are dividing over and over by a constant c, you can multiply by the reciprocal instead. Calculate the reciprocal first by doing 1.0/c.

Only execute expensive operations occasionally, e.g. Physics.Raycast()

If you have to do something expensive, you might be able to optimize it by doing it less often and caching the result. For example, consider a projectile script that uses Raycast:

 // Bullet.js
 var speed = 5.0;
 
 function FixedUpdate () {
    var distanceThisFrame = speed * Time.fixedDeltaTime;
    var hit : RaycastHit;
 
    // every frame, we cast a ray forward from where we are to where we will be next frame
    if(Physics.Raycast(transform.position, transform.forward, hit, distanceThisFrame)) {
        // Do hit
    } else {
        transform.position += transform.forward * distanceThisFrame;
    }
 }
Right away, we could improve the script by replacing FixedUpdate with Update and fixedDeltaTime with deltaTime. FixedUpdate refers to the Physics update, which happens more often than the frame update. But let’s go even further by only raycasting every n seconds. A smaller n gives greater temporal resolution, and a bigger n gives better performance. The bigger and slower your targets are, the bigger n can be before temporal aliasing occurs. (Appearance of latency, where the player hit the target, but the explosion appears where the target used to be n seconds ago, or the player hit the target, but the projectile goes right through).

 // BulletOptimized.js
 var speed = 5.0;
 var interval = 0.4; // this is 'n', in seconds.
 
 private var begin : Vector3;
 private var timer = 0.0;
 private var hasHit = false;
 private var timeTillImpact = 0.0;
 private var hit : RaycastHit;
 
 // set up initial interval
 function Start () {
    begin = transform.position;
    timer = interval+1;
 }
 
 function Update () {
    // don't allow an interval smaller than the frame.
    var usedInterval = interval;
    if(Time.deltaTime > usedInterval) usedInterval = Time.deltaTime;
 
    // every interval, we cast a ray forward from where we were at the start of this interval
    // to where we will be at the start of the next interval
    if(!hasHit && timer >= usedInterval) {
        timer = 0;
        var distanceThisInterval = speed * usedInterval;
 
        if(Physics.Raycast(begin, transform.forward, hit, distanceThisInterval)) {
            hasHit = true;
            if(speed != 0) timeTillImpact = hit.distance / speed;
        }
 
        begin += transform.forward * distanceThisInterval;
    }
 
    timer += Time.deltaTime;
 
    // after the Raycast hit something, wait until the bullet has traveled
    // about as far as the ray traveled to do the actual hit
    if(hasHit && timer > timeTillImpact) {
        // Do hit
    } else {
        transform.position += transform.forward * speed * Time.deltaTime;
    }
 }
Minimize callstack overhead in inner loops

Just calling a function has a little bit of overhead in itself. If you are calling things like x = Mathf.Abs(x) thousands of times per frame, it might be better to just do x = (x > 0 ? x : -x); instead.

Optimizing Physics Performance

The NVIDIA PhysX physics engine used by Unity is available on mobiles, but the performance limits of the hardware will be reached more easily on mobile platforms than desktops.

Here are some tips for tuning physics to get better performance on mobiles:-

You can adjust the Fixed Timestep setting (in the Time manager) to reduce the time spent on physics updates. Increasing the timestep will reduce the CPU overhead at the expense of the accuracy of the physics. Often, lower accuracy is an acceptable tradeoff for increased speed.
Set the Maximum Allowed Timestep in the Time manager in the 8–10fps range to cap the time spent on physics in the worst case scenario.
Mesh colliders have a much higher performance overhead than primitive colliders, so use them sparingly. It is often possible to approximate the shape of a mesh by using child objects with primitive colliders. The child colliders will be controlled collectively as a single compound collider by the rigidbody on the parent.
While wheel colliders are not strictly colliders in the sense of solid objects, they nonetheless have a high CPU overhead.

Rigidbody 2D
SWITCH TO SCRIPTING
A Rigidbody 2D component places an object under the control of the physics engine. Many concepts familiar from the standard Rigidbody component carry over to Rigidbody 2D; the differences are that in 2D, objects can only move in the XY plane and can only rotate on an axis perpendicular to that plane.

The Rigidbody 2D component. This appears differently in the Unity Editor depending on which Body Type you have selected. See Body Type, below, to learn more.
The Rigidbody 2D component. This appears differently in the Unity Editor depending on which Body Type you have selected. See Body Type, below, to learn more.
How a Rigidbody 2D works

Usually, the Unity Editor’s Transform component defines how a GameObject (and its child GameObjects) is positioned, rotated and scaled within the Scene. When it is changed, it updates other components, which may update things like where they render or where colliders are positioned. The 2D physics engine is able to move colliders and make them interact with each other, so a method is required for the physics engine to communicate this movement of colliders back to the Transform components. This movement and connection with colliders is what a Rigidbody 2D component is for.

The Rigidbody 2D component overrides the Transform and updates it to a position/rotation defined by the Rigidbody 2D. Note that while you can still override the Rigidbody 2D by modifying the Transform component yourself (because Unity exposes all properties on all components), doing so will cause problems such as GameObjects passing through or into each other, and unpredictable movement.

Any Collider 2D component added to the same GameObject or child GameObject is implicitly attached to that Rigidbody 2D. When a Collider 2D is attached to the Rigidbody 2D, it moves with it. A Collider 2D should never be moved directly using the Transform or any collider offset; the Rigidbody 2D should be moved instead. This offers the best performance and ensures correct collision detection. Collider 2Ds attached to the same Rigidbody 2D won’t collide with each other. This means you can create a set of colliders that act effectively as a single compound collider, all moving and rotating in sync with the Rigidbody 2D.

When designing a Scene, you are free to use a default Rigidbody 2D and start attaching colliders. These colliders allow any other colliders attached to different Rigidbody 2Ds to collide with each other.

Tip

Adding a Rigidbody 2D allows a sprite to move in a physically convincing way by applying forces from the scripting API. When the appropriate collider component is also attached to the sprite GameObject, it is affected by collisions with other moving GameObjects. Using physics simplifies many common gameplay mechanics and allows for realistic behavior with minimal coding.


Body Type

The Rigidbody 2D component has a setting at the top labelled Body Type. The option you choose for this affects the other settings available on the component.


There are three options for Body Type; each defines a common and fixed behavior. Any Collider 2D attached to a Rigidbody 2D inherits the Rigidbody 2D’s Body Type. The three options are:

Dynamic
Kinematic
Static
The option you choose defines:

Movement (position & rotation) behavior
Collider interaction
Note that although Rigidbody 2Ds are often described as colliding with each other, it is the Collider 2Ds attached to each of those bodies which collide. Rigidbody 2Ds cannot collide with each other without colliders.

Changing the Body Type of a Rigidbody 2D can be a tricky process. When a Body Type changes, various mass-related internal properties are recalculated immediately, and all existing contacts for the Collider 2Ds attached to the Rigidbody 2D need to be re-evaluated during the GameObject’s next FixedUpdate. Depending on how many contacts and Collider 2Ds are attached to the body, changing the Body Type can cause variations in performance.

Body Type: Dynamic


A Dynamic Rigidbody 2D is designed to move under simulation. It has the full set of properties available to it such as finite mass and drag, and is affected by gravity and forces. A Dynamic body will collide with every other body type, and is the most interactive of body types. This is the default body type for a Rigidbody 2D, because it is the most common body type for things that need to move. It’s also the most performance-expensive body type, because of its dynamic nature and interactivity with everything around it. All Rigidbody 2D properties are available with this body type.

Do not use the Transform component to set the position or rotation of a Dynamic Rigidbody 2D. The simulation repositions a Dynamic Rigidbody 2D according to its velocity; you can change this directly via forces applied to it by scripts, or indirectly via collisions and gravity.

Property:	Function:
Body Type	Set the RigidBody 2D’s component settings, so that you can manipulate movement (position and rotation) behavior and Collider 2D interaction. 
Options are: Dynamic, Kinematic, Static
Material	Use this to specify a common material for all Collider 2Ds attached to a specific parent Rigidbody 2D. 
Note: A Collider 2D uses its own Material property if it has one set. If there is no Material specified here or in the Collider 2D, the default option is None (Physics Material 2D). This uses a default Material which you can set in the Physics 2D Settings window. 
A Collider 2D uses the following order of priority to determine which Material setting to use: 
1. A Physics Material 2D specified on the Collider 2D itself.
2. A Physics Material 2D specified on the attached Rigidbody 2D.
A Physics Material 2D default material specified in the Physics 2D Settings.
TIP: Use this to ensure that all Collider 2Ds attached to the same Static Body Type Rigidbody 2D can all use the same Material.
Simulated	Enable Simulated (check the box) if you want the Rigidbody 2D and any attached Collider 2Ds and Joint 2Ds to interact with the physics simulation during run time. If this is disabled (the box is unchecked), these components do not interact with the simulation. See Rigidbody 2D properties: Simulated, below, for more details. This box is checked by default.
Use Auto Mass	Check the box if you want the Rigidbody 2D to automatically detect the GameObject’s mass from its Collider 2D.
Mass	Define the mass of the Rigidbody 2D. This is grayed out if you have selected Use Auto Mass.
Linear Drag	Drag coefficient affecting positional movement.
Angular Drag	Drag coefficient affecting rotational movement.
Gravity Scale	Define the degree to which the GameObject is affected by gravity.
        Collision Detection	Define how collisions with other GameObjects are detected.
        Discrete	A collision is registered only if the GameObject’s Collider 2D is in contact with another during a physics update.
Sleeping Mode	Define how the GameObject “sleeps” to save processor time when it is at rest.
        Never Sleep	Sleeping is disabled (this should be avoided where possible, as it can impact system resources).
        Start Awake	GameObject is initially awake.
        Start Asleep	GameObject is initially asleep but can be woken by collisions.
Interpolate	Define how the GameObject’s movement is interpolated between physics updates (useful when motion tends to be jerky).
        None	No movement smoothing is applied.
        Interpolate	Movement is smoothed based on the GameObject’s positions in previous frames.
        Extrapolate	Movement is smoothed based on an estimate of its position in the next frame.
Constraints	Define any restrictions on the Rigidbody 2D’s motion.
Freeze Position	Stops the Rigidbody 2D moving in the world X & Y axes selectively.
Freeze Rotation	Stops the Rigidbody 2D rotating around the Z axes selectively.
Body Type: Kinematic


A Kinematic Rigidbody 2D is designed to move under simulation, but only under very explicit user control. While a Dynamic Rigidbody 2D is affected by gravity and forces, a Kinematic Rigidbody 2D isn’t. For this reason, it is fast and has a lower demand on system resources than a Dynamic Rigidbody 2D. Kinematic Rigidbody 2D is designed to be repositioned explicitly via Rigidbody2D.MovePosition or Rigidbody2D.MoveRotation. Use physics queries to detect collisions, and scripts to decide where and how the Rigidbody 2D should move.

A Kinematic Rigidbody 2D does still move via its velocity, but the velocity is not affected by forces or gravity. A Kinematic Rigidbody 2D does not collide with other Kinematic Rigidbody 2Ds or with Static Rigidbody 2Ds; it only collides with Kinematic Rigidbody 2Ds. Similar to a Static Rigidbody 2D (see below), a Kinematic Rigidbody 2D behaves like an immovable object (as if it has infinite mass) during collisions. Mass-related properties are not available with this Body Type.

Property:	Function:
Body Type	Set the RigidBody 2D’s component settings, so that you can manipulate movement (position and rotation) behavior and Collider 2D interaction. 
Options are: Dynamic, Kinematic, Static
Material	Use this to specify a common material for all Collider 2Ds attached to a specific parent Rigidbody 2D. 
Note: A Collider 2D uses its own Material property if it has one set. If there is no Material specified here or in the Collider 2D, the default option is None (Physics Material 2D). This uses a default Material which you can set in the Physics 2D Settings window. 
A Collider 2D uses the following order of priority to determine which Material setting to use: 
1. A Physics Material 2D specified on the Collider 2D itself.
2. A Physics Material 2D specified on the attached Rigidbody 2D.
A Physics Material 2D default material specified in the Physics 2D Settings.
TIP: Use this to ensure that all Collider 2Ds attached to the same Static Body Type Rigidbody 2D can all use the same Material.
Simulated	Enable Simulated (check the box) if you want the Rigidbody 2D and any attached Collider 2Ds and Joint 2Ds to interact with the physics simulation during run time. If this is disabled (the box is unchecked), these components do not interact with the simulation. See Rigidbody 2D properties: Simulated, below, for more details. This box is checked by default.
Use Full Kinematic Contacts	Enable this setting (check the box) if you want the Kinematic Rigidbody 2D to collide with all Rigidbody 2D Body Types. This is similar to a Dynamic Rigidbody 2D, except the Kinematic Rigidbody 2D is not moved by the physics engine when contacting another Rigidbody 2D component; instead it acts as an immovable object, with infinite mass. When Use Full Kinematic Contacts is disabled, the Kinematic Rigidbody 2D only collides with Dynamic Rigidbody 2Ds. See Rigidbody 2D properties: Use Full Kinematic Contacts, below, for more details. This box is unchecked by default.
Collision Detection	Define how collisions with other GameObjects are detected.
        Discrete	A collision is registered only if the GameObject’s Collider 2D is in contact with another during a physics update.
        Continuous	A collision is registered if the GameObject’s Collider 2D appears to have contacted another between updates.
Sleeping Mode	Define how the GameObject “sleeps” to save processor time when it is at rest.
        Never Sleep	Sleeping is disabled (this should be avoided where possible, as it can impact system resources).
        Start Awake	GameObject is initially awake.
        Start Asleep	GameObject is initially asleep but can be woken by collisions.
Interpolate	Define how the GameObject’s movement is interpolated between physics updates (useful when motion tends to be jerky).
        None	No movement smoothing is applied.
        Interpolate	Movement is smoothed based on the GameObject’s positions in previous frames.
        Extrapolate	Movement is smoothed based on an estimate of its position in the next frame.
Constraints	Define any restrictions on the Rigidbody 2D’s motion.
        Freeze Position	Stops the Rigidbody 2D moving in the world’s x & y axes selectively.
        Freeze Rotation	Stops the Rigidbody 2D rotating around the world’s z axis selectively.
Body Type: Static


A Static Rigidbody 2D is designed to not move under simulation at all; if anything collides with it, a Static Rigidbody 2D behaves like an immovable object (as though it has infinite mass). It is also the least resource-intensive body type to use. A Static body only collides with Dynamic Rigidbody 2Ds. Having two Static Rigidbody 2Ds collide is not supported, since they are not designed to move.

Only a very limited set of properties are available for this Body Type.

Property:	Function:
Body Type	Set the RigidBody 2D’s component settings, so that you can manipulate movement (position and rotation) behavior and Collider 2D interaction. 
Options are: Dynamic, Kinematic, Static
Material	Use this to specify a common material for all Collider 2Ds attached to a specific parent Rigidbody 2D. 
Note: A Collider 2D uses its own Material property if it has one set. If there is no Material specified here or in the Collider 2D, the default option is None (Physics Material 2D). This uses a default Material which you can set in the Physics 2D Settings window. 
A Collider 2D uses the following order of priority to determine which Material setting to use: 
1. A Physics Material 2D specified on the Collider 2D itself.
2. A Physics Material 2D specified on the attached Rigidbody 2D.
A Physics Material 2D default material specified in the Physics 2D Settings.
TIP: Use this to ensure that all Collider 2Ds attached to the same Static Body Type Rigidbody 2D can all use the same Material.
Simulated	Enable Simulated (check the box) if you want the Rigidbody 2D and any attached Collider 2Ds and Joint 2Ds to interact with the physics simulation during run time. If this is disabled (the box is unchecked), these components do not interact with the simulation. See Rigidbody 2D properties: Simulated, below, for more details. This box is checked by default.
Use Full Kinematic Contacts	Enable this setting (check the box) if you want the Kinematic Rigidbody 2D to collide with all Rigidbody 2D Body Types. This is similar to a Dynamic Rigidbody 2D, except the Kinematic Rigidbody 2D is not moved by the physics engine when contacting another Rigidbody 2D component; instead it acts as an immovable object, with infinite mass. When Use Full Kinematic Contacts is disabled, the Kinematic Rigidbody 2D only collides with Dynamic Rigidbody 2Ds. See Rigidbody 2D properties: Use Full Kinematic Contacts, below, for more details. This box is unchecked by default.
There are two ways to mark a Rigidbody 2D as Static:

For the GameObject with the Collider 2D component not to have a Rigidbody 2D component at all. All such Collider 2Ds are internally considered to be attached to a single hidden Static Rigidbody 2D component.
For the GameObject to have a Rigidbody 2D and for that Rigidbody 2D to be set to Static.
Method 1 is a shorthand for making Static Collider 2Ds. When creating large numbers of Static Collider 2Ds, it is easier not to have to add a Rigidbody 2D for each GameObject with a Collider 2D.

Method 2 exists for performance reasons. If a Static Collider 2D needs to be moved or reconfigured at run time, it is faster to do so when it has its own Rigidbody 2D. If a group of Collider 2Ds needs to be moved or reconfigured at run time, it is faster to have them all be children of one parent Rigidbody 2D marked as Static than to move each GameObject individually.

Note: As stated above, Static Rigidbody 2Ds are designed not to move, and collisions between two Static Rigidbody 2D objects that intersect are not registered. However, Static Rigidbody 2Ds and Kinematic Rigidbody 2Ds will interact with each other if one of their Collider 2Ds is set to be a trigger. There is also a feature that changes what a Kinematic body will interact with (see Use Full Kinematic Contacts, below).

Rigidbody 2D properties


Simulated

Use the Simulated property to stop (unchecked) and start (checked) a Rigidbody 2D and any attached Collider 2Ds and Joint 2Ds from interacting with the 2D physics simulation. Changing this property is much more memory and processor-efficient than enabling or disabling individual Collider 2D and Joint 2D components.

When the Simulation box is checked, the following occurs:

The Rigidbody 2D moves via the simulation (gravity and physics forces are applied)
Any attached Collider 2Ds continue creating new contacts and continuously re-evaluate contacts
Any attached Joint 2Ds are simulated and constrain the attached Rigidbody 2D
All internal physics objects for Rigidbody 2D, Collider 2D & Joint 2D stay in memory
When the Simulated box is unchecked, the following occurs:

The Rigidbody 2D is not moved by the simulation (gravity and physics forces are not applied)
The Rigidbody 2D does not create new contacts, and any attached Collider 2D contacts are destroyed
Any attached Joint 2Ds are not simulated, and do not constrain any attached Rigidbody 2Ds
All internal physics objects for Rigidbody 2D, Collider 2D and Joint 2D are left in memory
Why is unchecking Simulated more efficient than individual component controls?

In the 2D physics simulation, a Rigidbody 2D component controls the position and rotation of attached Collider 2D components, and allows Joint 2D components to use these positions and rotations as anchor points. A Collider 2D moves when the Rigidbody 2D it is attached to moves. The Collider 2D then calculates contacts with other Collider 2Ds attached to other Rigidbody 2Ds. Joint 2Ds also constrain Rigidbody 2D positions and rotations. All of this takes simulation time.

You can stop and start individual elements of the 2D physics simulation by enabling and disabling components individually. You can do this on both Collider 2D and Joint 2D components. However, enabling and disabling individual elements of the physics simulations has memory use and processor power costs. When elements of the simulation are disabled, the 2D physics engine doesn’t produce any internal GameObjects or physics-based components to simulate. When elements of the simulation are enabled, the 2D physics engine does have internal GameObjects and physics-based components to simulate. Enabling and disabling of 2D physics simulation components means internal GameObjects and physics-based components have to be created and destroyed; disabling the simulation is easier and more efficient than disabling individual components.

NOTE: When a Rigidbody 2D’s Simulated option is unchecked, any attached Collider 2D is effectively ‘invisible’, that is; it cannot be detected by any physics queries, such as Physics.Raycast.


Use Full Kinematic Contacts

Enable this setting (check the checkbox) if you want the Kinematic Rigidbody 2D to collide with all Rigidbody 2D Body Types. This is similar to a Dynamic Rigidbody 2D, except the Kinematic Rigidbody 2D is not moved by the physics engine when contacting another Rigidbody 2D; it acts as an immovable object, with infinite mass.

When this setting is disabled (unchecked), a Kinematic Rigidbody 2D only collides with Dynamic Rigidbody 2Ds; it does not collide with other Kinematic Rigidbody 2Ds or Static Rigidbody 2Ds (note that trigger colliders are an exception to this rule). This means that no collision scripting callbacks (OnCollisionEnter, OnCollisionStay, OnCollisionExit) occur.

This can be inconvenient when you are using physics queries (such as Physics.Raycast) to detect where and how a Rigidbody 2D should move, and when you require multiple Kinematic Rigidbody 2Ds to interact with each other. Enable Use Full Kinematic Contacts to make Kinematic Rigidbody 2D components interact in this way.

Use Full Kinematic Contacts allows explicit position and rotation control of a Kinematic Rigidbody 2D, but still allows full collision callbacks. In a set-up where you need explicit control of all Rigidbody 2Ds, use Kinematic Rigidbody 2Ds in place of Dynamic Rigidbody 2Ds to still have full collision callback support.

iOS Player Settings
This page details the Player Settings specific to iOS. A description of the general Player Settings can be found here.

Note that Unity iOS requires 7.0 or higher. iOS 6.0 and earlier are not supported.

Resolution and presentation


Property:	Function:
Orientation
Default Orientation 
This setting is shared between iOS and Android devices	The game’s screen orientation. The options are: 
Portrait (home button at the bottom), 
Portrait Upside Down (home button at the top), 
Landscape Left (home button on the right side), 
Landscape Right (home button on the left side), and 
Auto Rotation (screen orientation changes with device orientation)
Use Animated Autorotation	Check this box if you want orientation changes to animate the screen rotation rather than just switch. This is only visible when Default Orientation is set to Auto Rotation.
Allowed Orientations for Auto Rotation (only visible when Default Orientation is set to Auto Rotation.)
Portrait	Allow portrait orientation.
Portrait Upside Down	Allow portrait upside-down orientation.
Landscape Right	Allow landscape right orientation (home button on the left side).
Landscape Left	Allow landscape left orientation (home button is on the right side).
Multitaking Support
Requires Fullscreen	Check this box if your game requires fullscreen.
Status Bar
Status Bar Hidden	Check this box to hide the the status bar when the application launches.
Status Bar Style	Define the style of the status bar when the application launches. The options are Default, Black Translucent and Black Opaque.
Disable Depth and Stencil	Check this box to disable the depth and stencil buffers.
Show Loading Indicator	Select how the loading indicator be displayed. The options are Don’t Show, White Large, White, and Gray.
Icon


Property:	Function:
Override for iPhone	Check this box if you want to assign a custom icon you would like to be used for your iPhone/iPad game. Different sizes of the icon fill in the squares below. If any icon textures are omitted, the icon texture with the nearest size (preferring larger resolution textures) is scaled accordingly.
Prerendered icon	If unchecked, iOS applies its standard styling effects to the application icon.
Splash Image


There are two ways to implement splash images on iOS: Launch Images and Launch Screens.

Launch Images

Launch Images are static splash screen images that occupy the entire screen.

For devices that use iOS 7, Launch Images are the only launch screen option. There is no support for versions prior to iOS 7.0. For devices that use iOS 8 or newer, you can use either Launch Images or Launch Screens.

Launch Images are defined in an Asset catalog (Images.xcassets/LaunchImage). Always add a Launch Screen for each supported size and orientation combination.

Only iPhone 6+ supports landscape orientation; other iPhones can only use portrait. Launch Images are selected in the following order:

The specific Launch Image override, if the texture is set
Default Unity splash screen launch image, which is a solid blue-black color
You need to set all Launch Images for your build.

Launch Screens

A Launch Screen is an XIB file from which iOS creates a splash screen dynamically on the device.

Launch Screens have a limitation, in that it is not possible to display different contents depending on orientation on iPad devices. Therefore, Launch Screens are only supported on iPhone devices. All iPhones support landscape Launch Screens; however, due to a bug in iOS, Landscape Left is shown instead of Landscape Right on certain iOS versions.

Property:	Function:
Mobile Splash Screen	Specifies texture which should be used for iOS Splash Screen. Standard Splash Screen size is 320x480.(This is shared between Android and iOS)
iPhone 3.5"/Retina	Specifies texture which should be used for iOS 3.5" Retina Splash Screen. Splash Screen size is 640x960.
iPhone 4"/Retina	Specifies texture which should be used for iOS 4" Retina Splash Screen. Splash Screen size is 640x1136.
iPhone 4.7"/Retina	Specifies texture which should be used for iOS 4.7" Retina Splash Screen. Splash Screen size is 750x1334.
iPhone 5.5"/Retina	Specifies texture which should be used for iOS 5.5" Retina Splash Screen. Splash Screen size is 1242x2208.
iPhone 5.5" Landscape/Retina	Specifies texture which should be used for iOS 5.5" Landscape/Retina Splash Screen. Splash Screen size is 2208x1242.
iPad Portrait	Specifies texture which should be used as iPad Portrait orientation Splash Screen. Standard Splash Screen size is 768x1024.
iPad Landscape	Specifies texture which should be used as iPad Landscape orientation Splash Screen. Standard Splash Screen size is 1024x768.
iPad Portrait/Retina	Specifies texture which should be used as the iPad Retina Portrait orientation Splash Screen. Standard Splash Screen size is 1536x2048.
iPad Landscape/Retina	Specifies texture which should be used as the iPad Retina Landscape orientation Splash Screen. Standard Splash Screen size is 2048x1536.
Launch Screen type	Allows you to select between the launch screen types
- None	The behavior is as if only launch images are used.
- Default	A launch screen that is very much like a launch image. One image is selected for portrait and landscape. The selection order: iPhone 6+ launch images, shared mobile launch image, default unity launch image for iPhone 6+. The images are displayed using aspect-fill mode.
- Image with background, relative size	A center-aligned image is shown, with the rest of area filled with solid color. The image size is user-specified percentage of the screen size, computed in the smaller dimension (vertical on landscape, horizontal in portrait orientations). User also specifies background color and images for portrait and landscape orientations. Image selection order: the user-specified image, shared mobile launch image, default unity launch image for iPhone 6+. The images are displayed using aspect-fill mode.
- Image with background, constant size	Same as relative size option except that the size of the image is defined by user-specified number of points.
- Custom Xib	An user-specified XIB file from any location.
In Unity Personal Edition the Unity Splash Screen displays as soon as engine initialises, in addition to your chosen splash screen.

Debugging and Crash Reporting


Property:	Function:
Enable Internal Profiler	Enables an internal profiler which collects performance data of the application and prints a report to the console. The report contains the number of milliseconds that it took for each Unity subsystem to execute on each frame. The data is averaged across 30 frames.
On .Net UnhandledException	The action taken on .NET unhandled exception. The options are Crash (the application crashes hardly and forces iOS to generate a crash report that can be submitted to iTunes by app users and inspected by developers), Silent Exit (the application exits gracefully).
Log ObjC uncaught exceptions	Enables a custom Objective-C Uncaught Exception handler, which will print exception information to console.
Enable Crash Report API	Enables a custom crash reporter to capture crashes. Crash logs will be available to scripts via CrashReport API.
Other Settings


Property:	Function:
Rendering
Rendering Path	The rendering path enabled for the game.
Automatic Graphics API	Allows you to select which graphics API is used. When checked, Unity will include Metal, and GLES2 as a fallback for devices where Metal is not supported. When unchecked, you can manually pick and reorder the graphics APIs. Manually picking just one API will adjust your app’s info.plist which will result in appropriate app store restrictions.
Static Batching	Set this to use Static batching on your build (enabled by default).
Dynamic Batching	Set this to use Dynamic Batching on your build (enabled by default).
GPU Skinning	Should DX11/ES3 GPU skinning be enabled?
Identification
Bundle Identifier	The string used in your provisioning certificate from your Apple Developer Network account(This is shared between iOS and Android)
Bundle Version	Specifies the build version number of the bundle, which identifies an iteration (released or unreleased) of the bundle. The version is specified in the common format of a string containing numbers separated by dots (eg, 4.3.2).
Build	The build number can be entered here to allow you to keep track of the number of builds that have been made
Apple Developer Team ID	Set this property with your Apple Developer Team ID. You can find this on the Apple Developer website under Account > Membership. This sets the Team ID for the generated Xcode project, allowing developers to use the Build and Run functionality. An Apple Developer Team ID must be set here for automatic signing of your app.
Configuration
Scripting Backend	Allows you to select between IL2CPP and Mono2x scripting backends. The default is IL2CPP, and in most normal situations there should be no reason to switch to the older Mono2x backend. Unless you are running into bugs specifically relating to IL2CPP, you should not select Mono2x. Mono2x builds are no longer accepted in the App store.
Target Device	Which devices are targeted by the game? The options are iPhone Only, iPad Only and iPhone + iPad.
Target SDK	Which SDK is targeted by the game? The options are iPhone Only, Device SDK and Simulator SDK.
Target minimum iOS Version	Defines the minimum version of iOS that the game will work on.
Use on Demand Resource	When enabled allows you to use one demand resources.
Accelerometer Frequency	How often is the accelerometer sampled? The options are Disabled (ie, no samples are taken), 15Hz, 30Hz, 60Hz and 100Hz.
Location Usage Description	This field allows you to enter the reason for accessing the users location.
Override iPod Music	If selected, the application will silence user’s iPod music. Otherwise user’s iPod music will continue playing in the background.
Prepare iOS for Recording	When selected, the microphone recording APIs are initialised. This makes recording latency lower, though on iPhones it re-routes audio output via earphones only.
Requires Persistent WiFi	Specifies whether the application requires a Wi-Fi connection. iOS maintains the active Wi-Fi connection while the application is running.
Behaviour in Background	Specifies what the application should do when the user presses the home button.
- Suspend	This is the standard behaviour, the app is suspended, but not quit.
- Exit	Instead of suspending, the app will quit when the home button is pressed.
- Custom	You can implement your own behaviour with background processing. See an example here.
Allow downloads over HTTP (nonsecure)	When this option is enabled it will allow you to download content over HTTP. Default and reccomended is HTTPS.
Supported URL schemes	A list of supported URL schemes.
Disable HW Statistics	By default, Unity iOS apps send anonymous HW statistics to Unity so we can provide you with aggregated information to help you make decisions as a developer. These stats can be found at http://stats.unity3d.com/. Checking this option disables the sending of these statistics for your app.
Architecture	Allows you to select which architecture to target. Universal is recommended default. Some apps that are shipping on high-end devices only might consider selecting the Arm64-only option. Armv7 is for consistency purposes.
- Universal	The recommended option. Supports both architectures.
- Armv7	Support only the older Armv7 architecture.
- Arm64	Support only the newer Arm64 architecture.
Scripting Define Symbols	Custom compilation flags (see the platform dependent compilation page for details).
Optimization
Api Compatibility Level	Specifies active .NET API profile. See below
- .Net 2.0	.Net 2.0 libraries. Maximum .net compatibility, biggest file sizes
- .Net 2.0 Subset	Subset of full .net compatibility, smaller file sizes
Prebake Collision Meshes	Should collision data be added to meshes at build time?
Preload Shaders	Should shaders be loaded when the player starts up?
Preloaded Assets	An array of assets to be loaded when the player starts up.
AOT compilation options	Additional AOT compiler options.
SDK Version	Specifies iPhone OS SDK version to use for building in Xcode
- Device SDK	SDK to run on actual hardware.
- Simulator SDK	SDK to run only on the simulator.
Target iOS Version	Specifies lowest iOS version where final application will able to run. Selecting a lower version will mean more devices will be able to run your app. Selecting a higher version means you gain access to features introduced in those higher versions, but users who have not upgraded their device will not be able to use the app. Selecing the ‘Unknown’ option allows you to pick one in your xcode project instead (in case there are newer/beta versions of iOS which have not yet been added to our list in the editor).
Strip Engine Code	Enable code stripping. (This setting is only available with the IL2CPP scripting backend.)
Script Call Optimization	Optionally disable exception handling for a speed boost at runtime
- Slow and Safe	Full exception handling will occur (with some performance impact on the device when using the Mono scripting backend)
- Fast but no Exceptions	No data provided for exceptions on the device (the game will run faster when using the Mono scripting backend)
Vertex Compression	Select which vertex channels should be compressed. Compression can save memory and bandwidth but precision will be lower.
Optimize Mesh Data	Remove any data from meshes that is not required by the material applied to them (tangents, normals, colors, UV).
Note: Be sure to select the correct SDK - if you select Device, say, but then target the Simulator in Xcode then the build will fail with a lot of error messages.

API Compatibility Level

You can choose your mono api compatibility level for all targets. Sometimes a 3rd party .net dll will use things that are outside of the .net compatibility level that you would like to use. In order to understand what is going on in such cases, and how to best fix it, get “Reflector” on windows.

Drag the .net assemblies for the api compatilibity level in question into reflector. You can find these in Frameworks/Mono/lib/mono/YOURSUBSET/
Also drag in your 3rd party assembly.
Right click your 3rd party assembly, and select “Analyze”.
In the analysis report, inspect the “Depends on” section. Anything that the 3rd party assembly depends on, but is not available in the .net compatibility level of your choice will be highlighted in red there.
Details

Bundle Identifier

The Bundle Identifier string must match the provisioning profile of the game you are building. The basic structure of the identifier is com.CompanyName.GameName. This structure may vary internationally based on where you live, so always default to the string provided to you by Apple for your Developer Account. Your GameName is set up in your provisioning certificates, that are manageable from the Apple iPhone Developer Center website. Please refer to the Apple iPhone Developer Center website for more information on how this is performed.

Stripping Level

Most games don’t use all necessary dlls. With this option, you can strip out unused parts to reduce the size of the built player on iOS devices. If your game is using classes that would normally be stripped out by the option you currently have selected, you’ll be presented with a Debug message when you make a build.

Script Call Optimization

A good development practice on iOS is to never rely on exception handling (either internally or through the use of try/catch blocks). When using the default Slow and Safe option, any exceptions that occur on the device will be caught and a stack trace will be provided. When using the Fast but no Exceptions option, any exceptions that occur will crash the game, and no stack trace will be provided. In addition, the AppDomain.UnhandledException event will be raised to allow project-specific code access to the exception information. With the Mono scripting backend the game will run faster since the processor is not diverting power to handle exceptions. There is no performance benefit with the Fast but no Exceptions option when using the IL2CPP scripting backend. When releasing your game to the world, it’s best to publish with the Fast but no Exceptions option.

Incremental Builds

The C++ code generated by the IL2CPP scripting backend can be updated incrementally, allowing incremental C++ build systems to compile only the changes source files. This can significantly lower iteration times with the IL2CPP scripting backend.

To use incremental builds, choose the “Append” option after selecting “Build” from the “Build Settings” dialog. The “Replace” option will perform a clean build.

Command line arguments
Unity is usually launched by double-clicking its icon from the desktop. However, it is also possible to run it from the command line (from the macOS Terminal or the Windows Command Prompt). When launched in this way, Unity can receive commands and information on startup, which can be very useful for test suites, automated builds and other production tasks.

On macOS, type the following into the Terminal to launch Unity:

/Applications/Unity/Unity.app/Contents/MacOS/Unity
On 64-bit Windows, type the following into the Command Prompt to launch Unity:

C:\Program Files\Unity\Editor\Unity.exe
On 32-bit Windows, type the following into the Command Prompt to launch Unity:

C:\Program Files (x86)\Unity\Editor\Unity.exe
Use the same method to launch standalone Unity games.

Launching Unity silently

On macOS, type the following into the Terminal to silently launch Unity:

/Applications/Unity/Unity.app/Contents/MacOS/Unity -quit -batchmode -serial    R3-XXXX-XXXX-XXXX-XXXX-XXXX -username 'JoeBloggs@example.com' -password 'MyPassw0rd'
On 64-bit Windows, type the following into the Command Prompt to silently launch Unity:

     "C:\Program Files\Unity\Editor\Unity.exe" -quit -batchmode -serial    R3-XXXX-XXXX-XXXX-XXXX-XXXX -username "JoeBloggs@example.com" -password "MyPassw0rd"
On 32-bit Windows, type the following into the Command Prompt to silently launch Unity:

 "C:\Program Files (x86)\Unity\Editor\Unity.exe" -quit -batchmode -serial    R3-XXXX-XXXX-XXXX-XXXX-XXXX -username "JoeBloggs@example.com" -password "MyPassw0rd"
Options

As mentioned above, the Editor and built Unity games can be supplied with additional commands and information on startup. This section describes the command line options available.

Command	Details
-assetServerUpdate <IP[:port] projectName username password [r <revision>]>	Force an update of the project in the Asset Server given by IP:port. The port is optional, and if not given it is assumed to be the standard one (10733). It is advisable to use this command in conjunction with the -projectPath argument to ensure you are working with the correct project. If no project name is given, then the last project opened by Unity is used. If no project exists at the path given by -projectPath, then one is created automatically.
-batchmode	Run Unity in batch mode. This should always be used in conjunction with the other command line arguments, because it ensures no pop-up windows appear and eliminates the need for any human intervention. When an exception occurs during execution of the script code, the Asset server updates fail, or other operations that fail, Unity immediately exits with return code 1. 
Note that in batch mode, Unity sends a minimal version of its log output to the console. However, the Log Files still contain the full log information. Opening a project in batch mode while the Editor has the same project open is not supported; only a single instance of Unity can run at a time.
-buildLinux32Player <pathname>	Build a 32-bit standalone Linux player (for example, -buildLinux32Player path/to/your/build).
-buildLinux64Player <pathname>	Build a 64-bit standalone Linux player (for example, -buildLinux64Player path/to/your/build).
-buildLinuxUniversalPlayer <pathname>	Build a combined 32-bit and 64-bit standalone Linux player (for example, -buildLinuxUniversalPlayer path/to/your/build).
-buildOSXPlayer <pathname>	Build a 32-bit standalone Mac OSX player (for example, -buildOSXPlayer path/to/your/build.app).
-buildOSX64Player <pathname>	Build a 64-bit standalone Mac OSX player (for example, -buildOSX64Player path/to/your/build.app).
-buildOSXUniversalPlayer <pathname>	Build a combined 32-bit and 64-bit standalone Mac OSX player (for example, -buildOSXUniversalPlayer path/to/your/build.app).
-buildTarget <name>	Allows the selection of an active build target before a project is loaded. Possible options are: win32, win64, osx, linux, linux64, ios, android, web, webstreamed, webgl, xboxone, ps4, psp2, wsaplayer, tizen, samsungtv.
-buildWindowsPlayer <pathname>	Build a 32-bit standalone Windows player (for example, -buildWindowsPlayer path/to/your/build.exe).
-buildWindows64Player <pathname>	Build a 64-bit standalone Windows player (for example, -buildWindows64Player path/to/your/build.exe).
-cleanedLogFile	Detailed debugging feature. StackTraceLogging allows features to be controlled to allow detailed logging. All settings allow None, Script Only and Full to be selected.
-createProject <pathname>	Create an empty project at the given path.
-editorTestsCategories	Filter editor tests by categories. Separate test categories with a comma.
-editorTestsFilter	Filter editor tests by names. Separate test names with a comma.
-editorTestsResultFile	Path where the result file should be placed. If the path is a folder, a default file name is used. If not specified, the results are placed in the project’s root folder.
-executeMethod <ClassName.MethodName>	Execute the static method as soon as Unity is started, the project is open and after the optional Asset server update has been performed. This can be used to do tasks such as continous integration, performing Unit Tests, making builds or preparing data. To return an error from the command line process, either throw an exception which causes Unity to exit with return code 1, or call EditorApplication.Exit with a non-zero return code. To pass parameters, add them to the command line and retrieve them inside the function using System.Environment.GetCommandLineArgs. To use -executeMethod, you need to place the enclosing script in an Editor folder. The method to be executed must be defined as static.
-exportPackage <exportAssetPath1 exportAssetPath2 ExportAssetPath3 exportFileName>	Export a package, given a path (or set of given paths). In this example exportAssetPath is a folder (relative to to the Unity project root) to export from the Unity project, and exportFileName is the package name. Currently, this option only exports whole folders at a time. This command normally needs to be used with the -projectPath argument.
-force-d3d9 (Windows only)	Make the Editor use Direct3D 9 for rendering. Normally the graphics API depends on player settings (typically defaults to D3D11).
-force-d3d11 (Windows only)	Make the Editor use Direct3D 11 for rendering. Normally the graphics API depends on player settings (typically defaults to D3D11).
-force-glcore (Windows only)	Make the Editor use OpenGL 3/4 core profile for rendering. The Editor tries to use the best OpenGL version available and all OpenGL extensions exposed by the OpenGL drivers. If the platform isn’t supported, Direct3D is used.
-force-glcoreXY (Windows only)	Similar to -force-glcore, but requests a specific OpenGL context version. Accepted values for XY: 32, 33, 40, 41, 42, 43, 44 or 45.
-force-gles (Windows only)	Make the Editor use OpenGL for Embedded Systems for rendering. The Editor tries to use the best OpenGL ES version available, and all OpenGL ES extensions exposed by the OpenGL drivers.
-force-glesXY (Windows only)	Similar to -force-gles, but requests a specific OpenGL ES context version. Accepted values for XY: 30, 31 or 32.
-force-clamped (Windows only)	Used with -force-glcoreXY to prevent checking for additional OpenGL extensions, allowing it to run between platforms with the same code paths.
-force-free	Make the Editor run as if there is a free Unity license on the machine, even if a Unity Pro license is installed.
-importPackage <pathname>	Import the given package. No import dialog is shown.
-logFile <pathname>	Specify where the Editor or Windows/Linux/OSX standalone log file are written.
-nographics	When running in batch mode, do not initialize the graphics device at all. This makes it possible to run your automated workflows on machines that don’t even have a GPU (automated workflows only work when you have a window in focus, otherwise you can’t send simulated input commands). Please note that -nographics does not allow you to bake GI on OSX, since Enlighten requires GPU acceleration.
-password <password>	The password of the user, required when launching.
-projectPath <pathname>	Open the project at the given path.
-quit	Quit the Unity Editor after other commands have finished executing. Note that this can cause error messages to be hidden (however, they still appear in the Editor.log file).
-returnlicense	Return the currently active license to the license server. Please allow a few seconds before the license file is removed, because Unity needs to communicate with the license server.
-runEditorTests	Run Editor tests from the project. This argument requires the projectPath, and it’s good practice to run it with batchmode argument. quit is not required, because the Editor automatically closes down after the run is finished.
-serial <serial>	Activate Unity with the specified serial key. It is good practice to pass the -batchmode and -quit arguments as well, in order to quit Unity when done, if using this for automated activation of Unity. Please allow a few seconds before the license file is created, because Unity needs to communicate with the license server. Make sure that license file folder exists, and has appropriate permissions before running Unity with this argument. If activation fails, see the Editor.log for info.
-silent-crashes	Don’t display a crash dialog.
-username <username>	Enter a username into the log-in form during activation of the Unity Editor.
Examples

C#:

using UnityEditor;
class MyEditorScript
{
     static void PerformBuild ()
     {
         string[] scenes = { "Assets/MyScene.unity" };
         BuildPipeline.BuildPlayer(scenes, ...);
     }
}
JavaScript:

static void PerformBuild ()
{
    string[] scenes = { "Assets/MyScene.unity" };
    BuildPipeline.BuildPlayer(scenes, ...);
}
The following command executes Unity in batch mode, executes the MyEditorScript.MyMethod method, and then quits upon completion.

Windows:

C:\program files\Unity\Editor\Unity.exe -quit -batchmode -executeMethod MyEditorScript.MyMethod
Mac OS:

/Applications/Unity/Unity.app/Contents/MacOS/Unity -quit -batchmode -executeMethod MyEditorScript.MyMethod
The following command executes Unity in batch mode, and updates from the Asset server using the supplied project path. The method is executed after all Assets have been downloaded and imported from the Asset server. After the function has finished execution, Unity automatically quits.

/Applications/Unity/Unity.app/Contents/MacOS/Unity -batchmode -projectPath ~/UnityProjects/AutobuildProject -assetServerUpdate 192.168.1.1 MyGame AutobuildUser l33tpa33 -executeMethod MyEditorScript.PerformBuild -quit
Unity Editor special command line arguments

These should only be used under special circumstances, or when directed by Unity Support.

Command	Details
-enableIncompatibleAssetDowngrade	Use this when you have Assets made by a newer, incompatible version of Unity, that you want to downgrade to work with your current version of Unity. When enabled, Unity presents you with a dialog asking for confirmation of this downgrade if you attempt to open a project that would require it. 
Note: This procedure is unsupported and highly risky, and should only be used as a last resort.
Unity Standalone Player command line arguments

Standalone players built with Unity also understand some command line arguments:

Command	Details
-adapter N (Windows only)	Allows the game to run full-screen on another display. The N maps to a Direct3D display adaptor. In most cases there is a one-to-one relationship between adapters and video cards. On cards that support multi-head (that is, they can drive multiple monitors from a single card) each “head” may be its own adapter.
-batchmode	Run the game in “headless” mode. The game does not display anything or accept user input. This is mostly useful for running servers for networked games.
-force-d3d9 (Windows only)	Force the game to use Direct3D 9 for rendering. Normally, the graphics API depends on player settings, and typically defaults to D3D11.
-force-d3d9-ref (Windows only)	Force the game to run using Direct3D’s “Reference” software renderer. The DirectX SDK has to be installed for this to work. This is mostly useful for building automated test suites, where you want to ensure rendering is exactly the same no matter what graphics card is being used.
-force-d3d11 (Windows only)	Force the game to use Direct3D 11 for rendering.
-force-d3d11-no-singlethreaded	Force DirectX 11.0 to be created without a D3D11_CREATE_DEVICE_SINGLETHREADED flag.
-force-opengl (Windows only)	Force the game to use OpenGL for rendering, even if Direct3D is available. Usually, OpenGL is only used if Direct3D 9.0c is not available.
-force-glcore (Windows only)	Force the Editor to use OpenGL core profile for rendering. The Editor tries to use on the best OpenGL version available and all OpenGL extensions exposed by the OpenGL drivers. If the platform isn’t supported, Direct3D is used.
-force-glcoreXY (Windows only)	Similar to -force-glcore, but requests a specific OpenGL context version. Accepted values for XY: 32, 33, 40, 41, 42, 43, 44 or 45.
-force-clamped (Windows only)	Used together with -force-glcoreXY, this prevents checking for additional OpenGL extensions, allowing it to run between platforms with the same code paths.
-nographics	When running in batch mode, do not initialize graphics device at all. This makes it possible to run your automated workflows on machines that don’t even have a GPU.
-nolog (Linux & Windows only)	Do not produce an output log. Normally output_log.txt is written in the *_Data folder next to the game executable, where Debug.Log output is printed.
-popupwindow	Create the window as a a pop-up window, without a frame.
-screen-fullscreen	Override the default full-screen state. This must be 0 or 1.
-screen-height	Override the default screen height. This must be an integer from a supported resolution.
-screen-width	Override the default screen width. This must be an integer from a supported resolution.
-screen-quality	Override the default screen quality. Example usage would be: /path/to/myGame -screen-quality Beautiful
-show-screen-selector	Forces the screen selector dialog to be shown.
-single-instance (Linux & Windows only)	Allow only one instance of the game to run at the time. If another instance is already running then launching it again with -single-instance focuses the existing one.
-parentHWND <HWND>|delayed (Windows only)	This embeds the Windows Standalone application into another application. When using this, you need to pass the parent application’s window handle to the Windows Standalone application. 
When passing -parentHWND delayed, the Unity application is hidden while running. You must also call SetParent for Unity in the application, which embeds the Unity window.
For more information, see this example: EmbeddedWindow.zip
Windows Store command line arguments

Windows Store Apps don’t accept command line arguments by default, so to pass them you need to call a special function from App.xaml.cs/cpp or App.cs/cpp. For example:

appCallbacks.AddCommandLineArg("-nolog");
You should call this before the appCallbacks.Initialize*() function.

Command	Details
-nolog	Don’t produce UnityPlayer.log.
-force-driver-type-warp	Force the DirectX 11.0 driver type WARP device (see Microsoft’s documentation on Windows Advanced Rasterization Platform for more information).
-force-d3d11-no-singlethreaded	Force DirectX 11.0 to be created without a D3D11_CREATE_DEVICE_SINGLETHREADED flag.
-force-gfx-direct	Force single threaded rendering.
-force-feature-level–9–1	Force DirectX 11.0 feature level 9.1.
-force-feature-level–9–2	Force DirectX 11.0 feature level 9.2.
-force-feature-level–9–3	Force DirectX 11.0 feature level 9.3.
-force-feature-level–10–0	Force DirectX 11.0 feature level 10.0.
-force-feature-level–10–1	Force DirectX 11.0 feature level 10.1.
-force-feature-level–11–0	Force DirectX 11.0 feature level 11.0.

Lighting Window
The Lighting Window (menu: Window > Lighting) is the main control point for Unity’s Global Illumination (GI) features. Although GI in Unity gives good results with default settings, the window’s properties allow you to adjust many aspects of the GI process to customise your scene or optimise for quality, speed and storage space as you need. This window also contains lighting-related settings that were available under the Render Settings in Unity versions prior to 5.0. These include ambient light, halos, cookies and fog.

Overview

The controls of the Lighting window are divided among three tabs:

The Object tab lets you choose subsets of the objects in your scene and change settings for them. This is very useful for selecting which objects should participate in the GI computations and also for applying settings consistently to groups of objects.
The Scene tab has settings that apply to the scene overall rather than individual objects. These settings control lighting effects and also optimisation options.
The Lightmaps tab shows the lightmap asset files generated by the GI process.
The tabs are described in full detail below.

Regardless of which tab is selected, the window also has an Auto checkbox near the bottom. If enabled, this allows the lightmap data to be updated while you edit the scene (although you should note that the update usually take a few seconds rather than happening instantaneously). If Auto is disabled then the Build button to the right of the checkbox becomes active; use the button to trigger lightmap updates as you need them. The Build button also has a dropdown menu with an option to clear the baked data from the scene (without clearing the GI Cache).

Below the Auto checkbox is a small panel showing lightmap statistics.

Object Tab

The Object tab lets you select groups of objects and apply settings to them.

At the top of the tab is a set of Scene Filter buttons that let you limit the hierarchy view to specific types of objects: Lights, Renderers, Terrains. Selecting All will show the hierarchy view as normal.


If you select any of the other buttons then the hierarchy view will be limited to showing just those object types. This is essentially just a quick way to access the standard hierarchy view filter for the most common cases.


Note that the filter does not affect which object is currently selected, so it is possible to have, say, a terrain object selected even when the hierarchy is filtered to show only lights.

The relevance of the filter buttons is that each of the three object types has its own set of properties, each described in detail below.

Lights

For a light object, the Object tab essentially shows the same information as the light component’s standard inspector panel. The properties that specifically affect GI are Baking and Bounce Intensity.

Baking allows you to choose if the light should be baked if Baked GI is selected. Mixed will also bake it, but it will still be present at runtime to give direct lighting to non-static objects. Realtime works both for Precomputed Realtime GI and when not using global illumination.

Bounce Intensity allows you to vary the intensity of indirect light (ie, light that is bounced from one object to another. The value is a multiple of the default brightness calculated by the GI system; if you set Bounce Intensity to a value greater than one then bounced light will be made brighter, while a value less than one will make it dimmer. This is useful, for example, when a dark surface in shadow (such as the interior of a cave) needs to be rendered brighter in order to make detail visible. Or alternatively, if you want to use Precomputed Realtime GI in general, but want to limit a single light to give direct light only, you can set its Bounce Intensity to 0.

Renderers

For Renderers, the following properties are available:


Property:	Function:
Lightmap Static	This indicates to Unity that the object’s location is fixed and so it should participate in the GI. If an object is not marked as Lightmap Static then it can still be lit using Light Probes.
Scale in Lightmap	This value affects the number of pixels in the lightmap texture that are used for this object. With the default value of 1.0, the number of lightmap pixels used for the object is only dependent on its surface area (ie, same number of pixels per unit area for all objects). A value greater than 1.0 increases the number of pixels (ie, the lightmap resolution) used for this object while a value less than 1.0 decreases it. You can use this property to optimise lightmaps so that important and detailed areas are more accurately lit. For example, an isolated building with flat, dark walls might look fine with a low lightmap scale (less than 1.0) while a collection of colourful motorcycles displayed close together might warrant a high scale value.
Preserve UVs	Unity can recalculate the UV coordinates used for the realtime lightmap texture so as to improve its storage and performance characteristics. Please note that the recalculation process will sometimes make misjudgements about discontinuities in the original UV mapping. For example, an intentionally sharp edge may be misinterpreted as a continuous surface, resulting in artifacts where the seam should be. If Preserve UVs is enabled then the lightmapping UVs from the object will be translated to the lightmap to retain the effect intended by the artist. If Preserve UVs is switched off then Unity will calculate the realtime lightmap UVs based on the baked UVs so as to join adjacent “charts” and compact the lightmap as much as possible. This calculation is based on the two settings below (max distance and max angle). The realtime charts are packed with a half pixel border around them. This ensures that we get no leaking when rendering from them.
Auto UV Max Distance	Enlighten automatically generates simplified UVs by merging UV charts. Charts will only be simplified if the worldspace distance between the charts is smaller than this value.
Auto UV Max Angle	Enlighten automatically generates simplified UVs by merging UV charts. Charts will only be merged if the angle between the charts is smaller than this value.
Important GI	This tells Unity that light reflected or emitted from the object is likely to affect other objects in a noticeable way. This ensures that subtle illumination effects created by this object are not optimised away.
Advanced Parameters	Allows you to choose or create a set of Lightmap Parameters for the current object selection.
Terrains

For Terrains, the properties are a subset of those available for renderers.


Property:	Function:
Lightmap Static	This indicates to Unity that the object’s location is fixed and so it should participate in the GI. If an object is not marked as Lightmap Static then it can still be lit using Light Probes.
Scale in Lightmap	This value affects the number of pixels in the lightmap texture that are used for this object. With the default value of 1.0, the number of lightmap pixels used for the object is only dependent on its surface area (ie, same number of pixels per unit area for all objects). A value greater than 1.0 increases the number of pixels (ie, the lightmap resolution) used for this object while a value less than 1.0 decreases it. You can use this property to optimise lightmaps so that important and detailed areas are more accurately lit.
Advanced Parameters	Allows you to choose or create a set of Lightmap Parameters for the current object selection. The default parameter set for Terrains is the Very Low Resolution built-in set.
Scene Tab

The Scene tab contains settings that apply to the scene overall as opposed to individual objects. Note that Precomputed Realtime GI and Baked GI features can be enabled or disabled by clicking the checkboxes next to their names in the Scene tab.


Property:	Function:
Environment Lighting	
Skybox	A skybox is an image that appears behind everything else in the scene so as to simulate the sky or other distant background. This property lets you choose the skybox asset you want to use for the scene.
Sun	When a procedural skybox is used, you can use this to specify a directional light object to indicate the direction of the “sun” (or whatever large, distant light source is illuminating your scene). If this is set to None then the brightest directional light in the scene will be assumed to represent the sun.
Ambient Source	Ambient light is light that is present all around the scene and doesn’t come from any specific source object. There are three options for the source of the ambient light. Color simply uses a flat color for all ambient light in the scene. Gradient lets you choose the color separately for ambient light from the sky, horizon and ground and blends smoothly between them. Skybox uses the colors of the skybox (if specified by the property described above) to determine the ambient light coming from different angles; this allows for more precise effects than the simpler Gradient option.
Ambient Intensity	The brightness of the ambient light in the scene.
Ambient GI	Specifies the GI mode (Precomputed Realtime or Baked) that should be used for handling the ambient light. This property has no effect unless both modes are enabled for the scene.
Reflection Source	Allows you to specify whether to use the skybox for reflection effects (the default) or alternatively choose a cubemap to use instead. If the skybox is selected as the source then an additional option is provided to set the resolution of the skybox for reflection purposes.
Reflection Intensity	The degree to which the reflection source (skybox or cubemap) will be visible in reflective objects.
Reflection Bounces	A reflection “bounce” occurs where a reflection from one object is then reflected by another object. The reflections are captured in the scene through the use of Reflection Probes. This property lets you set how many times the bounces back and forth between objects are evaluated by the probes; if set to 1 then only the initial reflection (from the skybox or cubemap specified in the Reflection Source property) will be taken into account.
Precomputed Realtime GI	
Realtime Resolution	This sets the number of texels (ie, “texture pixels”) that will be used per unit of length for objects being lit by realtime GI. A resolution of 1 per unit is usually a good value (depending on the size of the objects in the scene) but for terrains and huge objects you will usually want to scale the resolution down. You can use the Lightmap Parameters or the Mesh Renderer’s Scale In Lightmap property to reduce the resolution. Note that this property also sets the Indirect Resolution if both Realtime and Baked GI are enabled - see the Baked GI properties below.
CPU Usage	This lets you set the approximate amount of CPU time that should be spent evaluating realtime GI at runtime. Higher CPU usage results in faster reactions from the lighting but may affect framerate, etc. This does not affect the CPU usage for the precomputation process performed in the editor. Note that higher CPU usage is achieved by increasing the number of threads assigned to the GI; processors with many cores may therefore suffer less of a performance hit.
Baked GI	
Baked Resolution	This sets the number of texels (ie, “texture pixels”) that will be used per unit of length for objects being lit by baked GI. This is typically set about ten times higher than the Realtime Resolution (see Precomputed Realtime GI above).
Baked Padding	The separation (in texel units) between separate shapes in the baked lightmap.
Compressed	Should the baked lightmap texture be compressed? A compressed lightmap requires less storage space but the compression process can introduce unwanted artifacts into the texture.
Indirect Resolution	(Only available when Precomputed Realtime GI is disabled) Resolution of the indirect lighting calculations. Equivalent to Realtime Resolution when using Precomputed Realtime GI.
Ambient Occlusion	The relative brightness of surfaces in ambient occlusion (ie, partial blockage of ambient light in interior corners). Higher values indicate greater contrast between the occluded and fully lit areas. This is only applied to the indirect lighting calculated by the GI system.
Final Gather	When the final gather option is enabled, the final light bounce in the GI calculation will be calculated at the same resolution as the baked lightmap. This improves the visual quality of the lightmap but at the cost of additional baking time in the editor.
General GI	
Directional Mode	The lightmap can be set up to store information about the dominant incoming light at each point on the objects’ surfaces. In Directional mode, a second lightmap is generated to store the dominant direction of incoming light. This allows diffuse normal mapped materials to work with the GI. In Directional Specular mode, further data is stored to allow full shading incorporating specular reflection and normal maps. Non-directional mode switches both these options off. Directional mode requires about twice as much storage space for the additional lightmap data; Directional Specular requires four times as much storage and also about twice as much texture memory. See the page on Directional Lightmapping for further details.
Indirect Intensity	A value that scales the brightness of indirect light as seen in the final lightmap (ie, ambient light or light bounced and emitted from objects). Setting this to 1.0 uses the default scaling; values less than 1.0 reduce the intensity while values greater than 1.0 increase it.
Bounce Boost	A scaling value to increase the amount of light bounced from surfaces onto other surfaces. The default value is 1.0 which indicates no increase.
Default Parameters	Unity uses a set of general parameters for the lightmapping in addition to properties of the Lighting window. A few defaults are available from the menu for this property but you can also create your own lightmap parameter file using the Create New option. See the Lightmap Parameters page for further details.
Atlas Size	The size in pixels of the full lightmap texture which incorporates separate regions for the individual object textures.
Fog	
Fog Color	The color used to draw fog in the scene. Note that fog is not available with the Deferred rendering path.
Fog Mode	The way in which the fogging accumulates with distance from the camera. The options are Linear, Exponential and Exponential Squared (these are in increasing order of fog accumulation with distance).
Start	(Only available for Linear fog mode) The distance from camera at which the fog starts.
End	(Only available for Linear fog mode) The distance from camera at which the fog completely obscures scene objects.
Other Settings	
Halo Texture	The texture used for drawing a Halos around lights.
Halo Strength	The visibility of halos around lights.
Flare Fade Speed	The time over which lens flares will fade from view after initially appearing.
Flare Strength	The visibility of lens flares from lights.
Spot Cookie	The Cookie texture used for spot lights.
The Build button will start the lighting build process. The button can expand into a dropdown where you can clear the baked data. This isn’t the same as clearing the GICache.

Lightmaps Tab

The final tab provides an easy way to set and locate the lightmap asset file used for the scene.


If you click the filename in the Lightmap Snapshot box, the Project view will show you the asset file. If you click the small pip next to the box, an object selection window will appear to let you select a different lightmap. If you rename the folder where the current lightmap asset is located and then set the Lightmap Snapshot property to None, a new file will be created the next time you build the lightmap. Using multiple files like this is a good way to test out GI settings and compare different sets of parameters.

The image below the Lightmap Snapshot box shows a preview of the lightmap. This is only available when Baked lights are used; the preview will be blank for Realtime lights.

Native Audio Plugin SDK
This document describes the built-in native audio plugin interface of Unity 5.0. We will do this by looking at some specific example plugins that grow in complexity as we move along. This way, we start out with very basic concepts and introduce more complex use-cases near the end of the document.

Download

First thing you need to do is to download the newest audio plugin SDK from here.

Overview

The native audio plugin system consists of two parts:

The native DSP (Digital Signal Processing) plugin which has to be implemented as a .dll (Windows) or .dylib (OSX) in C or C++. Unlike scripts and because of the high demands on performance this has to be compiled for any platform that you want to support, possibly with platform-specific optimizations.
The GUI which is developed in C#. Note that the GUI is optional, so you always start out plugin development by creating the basic native DSP plugin, and let Unity show a default slider-based UI for the parameter descriptions that the native plugin exposes. We recommend this approach to bootstrap any project.
Note that you can initially prototype the C# GUI as a .cs file that you just drop into the Assets/Editor folder (just like any other editor script). Later on you can move this into a proper MonoDevelop project as your code starts to grow and need better modularization and better IDE support. This enables you to compile it into a .dll, making it easier for the user to drop into the project and also in order to protect your code.

Also note that both the native DSP and GUI DLLs can contain multiple plugins and that the binding happens only through the names of the effects in the plugins regardless of what the DLL file is called.

What are all these files?

The native side of the plugin SDK actually only consists of one file (AudioPluginInterface.h), but to make it easy to have multiple plugin effects within the same DLL we have added supporting code to handle the effect definition and parameter registration in a simple unified way (AudioPluginUtil.h and AudioPluginUtil.cpp). Note that the NativePluginDemo project contains a number of example plugins to get you started and show a variety of different plugin types that are useful in a game context. We place this code in the public domain, so feel free to use this code as a starting point for your own creations.

Development of a plugin starts with defining which parameters your plugin should have. You don’t need to have a detailed master plan of all the parameters that the plugin will have laid out before you start, but it helps to roughly have an idea of how you want the user experience to be and what components you will need.

The example plugins that we provide have a bunch of utility functions that make it easy Let’s take a look at the “Ring Modulator” example plugin. This simple plugin multiplies the incoming signal by a sine wave, which gives a nice radio-noise / broken reception like effect, especially if multiple ring modulation effects with different frequencies are chained.

The basic scheme for dealing with parameters in the example plugins is to define them as enum-values that we use as indices into an array of floats for both convenience and brevity.

enum Param
{
    P_FREQ,
    P_MIX,
    P_NUM
};

int InternalRegisterEffectDefinition(UnityAudioEffectDefinition& definition)
{
    int numparams = P_NUM;
    definition.paramdefs = new UnityAudioParameterDefinition [numparams];
    RegisterParameter(definition, "Frequency", "Hz",
        0.0f, kMaxSampleRate, 1000.0f,
        1.0f, 3.0f,
        P_FREQ);
    RegisterParameter(definition, "Mix amount", "%",
        0.0f, 1.0f, 0.5f,
        100.0f, 1.0f,
        P_MIX);
    return numparams;
}
The numbers in the RegisterParameter calls are the minimum, maximum and default values followed by a scaling factor used for display only, i.e. in the case of a percentage-value the actual value goes from 0 to 1 and is scaled by 100 when displayed. There is no custom GUI code for this, but as mentioned earlier, Unity will generate a default GUI from these basic parameter definitions. Note that no checks are performed for undefined parameters, so the AudioPluginUtil system expects that all declared enum values (except P_NUM) are matched up with a corresponding parameter definition.

Behind the scenes the RegisterParameter function fills out an entry in the UnityAudioParameterDefinition array of the UnityAudioEffectDefinition structure that is associated with that plugin (see “AudioEffectPluginInterface.h”). The rest that needs to be set up in UnityAudioEffectDefinition is the callbacks to the functions that handle instantiating the plugin (CreateCallback), setting/getting parameters (SetFloatParameterCallback/UnityAudioEffect_GetFloatParameterCallback), doing the actual processing (UnityAudioEffect_ProcessCallback) and eventually destroying the plugin instance when done (UnityAudioEffect_ReleaseCallback).

To make it easy to have multiple plugins in the same DLL, each plugin is residing in its own namespace, and a specific naming convention for the callback functions is used such that the DEFINE_EFFECT and DECLARE_EFFECT macros can fill out the UnityAudioEffectDefinition structure. Underneath the hood all the effects definitions are stored in an array to which a pointer is returned by the only entry point of the library UnityGetAudioEffectDefinitions.

This is useful to know in case you want to develop bridge plugins that map from other plugin formats such as VST or AudioUnits to or from the Unity audio plugin interface, in which case you need to develop a more dynamic way to set up the parameter descriptions at load time.

Instantiating the plugin

The next thing is the data for the instance of the plugin. In the example plugins, we put all this into the EffectData structure. The allocation of this must happen in the corresponding CreateCallback which is called for each instance of the plugin in the mixer. In this simple example there’s only one sine-wave that is multiplied to all channels, other more advanced plugins need allocate additional data per input channel.

struct EffectData
{
    struct Data
    {
        float p[P_NUM]; // Parameters
        float s;        // Sine output of oscillator
        float c;        // Cosine output of oscillator
    };
    union
    {
        Data data;
        unsigned char pad[(sizeof(Data) + 15) & ~15];
    };
};
UNITY_AUDIODSP_RESULT UNITY_AUDIODSP_CALLBACK CreateCallback(
    UnityAudioEffectState* state)
{
    EffectData* effectdata = new EffectData;
    memset(effectdata, 0, sizeof(EffectData));
    effectdata->data.c = 1.0f;
    state->effectdata = effectdata;
    InitParametersFromDefinitions(
        InternalRegisterEffectDefinition, effectdata->data.p);
    return UNITY_AUDIODSP_OK;
}
The UnityAudioEffectState contains various data from the host such as the sampling rate, the total number of samples processed (for timing), or whether the plugin is bypassed, and is passed to all callback functions.

And obviously to free the plugin instance there is a corresponding function too:

UNITY_AUDIODSP_RESULT UNITY_AUDIODSP_CALLBACK ReleaseCallback(
    UnityAudioEffectState* state)
{
    EffectData::Data* data = &state->GetEffectData<EffectData>()->data;
    delete data;
    return UNITY_AUDIODSP_OK;
}
The main processing of audio happens in the ProcessCallback:

UNITY_AUDIODSP_RESULT UNITY_AUDIODSP_CALLBACK ProcessCallback(
    UnityAudioEffectState* state,
    float* inbuffer, float* outbuffer,
    unsigned int length,
    int inchannels, int outchannels)
{
    EffectData::Data* data = &state->GetEffectData<EffectData>()->data;
 
    float w = 2.0f * sinf(kPI * data->p[P_FREQ] / state->samplerate);
    for(unsigned int n = 0; n < length; n++)
    {
        for(int i = 0; i < outchannels; i++)
        {
            outbuffer[n * outchannels + i] =
                inbuffer[n * outchannels + i] *
                (1.0f - data->p[P_MIX] + data->p[P_MIX] * data->s);
        }
        data->s += data->c * w; // cheap way to calculate a sine-wave
        data->c -= data->s * w;
    }
 
    return UNITY_AUDIODSP_OK;
}
The GetEffectData function at the top is just a helper function casting the effectdata field of the state variable to the EffectData::Data in the structure we declared above.

Other simple plugins included are the NoiseBox plugin, which adds and multiplies the input signal by white noise at variable frequencies, or the Lofinator plugin, which does simple downsampling and quantization of the signal. All of these may be used in combination and with game-driven animated parameters to simulate anything from mobile phones to bad radio reception on walkies, broken loudspeakers etc.

The StereoWidener, which decomposes a stereo input signal into mono and side components with variable delay and then recombines these to increase the perceived stereo effect.

A bunch of simple plugins without custom GUIs to get started with.
A bunch of simple plugins without custom GUIs to get started with.
Which plugin to load on which platform?

Native audio plugins use the same scheme as other native or managed plugins in that they must be associated with their respective platforms via the plugin importer inspector. You can read more about the subfolders in which to place plugins here. The platform association is necessary so that the system knows which plugins to include on a each build target in the standalone builds, and with the introduction of 64-bit support this even has to be specified within a platform. OSX plugins are special in this regard since the Universal Binary format allows them to contain both 32 and 64 bit variants in the same bundle.

Native plugins in Unity that are called from managed code get loaded via the [DllImport] attribute referencing the function to be imported from the native DLL. However, in the case of native audio plugins things are different. The special problem that arises here is that the audio plugins need to be loaded before Unity starts creating any mixer assets that may need effects from the plugins. In the editor this is no problem, because we can just reload and rebuild the mixers that depend on plugins, but in standalone builds the plugins must be loaded before we create the mixer assets. To solve this, the current convention is to prefix the DLL of the plugin “audioplugin” (case insensitive) so that the system can detect this and add it to a list of plugins that will automatically be loaded at start. Remember that it’s only the definitions inside the plugin that define the names of the effects that show up inside Unity’s mixer, so the DLL can be called anything, but it needs to start with the string “audioplugin” to be detected as such.

For platforms such as IOS the plugin code needs to be statically linked into the Unity binary produced by the generated XCode project and there - just like plugin rendering devices - the plugin registration has to be added explicitly to the startup code of the app.


On OSX one bundle can contain both the 32- and 64 bit version of the plugin. You can also split them to save size.

Plugins with custom GUIs

Now let’s look at something a little more advanced: Effects for equalization and multiband compression. Such plugins have a much higher number of parameters than the simple plugins presented in the previous section and also there is some physical coupling between parameters that require a better way to visualize the parameters than just a bunch of simple sliders. Consider an equalizer for instance: Each band has 3 different filters that collectively contribute to the final equalization curve and each of these filters has the 3 parameters frequency, Q-factor and gain which are physically linked and define the shape of each filter. So it helps the user a lot, if an equalizer plugin has a nice big display showing the resulting curve, the individual filter contributions and can be operated in such a way that multiple parameters can be set simultaneously by simple dragging operations on the control instead of changing sliders one at a time.


Custom GUI of the Equalizer plugin. Drag the three bands to change the gains and frequencies of the filter curve. Hold shift down while dragging to change the shape of each band.

So once again, the definition, initialization, deinitialization and parameter handling follows the exact same enum-based method that the simple plugins use, and even the ProcessCallback code is rather short. Well, time to stop looking at the native code and open the AudioPluginDemoGUI.sln project in MonoDevelop. Here you will find the associated C# classes for the GUI code. The way it works is simple: Once Unity has loaded the native plugin DLLs and registered the contained audio plugins, it will start looking for corresponding GUIs that match the names of the registered plugins. This happens through the Name property of the EqualizerCustomGUI class which, like all custom plugin GUIs, must inherit from IAudioEffectPluginGUI. There’s only one important function inside this class which is the bool OnGUI(IAudioEffectPlugin plugin) function. Via the IAudioEffectPlugin plugin argument this function gets a handle to the native plugin that it can use to read and write the parameters that the native plugin has defined. So to read a parameter it calls:

plugin.GetFloatParameter("MasterGain", out masterGain);
which returns true if the parameter was found, and to set it, it calls:

plugin.SetFloatParameter("MasterGain", masterGain);
which also returns true if the parameter exists. And that’s basically the most important binding between GUI and native code. You can also use the function

plugin.GetFloatParameterInfo("NAME", out minVal, out maxVal, out defVal);
to query parameter “NAME” for it’s minimum, maximum and default values to avoid duplicate definitions of these in the native and UI code. Note that if your OnGUI function return true, the Inspector will show the default UI sliders below the custom GUI. This is again useful to bootstrap your GUI development as you have all the parameters available while developing your custom GUI and have an easy way to check that the right actions performed on it result in the expected parameter changes.

We won’t discuss the details about the DSP processing that is going on in the Equalizer and Multiband plugins here, for those interested, the filters are taken from Robert Bristow Johnson’s excellent Audio EQ Cookbook and to plot the curves Unity provides some internal API functions to draw antialiased curves for the frequency response.

One more thing to mention though is that both the Equalizer and Multiband plugins do also provide code to overlay the input and output spectra to visualize the effect of the plugins, which brings up an interesting point: The GUI code runs at much lower update rate (the frame rate) than the audio processing and doesn’t have access to the audio streams, so how do we read this data? For this, there is a special function for this in the native code:

UNITY_AUDIODSP_RESULT UNITY_AUDIODSP_CALLBACK GetFloatParameterCallback(
    UnityAudioEffectState* state,
    int index,
    float* value,
    char *valuestr)
{
    EffectData::Data* data = &state->GetEffectData<EffectData>()->data;
    if(index >= P_NUM)
        return UNITY_AUDIODSP_ERR_UNSUPPORTED;
    if(value != NULL)
        *value = data->p[index];
    if(valuestr != NULL)
        valuestr[0] = 0;
  return UNITY_AUDIODSP_OK;
}
It simply enables reading an array of floating-point data from the native plugin. Whatever that data is, the plugin system doesn’t care about, as long as the request doesn’t massively slow down the UI or the native code. For the Equalizer and Multiband code there is a utility class called FFTAnalyzer which makes it easy to feed in input and output data from the plugin and get a spectrum back. This spectrum data is then resampled by GetFloatBufferCallback and handed to the C# UI code. The reason that the data needs to be resampled is that the FFTAnalyzer runs the analysis at a fixed frequency resolution while GetFloatBufferCallback just returns the number of samples requested, which is determined by the width of the view that is displaying the data. For a very simple plugin that has a minimal amount of DSP code you might also take a look at the CorrelationMeter plugin, which simply plots the amplitude of the left channel against the amplitude of the right channel in order to show “how stereo” the signal is.



Left: Custom GUI of the CorrelationMeter plugin.

Right: Equalizer GUI with overlaid spectrum analysis (green curve is source, red is processed).

At this point we would also like to point out that both the Equalizer and Multiband effects are kept intentionally simple and unoptimized, but we think they serve as good examples of more complex UIs that are supported by the plugin system. There’s obviously a lot of work still in doing the relevant platform-specific optimizations, tons of parameter tweaking to make it fell really right and respond in the most musical way etc. etc… We might also implement some of these effects as built-in plugins in Unity at some point simply for the convenience of increasing Unity’s standard repertoire of plugins, but we sincerely hope that the reader will also take up the challenge to make some really awesome plugins – and who knows, they might at some point end up as built-in plugins. ;-)


Convolution reverb example plugin. The impulse response is decaying random noise, defined by the parameters. This is only for demonstration purposes, as a production plugin should allow the user to load arbitrary recorded impulses, the underlying convolution algorithm remains the same nevertheless.


Example of a loudness monitoring tool measuring levels at 3 different time scales. Also just for demonstration purposes, but a good place to start building a monitor tool that conforms to modern loudness standardizations. The curve rendering code is built into Unity.

Synchronizing to the DSP clock

Time for some fun exercises. Why not use the plugin system to generate sound instead of just processing it? Let’s try to do some simple bassline and drum synthesizers that should be familiar to people who listen to acid trance – some simple clones of some of the main synths that defined this genre. Take a look at Plugin_TeeBee.cpp and Plugin_TeeDee.cpp. These simple synths just generate patterns with random notes and have some parameters for tweaking the filters, envelopes and so forth in the synthesis engine. Again, we won’t discuss those details here, but just point out that the state->dsptick parameter is read in the ProcessCallback in order to determine the position in the “song”. This counter is a global sample position, so we just divide it by the length of each note specified in samples and fire a note event to the synthesis engine whenever this division has a zero remainder. This way, all plugin effects stay in sync to the same sample-based clock, and if you would for instance play a prerecorded piece of music with a known tempo through such an effect, you could use the timing info to apply tempo-synchronized filter effects or delays on the music.


Simple bassline and drum synthesizers to demonstrate tempo-synchronized effects.
Simple bassline and drum synthesizers to demonstrate tempo-synchronized effects.
Spatialization

The native audio plugin SDK is the foundation of the Spatialization SDK which allows developing custom spatialization effects that are instantiazed per audio source. More information about this can be found here.

Outlook

This is just the start of an effort to open up parts of the sound system to high performance native code. We have plans to integrate this in other parts of Unity as well to make the effects usable outside of the mixer as well as extending the SDK to support other parameter types than floats with support for better default GUIs as well as storage of binary data.

Have a lot of fun creating your own plugins. Hope to see them on the asset store. ;-)

“Disclaimer”

While there are many similarities in the design, Unity’s native audio SDK is not built on top of other plugin SDKs like Steinberg VST or Apple AudioUnits. It should be mentioned that it would be easy for the interested reader to implement basic wrappers for these using this SDK that allow using such plugins to be used in Unity. It is not something the dev-team of Unity is planning to do. Proper hosting of any plugin quickly gets very complex, and dealing with all the intricacies of expected invocation orders and handling custom GUI windows that are based on native code quickly grows by leaps and bounds which makes it less useful as example code.

While we do understand that it could potentially be quite useful to load your VST or AU plugin or even effects for just mocking up / testing sound design, bear in mind that using VST/AU also limits you to a few specific platforms. The potential of writing audio plugins based on the Unity SDK is that it extends to all platforms that support software-mixing and dynamically loaded native code. That said, there are valid use cases for mocking up early sound design with your favourite tools before deciding to devote time to develop custom plugins (or simply to be able to use metering plugins in the editor that don’t alter the sound in any way), so if anyone wants to make a nice solution for that, please do.

Configurable Joint
SWITCH TO SCRIPTING
Configurable Joints are extremely customisable since they incorporate all the functionality of the other joint types. You can use them to create anything from adapted versions of the existing joints to highly specialised joints of your own design.

Properties


Property:	Function:
Connected Body	The other Rigidbody object to which the joint is connected. You can set this to None to indicate that the joint is attached to a fixed position in space rather than another Rigidbody.
Anchor	The point where the center of the joint is defined. All physics-based simulation will use this point as the center in calculations
Axis	The local axis that will define the object’s natural rotation based on physics simulation
Auto Configure Connected Anchor	If this is enabled, then the Connected Anchor position will be calculated automatically to match the global position of the anchor property. This is the default behavior. If this is disabled, you can configure the position of the connected anchor manually.
Connected Anchor	Manual configuration of the connected anchor position.
Secondary Axis	Together, Axis and Secondary Axis define the local coordinate system of the joint. The third axis is set to be orthogonal to the other two.
X, Y, Z Motion	Allow movement along the X, Y or Z axes to be Free, completely Locked, or Limited according to the limit properties described below.
Angular X, Y, Z Motion	Allow rotation around the X, Y or Z axes to be Free, completely Locked, or Limited according to the limit properties described below.
Linear Limit Spring	A spring force applied to pull the object back when it goes past the limit position.
        Spring	The spring force. If this value is set to zero then the limit will be impassable; a value other than zero will make the limit elastic.
        Damper	The reduction of the spring force in proportion to the speed of the joint’s movement. Setting a value above zero allows the joint to “dampen” oscillations which would otherwise carry on indefinitely.
Linear Limit	Limit on the joint’s linear movement (ie, movement over distance rather than rotation), specified as a distance from the joint’s origin.
        Limit	The distance in world units from the origin to the limit.
        Bounciness	Bounce force applied to the object to push it back when it reaches the limit distance.
        Contact Distance	The minimum distance tolerance (between the joint position and the limit) at which the limit will be enforced. A high tolerance makes the limit less likely to be violated when the object is moving fast. However, this will also require the limit to be taken into account by the physics simulation more often and this will tend to reduce performance slightly.
Angular X Limit Spring	A spring torque applied to rotate the object back when it goes past the limit angle of the joint.
        Spring	The spring torque. If this value is set to zero then the limit will be impassable; a value other than zero will make the limit elastic.
        Damper	The reduction of the spring torque in proportion to the speed of the joint’s rotation. Setting a value above zero allows the joint to “dampen” oscillations which would otherwise carry on indefinitely.
Low Angular X Limit	Lower limit on the joint’s rotation around the X axis, specified as a angle from the joint’s original rotation.
        Limit	The limit angle.
        Bounciness	Bounce torque applied to the object when its rotation reaches the limit angle.
        Contact Distance	The minimum angular tolerance (between the joint angle and the limit) at which the limit will be enforced. A high tolerance makes the limit less likely to be violated when the object is moving fast. However, this will also require the limit to be taken into account by the physics simulation more often and this will tend to reduce performance slightly.
High Angular XLimit	This is similar to the Low Angular X Limit property described above but it determines the upper angular limit of the joint’s rotation rather than the lower limit.
Angular YZ Limit Spring	This is similar to the Angular X Limit Spring described above but applies to rotation around both the Y and Z axes.
Angular Y Limit	Analogous to the Angular X Limit properties described above but applies to the Y axis and regards both the upper and lower angular limits as being the same.
Angular Z Limit	Analogous to the Angular X Limit properties described above but applies to the Z axis and regards both the upper and lower angular limits as being the same.
Target Position	The target position that the joint’s drive force should move it to.
Target Velocity	The desired velocity with which the joint should move to the Target Position under the drive force.
XDrive	The drive force that moves the joint linearly along its local X axis.
        Mode	The mode determines whether the joint should move to reach a specified Position, a specified Velocity or both.
        Position Spring	The spring force that moves the joint towards its target position. This is only used when the drive mode is set to Position or Position and Velocity.
        Position Damper	The reduction of the spring force in proportion to the speed of the joint’s movement. Setting a value above zero allows the joint to “dampen” oscillations which would otherwise carry on indefinitely. This is only used when the drive mode is set to Position or Position and Velocity.
        Maximum Force	The force used to accelerate the joint toward its target velocity. This is only used when the drive mode is set to Velocity or Position and Velocity.
YDrive	This is analogous to the X Drive described above but applies to the joint’s Y axis.
ZDrive	This is analogous to the X Drive described above but applies to the joint’s Z axis.
Target Rotation	The orientation that the joint’s rotational drive should rotate towards, specified as a quaternion.
Target Angular Velocity	The angular velocity that the joint’s rotational drive should aim to achieve. This is specified as a vector whose length specifies the rotational speed and whose direction defines the axis of rotation.
Rotation Drive Mode	The way in which the drive force will be applied to the object to rotate it to the target orientation. If the mode is set to X and YZ, the torque will be applied around these axes as specified by the Angular X/YZ Drive properties described below. If Slerp mode is used then the Slerp Drive properties will determine the drive torque.
Angular X Drive	This specifies how the joint will be rotated by the drive torque around its local X axis. It is used only if the Rotation Drive Mode property described above is set to X & YZ.
        Mode	The mode determines whether the joint should move to reach a specified angular Position, a specified angular Velocity or both.
        Position Spring	The spring torque that rotates the joint towards its target position. This is only used when the drive mode is set to Position or Position and Velocity.
        Position Damper	The reduction of the spring torque in proportion to the speed of the joint’s movement. Setting a value above zero allows the joint to “dampen” oscillations which would otherwise carry on indefinitely. This is only used when the drive mode is set to Position or Position and Velocity.
        Maximum Force	The torque used to accelerate the joint toward its target velocity. This is only used when the drive mode is set to Velocity or Position and Velocity.
Angular YZDrive	This is analogous to the Angular X Drive described above but applies to both the joint’s Y and Z axes.
Slerp Drive	This specifies how the joint will be rotated by the drive torque around all local axes. It is used only if the Rotation Drive Mode property described above is set to Slerp.
        Mode	The mode determines whether the joint should move to reach a specified angular Position, a specified angular Velocity or both.
        Position Spring	The spring torque that rotates the joint towards its target position. This is only used when the drive mode is set to Position or Position and Velocity.
        Position Damper	The reduction of the spring torque in proportion to the speed of the joint’s movement. Setting a value above zero allows the joint to “dampen” oscillations which would otherwise carry on indefinitely. This is only used when the drive mode is set to Position or Position and Velocity.
        Maximum Force	The torque used to accelerate the joint toward its target velocity. This is only used when the drive mode is set to Velocity or Position and Velocity.
Projection Mode	This defines how the joint will be snapped back to its constraints when it unexpectedly moves beyond them (due to the physics engine being unable to reconcile the current combination of forces within the simulation). The options are None and Position and Rotation.
Projection Distance	The distance the joint must move beyond its constraints before the physics engine will attempt to snap it back to an acceptable position.
Projection Angle	The angle the joint must rotate beyond its constraints before the physics engine will attempt to snap it back to an acceptable position.
Configured in World Space	Should the values set by the various target and drive properties be calculated in world space instead of the object’s local space?
Swap Bodies	If enabled, this will make the joint behave as though the component were attached to the connected Rigidbody (ie, the other end of the joint).
Break Force	If the joint is pushed beyond its constraints by a force larger than this value then the joint will be permanently “broken” and deleted.
Break Torque	If the joint is rotated beyond its constraints by a torque larger than this value then the joint will be permanently “broken” and deleted.
Enable Collision	Should the object with the joint be able to collide with the connected object (as opposed to just passing through each other)?
Enable Preprocessing	If preprocessing is disabled then certain “impossible” configurations of the joint will be kept more stable rather than drifting wildly out of control.
Details

Like the other joints, the Configurable Joint allows you to restrict the movement of an object but also to drive it to a target velocity or position using forces. However, there are many configuration options available and they can be quite subtle when used in combination; you may need to experiment with different options to get the joint to behave exactly the way you want.

Constraining Movement

You can constrain both translational movement and rotation on each axis independently using the X, Y, Z Motion and X, Y, Z Rotation properties. If Configured In World Space is enabled then movements will be constrained to the world axes rather than the object’s local axes. Each of these properties can be set to Locked, Limited or Free:

A Locked axis will allow no movement at all. For example, an object locked in the world Y axis cannot move up or down.
A Limited axis allows free movement between predefined limits, as explained below. For example, a gun turret might be given a restricted arc of fire by limiting its Y rotation to a specific angular range.
A Free axis allows any movement.
You can limit translational movement using the Linear Limit property, which defines the maximum distance the joint can move from its point of origin. (measured along each axis separately). For example, you could constrain the puck for an air hockey table by locking the joint in the Y axis (in world space), leaving it free in the Z axis and setting the limit for the X axis to fit the width of the table; the puck would then be constrained to stay within the playing area.


You can also limit rotation using the Angular Limit properties. Unlike the linear limit, these allow you to specify different limit values for each axis. Additionally, you can also define separate upper and lower limits on the angle of rotation for the X axis (the other two axes use the same angle either side of the original rotation). For example, you could construct a “teeter table” using a flat plane with a joint constrained to allow slight tilting in the X and Z directions while leaving the Y rotation locked.

Bounciness and Springs

By default, a joint simply stops moving when it runs into its limit. However, an inelastic collision like this is rare in the real world and so it is useful to add some feeling of bounce to a constrained joint. You can use the Bounciness property of the linear and angular limits to make the constrained object bounce back after it hits its limit. Most collisions will look more natural with a small amount of bounciness but you can also set this property higher to simulate unusually bouncy boundaries like, say, the cushions of a pool table.

Bouncy joint does not cross the limit
Bouncy joint does not cross the limit
The joint limits can be softened further using the spring properties: Linear Limit Spring for translation and Angular X/YZ Limit Spring for rotation. If you set the Spring property to a value above zero, the joint will not abruptly stop moving when it hits a limit but will be drawn back to the limit position by a spring force (the strength of the force is determined by the Spring value). By default, the spring is perfectly elastic and will tend to catapult the joint back in the direction opposite to the collision. However, you can use the Damper property to reduce the elasticity and return the joint to the limit more gently. For example, you might use a spring joint to create a lever that can be pulled to the left or right but then springs back to an upright position. If the springs are perfectly elastic then the lever will tend to oscillate back and forth around the centre point after it is released. However, if you add enough damping then the spring will rapidly settle down to the neutral position.

Spring joint crosses the limit but is pulled back to it
Spring joint crosses the limit but is pulled back to it
Drive forces

Not only can a joint react to the movements of the attached object, but it can also actively apply drive forces to set the object in motion. Some joints simple need to keep the object moving at a constant speed as with, say, a rotary motor turning a fan blade. You can set your desired velocity for such joints using the Target Velocity and Target Angluar Velocity properties. You might also require joints that move their object towards a particular position in space (or a particular orientation); you can set these using the Target Position and Target Rotation properties. For example, you could implement a forklift by mounting the forks on a configurable joint and then setting the target height to raise them from a script.

With the target set, the X, Y, Z Drive and Angular X/YZ Drive (or alternatively Slerp Drive) properties then specify the force used to push the joint toward it. The Drives’ Mode property selects whether the joint should seek a target position, velocity or both. The Position Spring and Position Damper work in the same way as for the joint limits when seeking a target position. In velocity mode, the spring force is dependent on the “distance” between the current velocity and the target velocity; the damper helps the velocity to settle at the chosen value rather than oscillating endlessly around it. The Maximum Force property is a final refinement that prevents the force applied by the spring from exceeding a limit value regardless of how far the joint is from its target. This prevents the circumstance where a joint stretched far from its target rapidly snaps the object back in an uncontrolled way.

Note that with all the drive forces (except for Slerp Drive, described below), the force is applied separately in each axis. So, for example, you could implement a spacecraft that has a high forward flying speed but a relatively low speed in sideways steering motion.

Slerp Drive

While the other drive modes apply forces in separate axes, Slerp Drive uses the Quaternion’s spherical interpolation or “slerp” functionality to reorient the joint. Rather than isolating individual axes, the slerp process essentially finds the minimum total rotation that will take the object from the current orientation to the target and applies it on all axes as necessary. Slerp drive is slightly easier to configure and fine for most purposes but does not allow you to specify different drive forces for the X and Y/Z axes.

To enable Slerp drive, you should change the Rotation Drive Mode property from X and YZ to Slerp. Note that the modes are mutually exclusive; the joint will use either the Angular X/YZ Drive values or the Slerp Drive values but not both together.

Building Plugins for Android
This page describes Native Code Plugins for Android.

Building a Plugin for Android

To build a plugin for Android, you should first obtain the Android NDK and familiarize yourself with the steps involved in building a shared library.

If you are using C++ (.cpp) to implement the plugin you must ensure the functions are declared with C linkage to avoid name mangling issues.

extern "C" {
  float FooPluginFunction ();
}

Using Your Plugin from C#

Once built, the shared library should be copied to the Assets->Plugins->Android folder. Unity will then find it by name when you define a function like the following in the C# script:-

[DllImport ("PluginName")]
private static extern float FooPluginFunction ();

Please note that PluginName should not include the prefix (‘lib’) nor the extension (‘.so’) of the filename. You should wrap all native code methods with an additional C# code layer. This code should check Application.platform and call native methods only when the app is running on the actual device; dummy values can be returned from the C# code when running in the Editor. You can also use platform defines to control platform dependent code compilation.

Android Library Projects

You can drop pre-compiled Android library projects into the Assets->Plugins->Android folder. Pre-compiled means all .java files must have been compiled into jar files located in either the bin/ or the libs/ folder of the project. AndroidManifest.xml from these folders will get automatically merged with the main manifest file when the project is built.

See Android Library Projects for more details.

Deployment

For cross platform deployment, your project should include plugins for each supported platform (ie, libPlugin.so for Android, Plugin.bundle for Mac and Plugin.dll for Windows). Unity automatically picks the right plugin for the target platform and includes it with the player.

For specific Android platform (armv7, x86), the libraries (lib*.so) should be placed in the following:

Assets/Plugins/Android/libs/x86/

Assets/Plugins/Android/libs/armeabi-v7a/

Using Java Plugins

The Android plugin mechanism also allows Java to be used to enable interaction with the Android OS.

Building a Java Plugin for Android

There are several ways to create a Java plugin but the result in each case is that you end up with a .jar file containing the .class files for your plugin. One approach is to download the JDK, then compile your .java files from the command line with javac. This will create .class files which you can then package into a .jar with the jar command line tool. Another option is to use the Eclipse IDE together with the ADT.

Note: Unity expects Java plugins to be built using JDK v1.6. If you are using v1.7, you should include “-source 1.6 -target 1.6” in the command line options to the compiler.

Using Your Java Plugin from Native Code

Once you have built your Java plugin (.jar) you should copy it to the Assets->Plugins->Android folder in the Unity project. Unity will package your .class files together with the rest of the Java code and then access the code using the Java Native Interface (JNI). JNI is used both when calling native code from Java and when interacting with Java (or the JavaVM) from native code.

To find your Java code from the native side you need access to the Java VM. Fortunately, that access can be obtained easily by adding a function like this to your C/C++ code:

jint JNI_OnLoad(JavaVM* vm, void* reserved) {
  JNIEnv* jni_env = 0;
  vm->AttachCurrentThread(&jni_env, 0);
  return JNI_VERSION_1_6;
}

This is all that is needed to start using Java from C/C++. It is beyond the scope of this document to explain JNI completely. However, using it usually involves finding the class definition, resolving the constructor (<init>) method and creating a new object instance, as shown in this example:-

jobject createJavaObject(JNIEnv* jni_env) {
  jclass cls_JavaClass = jni_env->FindClass("com/your/java/Class");         // find class definition
  jmethodID mid_JavaClass = jni_env->GetMethodID (cls_JavaClass, "<init>", "()V");      // find constructor method
  jobject obj_JavaClass = jni_env->NewObject(cls_JavaClass, mid_JavaClass);     // create object instance
  return jni_env->NewGlobalRef(obj_JavaClass);                      // return object with a global reference
}

Using Your Java Plugin with helper classes

AndroidJNIHelper and AndroidJNI can be used to ease some of the pain with raw JNI.

AndroidJavaObject and AndroidJavaClass automate a lot of tasks and also use cacheing to make calls to Java faster. The combination of AndroidJavaObject and AndroidJavaClass builds on top of AndroidJNI and AndroidJNIHelper, but also has a lot of logic in its own right (to handle the automation). These classes also come in a ‘static’ version to access static members of Java classes.

You can choose whichever approach you prefer, be it raw JNI through AndroidJNI class methods, or AndroidJNIHelper together with AndroidJNI and eventually AndroidJavaObject/AndroidJavaClass for maximum automation and convenience.

UnityEngine.AndroidJNI is a wrapper for the JNI calls available in C (as described above). All methods in this class are static and have a 1:1 mapping to the Java Native Interface. UnityEngine.AndroidJNIHelper provides helper functionality used by the next level, but is exposed as public methods because they may be useful for some special cases.

Instances of UnityEngine.AndroidJavaObject and UnityEngine.AndroidJavaClass have a one-to-one mapping to an instance of java.lang.Object and java.lang.Class (or subclasses thereof) on the Java side, respectively. They essentially provide 3 types of interaction with the Java side:-

Call a method
Get the value of a field
Set the value of a field
The Call is separated into two categories: Call to a ‘void’ method, and Call to a method with non-void return type. A generic type is used to represent the return type of those methods which return a non-void type. The Get and Set always take a generic type representing the field type.

Example 1

//The comments describe what you would need to do if you were using raw JNI
 AndroidJavaObject jo = new AndroidJavaObject("java.lang.String", "some_string"); 
 // jni.FindClass("java.lang.String"); 
 // jni.GetMethodID(classID, "<init>", "(Ljava/lang/String;)V"); 
 // jni.NewStringUTF("some_string"); 
 // jni.NewObject(classID, methodID, javaString); 
 int hash = jo.Call<int>("hashCode"); 
 // jni.GetMethodID(classID, "hashCode", "()I"); 
 // jni.CallIntMethod(objectID, methodID);


Here, we’re creating an instance of java.lang.String, initialized with a string of our choice and retrieving the hash value for that string.

The AndroidJavaObject constructor takes at least one parameter, the name of class for which we want to construct an instance. Any parameters after the class name are for the constructor call on the object, in this case the string “some_string”. The subsequent Call to hashCode() returns an ‘int’ which is why we use that as the generic type parameter to the Call method.

Note: You cannot instantiate a nested Java class using dotted notation. Inner classes must use the $ separator, and it should work in both dotted and slashed format. So \[android.view.ViewGroup$LayoutParams __or__ android/view/ViewGroup$LayoutParams__ can be used, where a __LayoutParams__ class is nested in a __ViewGroup\] class.

Example 2

One of the plugin samples above shows how to get the cache directory for the current application. This is how you would do the same thing from C# without any plugins:-

 AndroidJavaClass jc = new AndroidJavaClass("com.unity3d.player.UnityPlayer"); 
 // jni.FindClass("com.unity3d.player.UnityPlayer"); 
 AndroidJavaObject jo = jc.GetStatic<AndroidJavaObject>("currentActivity"); 
 // jni.GetStaticFieldID(classID, "Ljava/lang/Object;"); 
 // jni.GetStaticObjectField(classID, fieldID); 
 // jni.FindClass("java.lang.Object"); 

 Debug.Log(jo.Call<AndroidJavaObject>("getCacheDir").Call<string>("getCanonicalPath")); 
 // jni.GetMethodID(classID, "getCacheDir", "()Ljava/io/File;"); // or any baseclass thereof! 
 // jni.CallObjectMethod(objectID, methodID); 
 // jni.FindClass("java.io.File"); 
 // jni.GetMethodID(classID, "getCanonicalPath", "()Ljava/lang/String;"); 
 // jni.CallObjectMethod(objectID, methodID); 
 // jni.GetStringUTFChars(javaString);


In this case, we start with AndroidJavaClass instead of AndroidJavaObject because we want to access a static member of com.unity3d.player.UnityPlayer rather than create a new object (an instance is created automatically by the Android UnityPlayer). Then we access the static field “currentActivity” but this time we use AndroidJavaObject as the generic parameter. This is because the actual field type (android.app.Activity) is a subclass of java.lang.Object, and any non-primitive type must be accessed as AndroidJavaObject. The exceptions to this rule are strings, which can be accessed directly even though they don’t represent a primitive type in Java.

After that it is just a matter of traversing the Activity through getCacheDir() to get the File object representing the cache directory, and then calling getCanonicalPath() to get a string representation.

Of course, nowadays you don’t need to do that to get the cache directory since Unity provides access to the application’s cache and file directory with Application.temporaryCachePath and Application.persistentDataPath.

Example 3

Finally, here is a trick for passing data from Java to script code using UnitySendMessage.

using UnityEngine; 
public class NewBehaviourScript : MonoBehaviour { 

    void Start () { 
        AndroidJNIHelper.debug = true; 
        using (AndroidJavaClass jc = new AndroidJavaClass("com.unity3d.player.UnityPlayer")) { 
            jc.CallStatic("UnitySendMessage", "Main Camera", "JavaMessage", "whoowhoo"); 
        } 
    } 

    void JavaMessage(string message) { 
        Debug.Log("message from java: " + message); 
    }
} 


The Java class com.unity3d.player.UnityPlayer now has a static method UnitySendMessage, equivalent to the iOS UnitySendMessage function on the native side. It can be used in Java to pass data to script code.

Here though, we call it directly from script code, which essentially relays the message on the Java side. This then calls back to the native/Unity code to deliver the message to the object named “Main Camera”. This object has a script attached which contains a method called “JavaMessage”.

Best practice when using Java plugins with Unity

As this section is mainly aimed at people who don’t have comprehensive JNI, Java and Android experience, we assume that the AndroidJavaObject/AndroidJavaClass approach has been used for interacting with Java code from Unity.

The first thing to note is that any operation you perform on an AndroidJavaObject or AndroidJavaClass is computationally expensive (as is the raw JNI approach). It is highly advisable to keep the number of transitions between managed and native/Java code to a minimum, for the sake of performance and also code clarity.

You could have a Java method to do all the actual work and then use AndroidJavaObject / AndroidJavaClass to communicate with that method and get the result. However, it is worth bearing in mind that the JNI helper classes try to cache as much data as possible to improve performance.

//The first time you call a Java function like 
AndroidJavaObject jo = new AndroidJavaObject("java.lang.String", "some_string"); // somewhat expensive
int hash = jo.Call<int>("hashCode"); // first time - expensive
int hash = jo.Call<int>("hashCode"); // second time - not as expensive as we already know the java method and can call it directly


The Mono garbage collector should release all created instances of AndroidJavaObject and AndroidJavaClass after use, but it is advisable to keep them in a using(){} statement to ensure they are deleted as soon as possible. Without this, you cannot be sure when they will be destroyed. If you set AndroidJNIHelper.debug to true, you will see a record of the garbage collector’s activity in the debug output.

//Getting the system language with the safe approach
void Start () { 
    using (AndroidJavaClass cls = new AndroidJavaClass("java.util.Locale")) { 
        using(AndroidJavaObject locale = cls.CallStatic<AndroidJavaObject>("getDefault")) { 
            Debug.Log("current lang = " + locale.Call<string>("getDisplayLanguage")); 

        } 
    } 
}


You can also call the .Dispose() method directly to ensure there are no Java objects lingering. The actual C# object might live a bit longer, but will be garbage collected by mono eventually.

Extending the UnityPlayerActivity Java Code

With Unity Android it is possible to extend the standard UnityPlayerActivity class (the primary Java class for the Unity Player on Android, similar to AppController.mm on Unity iOS).

An application can override any and all of the basic interaction between Android OS and Unity Android. You can enable this by creating a new Activity which derives from UnityPlayerActivity (UnityPlayerActivity.java can be found at /Applications/Unity/Unity.app/Contents/PlaybackEngines/AndroidPlayer/src/com/unity3d/player on Mac and usually at C:\Program Files\Unity\Editor\Data\PlaybackEngines\AndroidPlayer\src\com\unity3d\player on Windows).

To do this, first locate the classes.jar shipped with Unity Android. It is found in the installation folder (usually C:\Program Files\Unity\Editor\Data (on Windows) or /Applications/Unity (on Mac)) in a sub-folder called PlaybackEngines/AndroidPlayer/Variations/mono or il2cpp/Development or Release/Classes/. Then add classes.jar to the classpath used to compile the new Activity. The resulting .class file(s) should be compressed into a .jar file and placed in the Assets->Plugins->Android folder. Since the manifest dictates which activity to launch it is also necessary to create a new AndroidManifest.xml. The AndroidManifest.xml file should also be placed in the Assets->Plugins->Android folder (placing a custom manifest completely overrides the default Unity Android manifest).

The new activity could look like the following example, OverrideExample.java:

package com.company.product;

import com.unity3d.player.UnityPlayerActivity;

import android.os.Bundle;
import android.util.Log;

public class OverrideExample extends UnityPlayerActivity {

  protected void onCreate(Bundle savedInstanceState) {

    // call UnityPlayerActivity.onCreate()
    super.onCreate(savedInstanceState);

    // print debug message to logcat
    Log.d("OverrideActivity", "onCreate called!");
  }

  public void onBackPressed()
  {
    // instead of calling UnityPlayerActivity.onBackPressed() we just ignore the back button event
    // super.onBackPressed();
  }
}

And this is what the corresponding AndroidManifest.xml would look like:

<?xml version="1.0" encoding="utf-8"?>
<manifest xmlns:android="http://schemas.android.com/apk/res/android" package="com.company.product">
  <application android:icon="@drawable/app_icon" android:label="@string/app_name">
    <activity android:name=".OverrideExample"
             android:label="@string/app_name"
             android:configChanges="fontScale|keyboard|keyboardHidden|locale|mnc|mcc|navigation|orientation|screenLayout|screenSize|smallestScreenSize|uiMode|touchscreen">
        <intent-filter>
            <action android:name="android.intent.action.MAIN" />
            <category android:name="android.intent.category.LAUNCHER" />
        </intent-filter>
    </activity>
  </application>
</manifest>

UnityPlayerNativeActivity

It is also possible to create your own subclass of UnityPlayerNativeActivity. This has much the same effect as subclassing UnityPlayerActivity, but with improved input latency. Because touch/motion events are processed in native code, Java views do normally not see those events. There is, however, a forwarding mechanism in Unity which allows events to be propagated to the DalvikVM. To access this mechanism, you need to modify the manifest file as follows:

<?xml version="1.0" encoding="utf-8"?>
<manifest xmlns:android="http://schemas.android.com/apk/res/android" package="com.company.product">
  <application android:icon="@drawable/app_icon" android:label="@string/app_name">
    <activity android:name=".OverrideExampleNative"
             android:label="@string/app_name"
             android:configChanges="fontScale|keyboard|keyboardHidden|locale|mnc|mcc|navigation|orientation|screenLayout|screenSize|smallestScreenSize|uiMode|touchscreen">
  <meta-data android:name="android.app.lib_name" android:value="unity" />
  <meta-data android:name="unityplayer.ForwardNativeEventsToDalvik" android:value="true" />
        <intent-filter>
            <action android:name="android.intent.action.MAIN" />
            <category android:name="android.intent.category.LAUNCHER" />
        </intent-filter>
    </activity>
  </application>
</manifest> 


Note the “.OverrideExampleNative” attribute in the activity element and the two additional meta-data elements. The first meta-data is an instruction to use the Unity library libunity.so. The second enables events to be passed on to your custom subclass of UnityPlayerNativeActivity.

Examples

Native Plugin Sample

A simple example of the use of a native code plugin can be found here

This sample demonstrates how C code can be invoked from a Unity Android application. The package includes a scene which displays the sum of two values as calculated by the native plugin. Please note that you will need the Android NDK to compile the plugin.

Controls
IMGUI Control Types

There are a number of different IMGUI Controls that you can create. This section lists all of the available display and interactive Controls. There are other IMGUI functions that affect layout of Controls, which are described in the Layout section of the Guide.

Label

The Label is non-interactive. It is for display only. It cannot be clicked or otherwise moved. It is best for displaying information only.

/* GUI.Label example */


// JavaScript
function OnGUI () {
    GUI.Label (Rect (25, 25, 100, 30), "Label");
}


// C#
using UnityEngine;
using System.Collections;

public class GUITest : MonoBehaviour {
                    
    void OnGUI () {
        GUI.Label (new Rect (25, 25, 100, 30), "Label");
    }

}


The Label created by the example code
The Label created by the example code
Button

The Button is a typical interactive button. It will respond a single time when clicked, no matter how long the mouse remains depressed. The response occurs as soon as the mouse button is released.

Basic Usage

In UnityGUI, Buttons will return true when they are clicked. To execute some code when a Button is clicked, you wrap the the GUI.Button function in an if statement. Inside the if statement is the code that will be executed when the Button is clicked.

/* GUI.Button example */


// JavaScript
function OnGUI () {
    if (GUI.Button (Rect (25, 25, 100, 30), "Button")) {
        // This code is executed when the Button is clicked
    }
}


// C#
using UnityEngine;
using System.Collections;

public class GUITest : MonoBehaviour {
                    
    void OnGUI () {
        if (GUI.Button (new Rect (25, 25, 100, 30), "Button")) {
            // This code is executed when the Button is clicked
        }
    }

}


The Button created by the example code
The Button created by the example code
RepeatButton

RepeatButton is a variation of the regular Button. The difference is, RepeatButton will respond every frame that the mouse button remains depressed. This allows you to create click-and-hold functionality.

Basic Usage

In UnityGUI, RepeatButtons will return true for every frame that they are clicked. To execute some code while the Button is being clicked, you wrap the the GUI.RepeatButton function in an if statement. Inside the if statement is the code that will be executed while the RepeatButton remains clicked.

/* GUI.RepeatButton example */


// JavaScript
function OnGUI () {
    if (GUI.RepeatButton (Rect (25, 25, 100, 30), "RepeatButton")) {
        // This code is executed every frame that the RepeatButton remains clicked
    }
}


// C#
using UnityEngine;
using System.Collections;

public class GUITest : MonoBehaviour {
                    
    void OnGUI () {
        if (GUI.RepeatButton (new Rect (25, 25, 100, 30), "RepeatButton")) {
            // This code is executed every frame that the RepeatButton remains clicked
        }
    }

}


The Repeat Button created by the example code
The Repeat Button created by the example code
TextField

The TextField Control is an interactive, editable single-line field containing a text string.

Basic Usage

The TextField will always display a string. You must provide the string to be displayed in the TextField. When edits are made to the string, the TextField function will return the edited string.

/* GUI.TextField example */


// JavaScript
var textFieldString = "text field";

function OnGUI () {
    textFieldString = GUI.TextField (Rect (25, 25, 100, 30), textFieldString);
}


// C#
using UnityEngine;
using System.Collections;

public class GUITest : MonoBehaviour {
                    
    private string textFieldString = "text field";
    
    void OnGUI () {
        textFieldString = GUI.TextField (new Rect (25, 25, 100, 30), textFieldString);
    }

}


The TextField created by the example code
The TextField created by the example code
TextArea

The TextArea Control is an interactive, editable multi-line area containing a text string.

Basic Usage

The TextArea will always display a string. You must provide the string to be displayed in the TextArea. When edits are made to the string, the TextArea function will return the edited string.

/* GUI.TextArea example */


// JavaScript
var textAreaString = "text area";

function OnGUI () {
    textAreaString = GUI.TextArea (Rect (25, 25, 100, 30), textAreaString);
}


// C#
using UnityEngine;
using System.Collections;

public class GUITest : MonoBehaviour {
                    
    private string textAreaString = "text area";
    
    void OnGUI () {
        textAreaString = GUI.TextArea (new Rect (25, 25, 100, 30), textAreaString);
    }

}


The TextArea created by the example code
The TextArea created by the example code
Toggle

The Toggle Control creates a checkbox with a persistent on/off state. The user can change the state by clicking on it.

Basic Usage

The Toggle on/off state is represented by a true/false boolean. You must provide the boolean as a parameter to make the Toggle represent the actual state. The Toggle function will return a new boolean value if it is clicked. In order to capture this interactivity, you must assign the boolean to accept the return value of the Toggle function.

/* GUI.Toggle example */


// JavaScript
var toggleBool = true;

function OnGUI () {
    toggleBool = GUI.Toggle (Rect (25, 25, 100, 30), toggleBool, "Toggle");
}


// C#
using UnityEngine;
using System.Collections;

public class GUITest : MonoBehaviour {
                    
    private bool toggleBool = true;
    
    void OnGUI () {
        toggleBool = GUI.Toggle (new Rect (25, 25, 100, 30), toggleBool, "Toggle");
    }

}


The Toggle created by the example code
The Toggle created by the example code
Toolbar

The Toolbar Control is essentially a row of Buttons. Only one of the Buttons on the Toolbar can be active at a time, and it will remain active until a different Button is clicked. This behavior emulates the behavior of a typical Toolbar. You can define an arbitrary number of Buttons on the Toolbar.

Basic Usage

The active Button in the Toolbar is tracked through an integer. You must provide the integer as an argument in the function. To make the Toolbar interactive, you must assign the integer to the return value of the function. The number of elements in the content array that you provide will determine the number of Buttons that are shown in the Toolbar.

/* GUI.Toolbar example */


// JavaScript
var toolbarInt = 0;
var toolbarStrings : String[] = ["Toolbar1", "Toolbar2", "Toolbar3"];

function OnGUI () {
    toolbarInt = GUI.Toolbar (Rect (25, 25, 250, 30), toolbarInt, toolbarStrings);
}


// C#
using UnityEngine;
using System.Collections;

public class GUITest : MonoBehaviour {
                    
    private int toolbarInt = 0;
    private string[] toolbarStrings = {"Toolbar1", "Toolbar2", "Toolbar3"};
    
    void OnGUI () {
        toolbarInt = GUI.Toolbar (new Rect (25, 25, 250, 30), toolbarInt, toolbarStrings);
    }

}


The Toolbar created by the example code
The Toolbar created by the example code
SelectionGrid

The SelectionGrid Control is a multi-row Toolbar. You can determine the number of columns and rows in the grid. Only one Button can be active at time.

Basic Usage

The active Button in the SelectionGrid is tracked through an integer. You must provide the integer as an argument in the function. To make the SelectionGrid interactive, you must assign the integer to the return value of the function. The number of elements in the content array that you provide will determine the number of Buttons that are shown in the SelectionGrid. You also can dictate the number of columns through the function arguments.

/* GUI.SelectionGrid example */


// JavaScript
var selectionGridInt : int = 0;
var selectionStrings : String[] = ["Grid 1", "Grid 2", "Grid 3", "Grid 4"];

function OnGUI () {
    selectionGridInt = GUI.SelectionGrid (Rect (25, 25, 100, 30), selectionGridInt, selectionStrings, 2);

}


// C#
using UnityEngine;
using System.Collections;

public class GUITest : MonoBehaviour {
                    
    private int selectionGridInt = 0;
    private string[] selectionStrings = {"Grid 1", "Grid 2", "Grid 3", "Grid 4"};
    
    void OnGUI () {
        selectionGridInt = GUI.SelectionGrid (new Rect (25, 25, 300, 60), selectionGridInt, selectionStrings, 2);
    
    }

}


The SelectionGrid created by the example code
The SelectionGrid created by the example code
HorizontalSlider

The HorizontalSlider Control is a typical horizontal sliding knob that can be dragged to change a value between predetermined min and max values.

Basic Usage

The position of the Slider knob is stored as a float. To display the position of the knob, you provide that float as one of the arguments in the function. There are two additional values that determine the minimum and maximum values. If you want the slider knob to be adjustable, assign the slider value float to be the return value of the Slider function.

/* Horizontal Slider example */


// JavaScript
var hSliderValue : float = 0.0;

function OnGUI () {
    hSliderValue = GUI.HorizontalSlider (Rect (25, 25, 100, 30), hSliderValue, 0.0, 10.0);
}


// C#
using UnityEngine;
using System.Collections;

public class GUITest : MonoBehaviour {
                    
    private float hSliderValue = 0.0f;
    
    void OnGUI () {
        hSliderValue = GUI.HorizontalSlider (new Rect (25, 25, 100, 30), hSliderValue, 0.0f, 10.0f);
    }

}


The Horizontal Slider created by the example code
The Horizontal Slider created by the example code
VerticalSlider

The VerticalSlider Control is a typical vertical sliding knob that can be dragged to change a value between predetermined min and max values.

Basic Usage

The position of the Slider knob is stored as a float. To display the position of the knob, you provide that float as one of the arguments in the function. There are two additional values that determine the minimum and maximum values. If you want the slider knob to be adjustable, assign the slider value float to be the return value of the Slider function.

/* Vertical Slider example */


// JavaScript
var vSliderValue : float = 0.0;

function OnGUI () {
    vSliderValue = GUI.VerticalSlider (Rect (25, 25, 100, 30), vSliderValue, 10.0, 0.0);
}


// C#
using UnityEngine;
using System.Collections;

public class GUITest : MonoBehaviour {
                    
    private float vSliderValue = 0.0f;
    
    void OnGUI () {
        vSliderValue = GUI.VerticalSlider (new Rect (25, 25, 100, 30), vSliderValue, 10.0f, 0.0f);
    }

}


The Vertical Slider created by the example code
The Vertical Slider created by the example code
HorizontalScrollbar

The HorizontalScrollbar Control is similar to a Slider Control, but visually similar to Scrolling elements for web browsers or word processors. This control is used to navigate the ScrollView Control.

Basic Usage

Horizontal Scrollbars are implemented identically to Horizontal Sliders with one exception: There is an additional argument which controls the width of the Scrollbar knob itself.

/* Horizontal Scrollbar example */


// JavaScript
var hScrollbarValue : float;

function OnGUI () {
    hScrollbarValue = GUI.HorizontalScrollbar (Rect (25, 25, 100, 30), hScrollbarValue, 1.0, 0.0, 10.0);
}


// C#
using UnityEngine;
using System.Collections;

public class GUITest : MonoBehaviour {
                    
    private float hScrollbarValue;
    
    void OnGUI () {
        hScrollbarValue = GUI.HorizontalScrollbar (new Rect (25, 25, 100, 30), hScrollbarValue, 1.0f, 0.0f, 10.0f);
    }

}


The Horizontal Scrollbar created by the example code
The Horizontal Scrollbar created by the example code
VerticalScrollbar

The VerticalScrollbar Control is similar to a Slider Control, but visually similar to Scrolling elements for web browsers or word processors. This control is used to navigate the ScrollView Control.

Basic Usage

Vertical Scrollbars are implemented identically to Vertical Sliders with one exception: There is an additional argument which controls the height of the Scrollbar knob itself.

/* Vertical Scrollbar example */


// JavaScript
var vScrollbarValue : float;

function OnGUI () {
    vScrollbarValue = GUI. VerticalScrollbar (Rect (25, 25, 100, 30), vScrollbarValue, 1.0, 10.0, 0.0);
}


// C#
using UnityEngine;
using System.Collections;

public class GUITest : MonoBehaviour {
                    
    private float vScrollbarValue;
    
    void OnGUI () {
        vScrollbarValue = GUI. VerticalScrollbar (new Rect (25, 25, 100, 30), vScrollbarValue, 1.0f, 10.0f, 0.0f);
    }

}


The Vertical Scrollbar created by the example code
The Vertical Scrollbar created by the example code
ScrollView

ScrollViews are Controls that display a viewable area of a much larger set of Controls.

Basic Usage

ScrollViews require two Rects as arguments. The first Rect defines the location and size of the viewable ScrollView area on the screen. The second Rect defines the size of the space contained inside the viewable area. If the space inside the viewable area is larger than the viewable area, Scrollbars will appear as appropriate. You must also assign and provide a 2D Vector which stores the position of the viewable area that is displayed.

/* ScrollView example */


// JavaScript
var scrollViewVector : Vector2 = Vector2.zero;
var innerText : String = "I am inside the ScrollView";

function OnGUI () {
    // Begin the ScrollView
    scrollViewVector = GUI.BeginScrollView (Rect (25, 25, 100, 100), scrollViewVector, Rect (0, 0, 400, 400));

    // Put something inside the ScrollView
    innerText = GUI.TextArea (Rect (0, 0, 400, 400), innerText);

    // End the ScrollView
    GUI.EndScrollView();
}


// C#
using UnityEngine;
using System.Collections;

public class GUITest : MonoBehaviour {
                    
    private Vector2 scrollViewVector = Vector2.zero;
    private string innerText = "I am inside the ScrollView";
    
    void OnGUI () {
        // Begin the ScrollView
        scrollViewVector = GUI.BeginScrollView (new Rect (25, 25, 100, 100), scrollViewVector, new Rect (0, 0, 400, 400));
    
        // Put something inside the ScrollView
        innerText = GUI.TextArea (new Rect (0, 0, 400, 400), innerText);
    
        // End the ScrollView
        GUI.EndScrollView();
    }

}


The ScrollView created by the example code
The ScrollView created by the example code
Window

Windows are drag-able containers of Controls. They can receive and lose focus when clicked. Because of this, they are implemented slightly differently from the other Controls. Each Window has an id number, and its contents are declared inside a separate function that is called when the Window has focus.

Basic Usage

Windows are the only Control that require an additional function to work properly. You must provide an id number and a function name to be executed for the Window. Inside the Window function, you create your actual behaviors or contained Controls.

/* Window example */


// JavaScript
var windowRect : Rect = Rect (20, 20, 120, 50);

function OnGUI () {
    windowRect = GUI.Window (0, windowRect, WindowFunction, "My Window");
}

function WindowFunction (windowID : int) {
    // Draw any Controls inside the window here
}


// C#
using UnityEngine;
using System.Collections;

public class GUITest : MonoBehaviour {
                    
    private Rect windowRect = new Rect (20, 20, 120, 50);
    
    void OnGUI () {
        windowRect = GUI.Window (0, windowRect, WindowFunction, "My Window");
    }
    
    void WindowFunction (int windowID) {
        // Draw any Controls inside the window here
    }

}


The Window created by the example code
The Window created by the example code
GUI.changed

To detect if the user did any action in the GUI (clicked a button, dragged a slider, etc), read the GUI.changed value from your script. This gets set to true when the user has done something, making it easy to validate the user input.

A common scenario would be for a Toolbar, where you want to change a specific value based on which Button in the Toolbar was clicked. You don’t want to assign the value in every call to OnGUI(), only when one of the Buttons has been clicked.

/* GUI.changed example */


// JavaScript
private var selectedToolbar : int = 0;
private var toolbarStrings = ["One", "Two"];

function OnGUI () {
    // Determine which button is active, whether it was clicked this frame or not
    selectedToolbar = GUI.Toolbar (Rect (50, 10, Screen.width - 100, 30), selectedToolbar, toolbarStrings);

    // If the user clicked a new Toolbar button this frame, we'll process their input
    if (GUI.changed)
    {
        print ("The toolbar was clicked");

        if (selectedToolbar == 0)
        {
            print ("First button was clicked");
        }
        else
        {
            print ("Second button was clicked");
        }
    }
}


// C#
using UnityEngine;
using System.Collections;

public class GUITest : MonoBehaviour {
                    
    private int selectedToolbar = 0;
    private string[] toolbarStrings = {"One", "Two"};
    
    void OnGUI () {
        // Determine which button is active, whether it was clicked this frame or not
        selectedToolbar = GUI.Toolbar (new Rect (50, 10, Screen.width - 100, 30), selectedToolbar, toolbarStrings);
    
        // If the user clicked a new Toolbar button this frame, we'll process their input
        if (GUI.changed)
        {
            Debug.Log("The toolbar was clicked");
    
            if (0 == selectedToolbar)
            {
                Debug.Log("First button was clicked");
            }
            else
            {
                Debug.Log("Second button was clicked");
            }
        }
    }

}


GUI.changed will return true if any GUI Control placed before it was manipulated by the user

Troubleshooting on iOS devices
There are some situations with iOS where your game can work perfectly in the Unity editor but then doesn’t work or maybe doesn’t even start on the actual device. The problems are often related to code or content quality. This section describes the most common scenarios.

The game stops responding after a while. Xcode shows “interrupted” in the status bar.

There are a number of reasons why this may happen. Typical causes include:

Scripting errors such as using uninitialized variables, etc.
Using 3rd party Thumb compiled native libraries. Such libraries trigger a known problem in the iOS SDK linker and might cause random crashes.
Using generic types with value types as parameters (eg, List<int>, List<SomeStruct>, List<SomeEnum>, etc) for serializable script properties.
Using reflection when managed code stripping is enabled.
Errors in the native plugin interface (the managed code method signature does not match the native code function signature). Information from the XCode Debugger console can often help detect these problems (Xcode menu: View > Debug Area > Activate Console).
The Xcode console shows "Program received signal: “SIGBUS” or EXC_BAD_ACCESS error.

This message typically appears on iOS devices when your application receives a NullReferenceException. There two ways to figure out where the fault happened:

Managed stack traces

Unity includes software-based handling of the NullReferenceException. The AOT compiler includes quick checks for null references each time a method or variable is accessed on an object. This feature affects script performance which is why it is enabled only for development builds (enable the “script debugging” option in build settings dialog). If everything was done right and the fault actually is occurring in .NET code then you won’t see EXC_BAD_ACCESS anymore. Instead, the .NET exception text will be printed in the Xcode console (or else your code will just handle it in a “catch” statement). Typical output might be:

Unhandled Exception: System.NullReferenceException: A null value was found where an object instance was required.
  at DayController+$handleTimeOfDay$121+$.MoveNext () [0x0035a] in DayController.js:122 
This indicates that the fault happened in the handleTimeOfDay method of the DayController class, which works as a coroutine. Also if it is script code then you will generally be told the exact line number (eg, “DayController.js:122”). The offending line might be something like the following:

 Instantiate(_imgwww.assetBundle.mainAsset);
This might happen if, say, the script accesses an asset bundle without first checking that it was downloaded correctly.

Native stack traces

Native stack traces are a much more powerful tool for fault investigation but using them requires some expertise. Also, you generally can’t continue after these native (hardware memory access) faults happen. To get a native stack trace, type bt all into the Xcode Debugger Console. Carefully inspect the printed stack traces - they may contain hints about where the error occurred. You might see something like:

...
Thread 1 (thread 11523): 

1. 0 0x006267d0 in m_OptionsMenu_Start ()
1. 1 0x002e4160 in wrapper_runtime_invoke_object_runtime_invoke_void__this___object_intptr_intptr_intptr ()
1. 2 0x00a1dd64 in mono_jit_runtime_invoke (method=0x18b63bc, obj=0x5d10cb0, params=0x0, exc=0x2fffdd34) at /Users/mantasp/work/unity/unity-mono/External/Mono/mono/mono/mini/mini.c:4487
1. 3 0x0088481c in MonoBehaviour::InvokeMethodOrCoroutineChecked ()
...
First of all you should find the stack trace for “Thread 1”, which is the main thread. The very first lines of the stack trace will point to the place where the error occurred. In this example, the trace indicates that the NullReferenceException happened inside the “OptionsMenu” script’s “Start” method. Looking carefully at this method implementation would reveal the cause of the problem. Typically, NullReferenceExceptions happen inside the Start method when incorrect assumptions are made about initialization order. In some cases only a partial stack trace is seen on the Debugger Console:

Thread 1 (thread 11523): 

1. 0 0x0062564c in start ()
This indicates that native symbols were stripped during the Release build of the application. The full stack trace can be obtained with the following procedure:

Remove application from device.
Clean all targets.
Build and run.
Get stack traces again as described above.
EXC_BAD_ACCESS starts occurring when an external library is linked to the Unity iOS application.

This usually happens when an external library is compiled with the ARM Thumb instruction set. Currently such libraries are not compatible with Unity. The problem can be solved easily by recompiling the library without Thumb instructions. You can do this for the library’s Xcode project with the following steps:

in Xcode, select “View” > “Navigators” > “Show Project Navigator” from the menu
select the “Unity-iPhone” project, activate “Build Settings” tab
in the search field enter : “Other C Flags”
add -mno-thumb flag there and rebuild the library.
If the library source is not available you should ask the supplier for a non-thumb version of the library.

The Xcode console shows “WARNING -> applicationDidReceiveMemoryWarning()” and the application crashes immediately afterwards

(Sometimes you might see a message like Program received signal: “0”.) This warning message is often not fatal and merely indicates that iOS is low on memory and is asking applications to free up some memory. Typically, background processes like Mail will free some memory and your application can continue to run. However, if your application continues to use memory or ask for more, the OS will eventually start killing applications and yours could be one of them. Apple does not document what memory usage is safe, but empirical observations show that applications using less than 50% MB of all device RAM (roughly 200–256 MB for 2nd generation ipad) do not have major memory usage problems. The main metric you should rely on is how much RAM your application uses. Your application memory usage consists of three major components:

application code (the OS needs to load and keep your application code in RAM, but some of it might be discarded if really needed)
native heap (used by the engine to store its state, your assets, etc. in RAM)
managed heap (used by your Mono runtime to keep C# or JavaScript objects)
GLES driver memory pools: textures, framebuffers, compiled shaders, etc. Your application memory usage can be tracked by two Xcode Instruments tools: Activity Monitor, Object Allocations and VM Tracker. You can start from the Xcode Run menu: Product > Profile and then select specific tool. Activity Monitor tool shows all process statistics including Real memory which can be regarded as the total amount of RAM used by your application. Note: OS and device HW version combination might noticeably affect memory usage numbers, so you should be careful when comparing numbers obtained on different devices.

Note: The internal profiler shows only the heap allocated by .NET scripts. Total memory usage can be determined via Xcode Instruments as shown above. This figure includes parts of the application binary, some standard framework buffers, Unity engine internal state buffers, the .NET runtime heap (number printed by internal profiler), GLES driver heap and some other miscellaneous stuff.

The other tool displays all allocations made by your application and includes both native heap and managed heap statistics (don’t forget to check the Created and still living box to get the current state of the application). The important statistic is the Net bytes value.


To keep memory usage low:

Reduce the application binary size by using the strongest iOS stripping options, and avoid unnecessary dependencies on different .NET libraries. See the player settings and player size optimization manual pages for further details.
Reduce the size of your content. Use PVRTC compression for textures and use low poly models. See the manual page about reducing file size for more information.
Don’t allocate more memory than necessary in your scripts. Track mono heap size and usage with the internal profiler
Note: with Unity 3.0, the scene loading implementation has changed significantly and now all scene assets are preloaded. This results in fewer hiccups when instantiating game objects. If you need more fine-grained control of asset loading and unloading during gameplay, you should use Resources.Load and Object.Destroy.
Querying the OS about the amount of free memory may seem like a good idea to evaluate how well your application is performing. However, the free memory statistic is likely to be unreliable since the OS uses a lot of dynamic buffers and caches. The only reliable approach is to keep track of memory consumption for your application and use that as the main metric. Pay attention to how the graphs from the tools described above change over time, especially after loading new levels.

The game runs correctly when launched from Xcode but crashes while loading the first level when launched manually on the device.

There could be several reasons for this. You need to inspect the device logs to get more details. Connect the device to your Mac, launch Xcode and select Window > Organizer from the menu. Select your device in the Organizer’s left toolbar, then click on the “Console” tab and review the latest messages carefully. Additionally, you may need to investigate crash reports. You can find out how to obtain crash reports here: http://developer.apple.com/iphone/library/technotes/tn2008/tn2151.html.

The Xcode Organizer console contains the message “killed by SpringBoard”.

There is a poorly-documented time limit for an iOS application to render its first frames and process input. If your application exceeds this limit, it will be killed by SpringBoard. This may happen in an application with a first scene which is too large, for example. To avoid this problem, it is advisable to create a small initial scene which just displays a splash screen, waits a frame or two with yield and then starts loading the real scene. This can be done with code as simple as the following:

function Start() {
    yield;
    Application.LoadLevel("Test");
}
Type.GetProperty() / Type.GetValue() cause crashes on the device

Currently Type.GetProperty() and Type.GetValue() are supported only for the .NET 2.0 Subset profile. You can select the .NET API compatibility level in the Player Settings.

Note: Type.GetProperty() and Type.GetValue() might be incompatible with managed code stripping and might need to be excluded (you can supply a custom non-strippable type list during the stripping process to accomplish this). For further details, see the iOS player size optimization guide.

The game crashes with the error message “ExecutionEngineException: Attempting to JIT compile method ‘SometType`1<SomeValueType>:.ctor ()’ while running with –aot-only.”

The Mono .NET implementation for iOS is based on AOT (ahead of time compilation to native code) technology, which has its limitations. It compiles only those generic type methods (where a value type is used as a generic parameter) which are explicitly used by other code. When such methods are used only via reflection or from native code (ie, the serialization system) then they get skipped during AOT compilation. The AOT compiler can be hinted to include code by adding a dummy method somewhere in the script code. This can refer to the missing methods and so get them compiled ahead of time.

void _unusedMethod() {
    var tmp = new SomeType<SomeValueType>();
}
Note: value types are basic types, enums and structs.

Various crashes occur on the device when a combination of System.Security.Cryptography and managed code stripping is used

.NET Cryptography services rely heavily on reflection and so are not compatible with managed code stripping since this involves static code analysis. Sometimes the easiest solution to the crashes is to exclude the whole System.Security.Crypography namespace from the stripping process.

The stripping process can be customized by adding a custom link.xml file to the Assets folder of your Unity project. This specifies which types and namespaces should be excluded from stripping. Further details can be found in the iOS player size optimization guide.

link.xml

<linker>
       <assembly fullname="mscorlib">
               <namespace fullname="System.Security.Cryptography" preserve="all"/>
       </assembly>
</linker>
Application crashes when using System.Security.Cryptography.MD5 with managed code stripping

You might consider advice listed above or can work around this problem by adding extra reference to specific class to your script code:

object obj = new MD5CryptoServiceProvider();
“Ran out of trampolines of type 0/1/2” runtime error

This error usually happens if you use lots of recursive generics. You can hint to the AOT compiler to allocate more trampolines of type 0, type 1 or type 2. Additional AOT compiler command line options can be specified in the “Other Settings” section of the Player Settings. For type 1 trampolines, specify nrgctx-trampolines=ABCD, where ABCD is the number of new trampolines required (i.e. 4096). For type 2 trampolines specify nimt-trampolines=ABCD and for type 0 trampolines specify ntrampolines=ABCD.

After upgrading Xcode Unity iOS runtime fails with message “You are using Unity iPhone Basic. You are not allowed to remove the Unity splash screen from your game”

With some latest Xcode releases there were changes introduced in PNG compression and optimization tool. These changes might cause false positives in Unity iOS runtime checks for splash screen modifications. If you encounter such problems try upgrading Unity to the latest publicly available version. If it does not help you might consider following workaround:

Replace your Xcode project from scratch when building from Unity (instead of appending it)
Delete already installed project from device
Clean project in Xcode (Product->Clean)
Clear Xcode’s Derived Data folders (Xcode->Preferences->Locations) If this still does not help try disabling PNG re-compression in Xcode:
Open your Xcode project
Select “Unity-iPhone” project there
Select “Build Settings” tab there
Look for “Compress PNG files” option and set it to NO
App Store submission fails with “iPhone/iPod Touch: application executable is missing a required architecture. At least one of the following architecture(s) must be present: armv6” message

You might get such message when updating already existing application, which previously was submitted with armv6 support. Unity 4.x and Xcode 4.5 does not support armv6 platform anymore. To solve submission problem just set Target OS Version in Unity Player Settings to 4.3 or higher.

WWW downloads are working fine in Unity Editor and on Android, but not on iOS

Most common mistake is to assume that WWW downloads are always happening on separate thread. On some platforms this might be true, but you should not take it for granted. Best way to track WWW status is either to use yield statement or check status in Update method. You should not use busy while loops for that.

“PlayerLoop called recursively!” error occurs when using Cocoa via a native function called from a script

Some operations with the UI will result in iOS redrawing the window immediately (the most common example is adding a UIView with a UIViewController to the main UIWindow). If you call a native function from a script, it will happen inside Unity’s PlayerLoop, resulting in PlayerLoop being called recursively. In such cases, you should consider using the performSelectorOnMainThread method with waitUntilDone set to false. It will inform iOS to schedule the operation to run between Unity’s PlayerLoop calls.

Profiler or Debugger unable to see game running on iOS device

Check that you have built a Development build, and ticked the “Enable Script Debugging” and “Autoconnect profiler” boxes (as appropriate).
The application running on the device will make a multicast broadcast to 225.0.0.222 on UDP port 54997. Check that your network settings allow this traffic. Then, the profiler will make a connection to the remote device on a port in the range 55000 - 55511 to fetch profiler data from the device. These ports will need to be open for UDP access.
Missing DLLs

If your application runs ok in editor but you get errors in your iOS project this may be caused by missing DLLs (e.g. I18N.dll, I19N.West.dll). In this case, try copying those dlls from within the Unity.app to your project’s Assets/Plugins folder. The location of the DLLs within the unity app is: Unity.app/Contents/Frameworks/Mono/lib/mono/unity You should then also check the stripping level of your project to ensure the classes in the DLLs aren’t being removed when the build is optimised. Refer to the iOS Optimisation Page for more information on iOS Stripping Levels.

Xcode Debugger console reports: ExecutionEngineException: Attempting to JIT compile method ‘(wrapper native-to-managed) Test:TestFunc (int)’ while running with –aot-only

Typically such message is received when managed function delegate is passed to the native function, but required wrapper code wasn’t generated when building application. You can help AOT compiler by hinting which methods will be passed as delegates to the native code. This can be done by adding “MonoPInvokeCallbackAttribute” custom attribute. Currently only static methods can be passed as delegates to the native code.

Sample code:

using UnityEngine;
using System.Collections;
using System;
using System.Runtime.InteropServices;
using AOT;

public class NewBehaviourScript : MonoBehaviour {
    [DllImport ("__Internal")]
    private static extern void DoSomething (NoParamDelegate del1, StringParamDelegate del2);

    delegate void NoParamDelegate ();
    delegate void StringParamDelegate (string str);
    
    [MonoPInvokeCallback(typeof(NoParamDelegate))]
    public static void NoParamCallback() {
        Debug.Log ("Hello from NoParamCallback");
    }
    
    [MonoPInvokeCallback(typeof(StringParamDelegate))]
    public static void StringParamCallback(string str) {
        Debug.Log(string.Format("Hello from StringParamCallback {0}", str));
    }

    // Use this for initialization
    void Start() {
        DoSomething(NoParamCallback, StringParamCallback);
    }
}
Xcode throws compilation error: “ld : unable to insert branch island. No insertion point available. for architecture armv7”, “clang: error: linker command failed with exit code 1 (use -v to see invocation)”

That error usually means there is just too much code in single module. Typically it is caused by having lots of script code or having big external .NET assemblies included into build. And enabling script debugging might make things worse, because it adds quite few additional instructions to each function, so it is easier to hit that limit.

Enabling managed code stripping in player settings might help with this problem, especially if big external .NET assemblies are involved. But if the issue persists then the best solution is to split user script code into multiple assemblies. The easiest way to this is move some code to Plugins folder. Code at this location is put to different assembly. Also check the information about how special folder names affect script compilation